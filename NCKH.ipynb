{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "af8041eb-e317-4b6c-c812-4f9ae2bbaf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "30ce54a7-3f46-4a07-f86d-7dd213bde192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.11-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu128)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from cog) (2.32.4)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.40.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Downloading cog-0.16.11-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.50.0\n",
            "    Uninstalling starlette-0.50.0:\n",
            "      Successfully uninstalled starlette-0.50.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.128.2\n",
            "    Uninstalling fastapi-0.128.2:\n",
            "      Successfully uninstalled fastapi-0.128.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sse-starlette 3.2.0 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "google-adk 1.24.0 requires fastapi<1.0.0,>=0.124.1, but you have fastapi 0.118.3 which is incompatible.\n",
            "google-adk 1.24.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.11 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.16.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292936 sha256=c27a39908037c89283b30effae1b601a80b2106def04c873908a57c2fa43545f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "# TextVQA (phải chạy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "12d61b1f-5f56-4fab-a542-280d2d27da98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-11 15:39:53--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.108, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M   270MB/s    in 0.4s    \n",
            "\n",
            "2026-02-11 15:39:54 (270 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-02-11 15:39:54--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.108, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-02-11 15:39:54 (129 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-02-11 15:39:54--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.108, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-02-11 15:39:55 (172 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "e51eacef-c527-48c7-afa7-8dedd7daaa3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-11 15:39:55--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.108, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G  67.7MB/s    in 86s     \n",
            "\n",
            "2026-02-11 15:41:21 (78.1 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (Dhuy + HNhien)\n"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "4ecfdc11-70ec-48c3-a0a6-7703a0aa1ac7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "reshape position embedding from 900 to 196\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "<All keys matched successfully>\n",
            "Start training\n",
            "Loss: 17.1619\n",
            "==========================================\n",
            "Train Epoch: [0]  [   0/8650]  eta: 9:05:33  lr: 0.000020  loss: 17.1619  time: 3.7842  data: 0.8004  max mem: 6922\n",
            "Loss: 22.2107\n",
            "==========================================\n",
            "Loss: 17.6196\n",
            "==========================================\n",
            "Loss: 22.4522\n",
            "==========================================\n",
            "Loss: 18.5996\n",
            "==========================================\n",
            "Loss: 15.116\n",
            "==========================================\n",
            "Loss: 13.7835\n",
            "==========================================\n",
            "Loss: 22.9052\n",
            "==========================================\n",
            "Loss: 22.1199\n",
            "==========================================\n",
            "Loss: 22.657\n",
            "==========================================\n",
            "Loss: 16.3429\n",
            "==========================================\n",
            "Loss: 16.1788\n",
            "==========================================\n",
            "Loss: 13.1597\n",
            "==========================================\n",
            "Loss: 20.5788\n",
            "==========================================\n",
            "Loss: 15.4407\n",
            "==========================================\n",
            "Loss: 30.1561\n",
            "==========================================\n",
            "Loss: 16.5253\n",
            "==========================================\n",
            "Loss: 10.3148\n",
            "==========================================\n",
            "Loss: 17.5338\n",
            "==========================================\n",
            "Loss: 30.6049\n",
            "==========================================\n",
            "Loss: 24.2531\n",
            "==========================================\n",
            "Loss: 17.5263\n",
            "==========================================\n",
            "Loss: 18.6688\n",
            "==========================================\n",
            "Loss: 16.5131\n",
            "==========================================\n",
            "Loss: 12.7678\n",
            "==========================================\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2d6743aca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1600, in _shutdown_workers\n",
            "    self._pin_memory_thread.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 55, in train\n",
            "    loss = model.forward(image, question, answer, n, weights, train=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip_vqa.py\", line 63, in forward\n",
            "    answer_output = self.text_decoder(\n",
            "                    ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 886, in forward\n",
            "    outputs = self.bert(\n",
            "              ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 781, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 445, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 361, in forward\n",
            "    cross_attention_outputs = self.crossattention(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 286, in forward\n",
            "    attention_output = self.output(self_outputs[0], hidden_states)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 236, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False #\\\n",
        "    # --resume ./output/textVQA/checkpoint_05.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train textVQA + pretrain_animal (Khoi)"
      ],
      "metadata": {
        "id": "i9pDWKQLVYL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pWHhslGC2SmK",
        "collapsed": true,
        "outputId": "07882e84-bde9-4079-b6a6-376612adffd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 166, in main\n",
            "    model = blip_vqa(pretrained=config['pretrained'], image_size=config['image_size'], \n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip_vqa.py\", line 169, in blip_vqa\n",
            "    model = BLIP_VQA(**kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip_vqa.py\", line 20, in __init__\n",
            "    self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer, drop_path_rate=0.1)\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip.py\", line 293, in create_vit\n",
            "    visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/vit.py\", line 155, in __init__\n",
            "    Block(\n",
            "  File \"/content/drive/MyDrive/BLIP/models/vit.py\", line 95, in __init__\n",
            "    self.attn = Attention(\n",
            "                ^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/vit.py\", line 51, in __init__\n",
            "    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 115, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 124, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/init.py\", line 573, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound, generator=generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_animals.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_animals \\\n",
        "    --device cuda \\\n",
        "    --distributed False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Textvqa + PretrainTextCaps"
      ],
      "metadata": {
        "id": "c1Xp2adpkVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_Textcaps.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_textcaps \\\n",
        "    --device cuda \\\n",
        "    --distributed False"
      ],
      "metadata": {
        "id": "9RnYLAjmkcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA + resnet"
      ],
      "metadata": {
        "id": "ZU3eOQrIfwJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/dataset_animals"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ekwts1oGwoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain.py --config configs/pre_animals.yaml --output_dir output/pretrain_animals --checkpoint output/pretrain_animals/checkpoint_22.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5gFi5qxQmH",
        "outputId": "41cbfa9f-fb90-4d33-abf7-62afd1e11954",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating dataset\n",
            "Đang tải dữ liệu từ: /content/dataset_animals/dataset.json\n",
            "number of training samples: 20000\n",
            "Creating model\n",
            "/embeddings/word_embeddings is tied\n",
            "/embeddings/position_embeddings is tied\n",
            "/embeddings/LayerNorm is tied\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "/encoder/layer/2/crossattention/self/query is tied\n",
            "/encoder/layer/2/crossattention/self/key is tied\n",
            "/encoder/layer/2/crossattention/self/value is tied\n",
            "/encoder/layer/2/crossattention/output/dense is tied\n",
            "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/2/intermediate/dense is tied\n",
            "/encoder/layer/2/output/dense is tied\n",
            "/encoder/layer/2/output/LayerNorm is tied\n",
            "/encoder/layer/3/crossattention/self/query is tied\n",
            "/encoder/layer/3/crossattention/self/key is tied\n",
            "/encoder/layer/3/crossattention/self/value is tied\n",
            "/encoder/layer/3/crossattention/output/dense is tied\n",
            "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/3/intermediate/dense is tied\n",
            "/encoder/layer/3/output/dense is tied\n",
            "/encoder/layer/3/output/LayerNorm is tied\n",
            "/encoder/layer/4/crossattention/self/query is tied\n",
            "/encoder/layer/4/crossattention/self/key is tied\n",
            "/encoder/layer/4/crossattention/self/value is tied\n",
            "/encoder/layer/4/crossattention/output/dense is tied\n",
            "/encoder/layer/4/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/4/intermediate/dense is tied\n",
            "/encoder/layer/4/output/dense is tied\n",
            "/encoder/layer/4/output/LayerNorm is tied\n",
            "/encoder/layer/5/crossattention/self/query is tied\n",
            "/encoder/layer/5/crossattention/self/key is tied\n",
            "/encoder/layer/5/crossattention/self/value is tied\n",
            "/encoder/layer/5/crossattention/output/dense is tied\n",
            "/encoder/layer/5/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/5/intermediate/dense is tied\n",
            "/encoder/layer/5/output/dense is tied\n",
            "/encoder/layer/5/output/LayerNorm is tied\n",
            "/encoder/layer/6/crossattention/self/query is tied\n",
            "/encoder/layer/6/crossattention/self/key is tied\n",
            "/encoder/layer/6/crossattention/self/value is tied\n",
            "/encoder/layer/6/crossattention/output/dense is tied\n",
            "/encoder/layer/6/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/6/intermediate/dense is tied\n",
            "/encoder/layer/6/output/dense is tied\n",
            "/encoder/layer/6/output/LayerNorm is tied\n",
            "/encoder/layer/7/crossattention/self/query is tied\n",
            "/encoder/layer/7/crossattention/self/key is tied\n",
            "/encoder/layer/7/crossattention/self/value is tied\n",
            "/encoder/layer/7/crossattention/output/dense is tied\n",
            "/encoder/layer/7/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/7/intermediate/dense is tied\n",
            "/encoder/layer/7/output/dense is tied\n",
            "/encoder/layer/7/output/LayerNorm is tied\n",
            "/encoder/layer/8/crossattention/self/query is tied\n",
            "/encoder/layer/8/crossattention/self/key is tied\n",
            "/encoder/layer/8/crossattention/self/value is tied\n",
            "/encoder/layer/8/crossattention/output/dense is tied\n",
            "/encoder/layer/8/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/8/intermediate/dense is tied\n",
            "/encoder/layer/8/output/dense is tied\n",
            "/encoder/layer/8/output/LayerNorm is tied\n",
            "/encoder/layer/9/crossattention/self/query is tied\n",
            "/encoder/layer/9/crossattention/self/key is tied\n",
            "/encoder/layer/9/crossattention/self/value is tied\n",
            "/encoder/layer/9/crossattention/output/dense is tied\n",
            "/encoder/layer/9/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/9/intermediate/dense is tied\n",
            "/encoder/layer/9/output/dense is tied\n",
            "/encoder/layer/9/output/LayerNorm is tied\n",
            "/encoder/layer/10/crossattention/self/query is tied\n",
            "/encoder/layer/10/crossattention/self/key is tied\n",
            "/encoder/layer/10/crossattention/self/value is tied\n",
            "/encoder/layer/10/crossattention/output/dense is tied\n",
            "/encoder/layer/10/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/10/intermediate/dense is tied\n",
            "/encoder/layer/10/output/dense is tied\n",
            "/encoder/layer/10/output/LayerNorm is tied\n",
            "/encoder/layer/11/crossattention/self/query is tied\n",
            "/encoder/layer/11/crossattention/self/key is tied\n",
            "/encoder/layer/11/crossattention/self/value is tied\n",
            "/encoder/layer/11/crossattention/output/dense is tied\n",
            "/encoder/layer/11/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/11/intermediate/dense is tied\n",
            "/encoder/layer/11/output/dense is tied\n",
            "/encoder/layer/11/output/LayerNorm is tied\n",
            "resume checkpoint from output/pretrain_animals/checkpoint_16.pth\n",
            "Start training\n",
            "Train Epoch: [17]  [   0/2500]  eta: 3:19:38  lr: 0.000050  loss_ita: 6.1392  loss_itm: 0.6385  loss_lm: 4.7927  time: 4.7912  data: 1.5059  max mem: 7121\n",
            "Train Epoch: [17]  [  50/2500]  eta: 0:39:44  lr: 0.000050  loss_ita: 5.9841  loss_itm: 0.6401  loss_lm: 4.4402  time: 0.8945  data: 0.0004  max mem: 7129\n",
            "Train Epoch: [17]  [ 100/2500]  eta: 0:38:02  lr: 0.000050  loss_ita: 5.9510  loss_itm: 0.6436  loss_lm: 4.7203  time: 0.9362  data: 0.0003  max mem: 7129\n",
            "Train Epoch: [17]  [ 150/2500]  eta: 0:36:56  lr: 0.000050  loss_ita: 5.8191  loss_itm: 0.6290  loss_lm: 4.2306  time: 0.9299  data: 0.0006  max mem: 7132\n",
            "Train Epoch: [17]  [ 200/2500]  eta: 0:36:00  lr: 0.000050  loss_ita: 5.8432  loss_itm: 0.6357  loss_lm: 4.4590  time: 0.9304  data: 0.0005  max mem: 7132\n",
            "Train Epoch: [17]  [ 250/2500]  eta: 0:35:06  lr: 0.000050  loss_ita: 6.0463  loss_itm: 0.6259  loss_lm: 4.6369  time: 0.9230  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 300/2500]  eta: 0:34:17  lr: 0.000050  loss_ita: 5.9681  loss_itm: 0.6204  loss_lm: 4.2531  time: 0.9307  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 350/2500]  eta: 0:33:28  lr: 0.000050  loss_ita: 5.8556  loss_itm: 0.6375  loss_lm: 4.7831  time: 0.9301  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 400/2500]  eta: 0:32:40  lr: 0.000050  loss_ita: 6.1228  loss_itm: 0.6355  loss_lm: 4.8984  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 450/2500]  eta: 0:31:51  lr: 0.000050  loss_ita: 6.3495  loss_itm: 0.6393  loss_lm: 4.6135  time: 0.9222  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 500/2500]  eta: 0:31:04  lr: 0.000050  loss_ita: 6.1634  loss_itm: 0.6500  loss_lm: 4.8925  time: 0.9304  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 550/2500]  eta: 0:30:17  lr: 0.000050  loss_ita: 5.9357  loss_itm: 0.6350  loss_lm: 4.1571  time: 0.9271  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 600/2500]  eta: 0:29:30  lr: 0.000050  loss_ita: 5.8463  loss_itm: 0.6394  loss_lm: 4.1982  time: 0.9227  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [ 650/2500]  eta: 0:28:43  lr: 0.000050  loss_ita: 6.0158  loss_itm: 0.6351  loss_lm: 4.7299  time: 0.9328  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 700/2500]  eta: 0:27:57  lr: 0.000050  loss_ita: 6.1428  loss_itm: 0.6314  loss_lm: 4.6872  time: 0.9312  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 750/2500]  eta: 0:27:10  lr: 0.000050  loss_ita: 6.2539  loss_itm: 0.6385  loss_lm: 4.5769  time: 0.9238  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 800/2500]  eta: 0:26:23  lr: 0.000050  loss_ita: 6.0947  loss_itm: 0.6348  loss_lm: 4.4382  time: 0.9340  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 850/2500]  eta: 0:25:36  lr: 0.000050  loss_ita: 5.9637  loss_itm: 0.6348  loss_lm: 4.9964  time: 0.9370  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 900/2500]  eta: 0:24:49  lr: 0.000050  loss_ita: 6.2077  loss_itm: 0.6503  loss_lm: 4.5971  time: 0.9215  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 950/2500]  eta: 0:24:03  lr: 0.000050  loss_ita: 6.2782  loss_itm: 0.6304  loss_lm: 4.1598  time: 0.9229  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1000/2500]  eta: 0:23:16  lr: 0.000050  loss_ita: 6.1663  loss_itm: 0.6368  loss_lm: 4.6378  time: 0.9374  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1050/2500]  eta: 0:22:30  lr: 0.000050  loss_ita: 6.2009  loss_itm: 0.6334  loss_lm: 4.4458  time: 0.9369  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1100/2500]  eta: 0:21:43  lr: 0.000050  loss_ita: 6.1823  loss_itm: 0.6355  loss_lm: 4.9945  time: 0.9254  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1150/2500]  eta: 0:20:56  lr: 0.000050  loss_ita: 6.0765  loss_itm: 0.6280  loss_lm: 4.5967  time: 0.9312  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1200/2500]  eta: 0:20:10  lr: 0.000050  loss_ita: 6.0619  loss_itm: 0.6410  loss_lm: 4.8511  time: 0.9367  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [1250/2500]  eta: 0:19:23  lr: 0.000050  loss_ita: 6.0537  loss_itm: 0.6422  loss_lm: 5.3150  time: 0.9227  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1300/2500]  eta: 0:18:36  lr: 0.000050  loss_ita: 5.9791  loss_itm: 0.6471  loss_lm: 4.5806  time: 0.9206  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1350/2500]  eta: 0:17:50  lr: 0.000050  loss_ita: 5.7339  loss_itm: 0.6316  loss_lm: 4.2935  time: 0.9418  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1400/2500]  eta: 0:17:03  lr: 0.000050  loss_ita: 5.8574  loss_itm: 0.6248  loss_lm: 4.8669  time: 0.9227  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1450/2500]  eta: 0:16:17  lr: 0.000050  loss_ita: 5.8236  loss_itm: 0.6454  loss_lm: 4.4377  time: 0.9271  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1500/2500]  eta: 0:15:30  lr: 0.000050  loss_ita: 5.6852  loss_itm: 0.6495  loss_lm: 4.7744  time: 0.9329  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1550/2500]  eta: 0:14:44  lr: 0.000050  loss_ita: 5.4990  loss_itm: 0.6276  loss_lm: 4.4243  time: 0.9361  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1600/2500]  eta: 0:13:57  lr: 0.000050  loss_ita: 5.6231  loss_itm: 0.6459  loss_lm: 4.4868  time: 0.9241  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1650/2500]  eta: 0:13:10  lr: 0.000050  loss_ita: 5.7932  loss_itm: 0.6374  loss_lm: 4.5288  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1700/2500]  eta: 0:12:24  lr: 0.000050  loss_ita: 5.9085  loss_itm: 0.6344  loss_lm: 4.4765  time: 0.9344  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1750/2500]  eta: 0:11:37  lr: 0.000050  loss_ita: 6.1626  loss_itm: 0.6300  loss_lm: 4.4793  time: 0.9218  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1800/2500]  eta: 0:10:51  lr: 0.000050  loss_ita: 6.1059  loss_itm: 0.6371  loss_lm: 4.8303  time: 0.9242  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1850/2500]  eta: 0:10:04  lr: 0.000050  loss_ita: 5.7550  loss_itm: 0.6354  loss_lm: 4.7988  time: 0.9292  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [1900/2500]  eta: 0:09:18  lr: 0.000050  loss_ita: 5.6715  loss_itm: 0.6439  loss_lm: 4.9443  time: 0.9322  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [1950/2500]  eta: 0:08:31  lr: 0.000050  loss_ita: 5.9078  loss_itm: 0.6322  loss_lm: 4.9832  time: 0.9235  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2000/2500]  eta: 0:07:45  lr: 0.000050  loss_ita: 6.1245  loss_itm: 0.6394  loss_lm: 4.4271  time: 0.9317  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2050/2500]  eta: 0:06:58  lr: 0.000050  loss_ita: 6.3317  loss_itm: 0.6358  loss_lm: 4.5954  time: 0.9336  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2100/2500]  eta: 0:06:12  lr: 0.000050  loss_ita: 6.1765  loss_itm: 0.6298  loss_lm: 4.4585  time: 0.9223  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2150/2500]  eta: 0:05:25  lr: 0.000050  loss_ita: 5.8163  loss_itm: 0.6358  loss_lm: 4.6149  time: 0.9204  data: 0.0002  max mem: 7134\n",
            "Train Epoch: [17]  [2200/2500]  eta: 0:04:39  lr: 0.000050  loss_ita: 5.7070  loss_itm: 0.6437  loss_lm: 4.4813  time: 0.9407  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2250/2500]  eta: 0:03:52  lr: 0.000050  loss_ita: 5.9511  loss_itm: 0.6427  loss_lm: 4.4372  time: 0.9190  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2300/2500]  eta: 0:03:05  lr: 0.000050  loss_ita: 6.0164  loss_itm: 0.6342  loss_lm: 4.4307  time: 0.9265  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2350/2500]  eta: 0:02:19  lr: 0.000050  loss_ita: 5.9607  loss_itm: 0.6364  loss_lm: 4.5868  time: 0.9313  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2400/2500]  eta: 0:01:32  lr: 0.000050  loss_ita: 6.0198  loss_itm: 0.6424  loss_lm: 4.5925  time: 0.9271  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2450/2500]  eta: 0:00:46  lr: 0.000050  loss_ita: 6.2225  loss_itm: 0.6522  loss_lm: 4.5737  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2499/2500]  eta: 0:00:00  lr: 0.000050  loss_ita: 6.3974  loss_itm: 0.6369  loss_lm: 5.1242  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17] Total time: 0:38:44 (0.9298 s / it)\n",
            "Averaged stats: lr: 0.0001  loss_ita: 5.9935  loss_itm: 0.6356  loss_lm: 4.6061\n",
            "Train Epoch: [18]  [   0/2500]  eta: 1:20:04  lr: 0.000045  loss_ita: 6.2714  loss_itm: 0.6457  loss_lm: 4.7524  time: 1.9218  data: 0.8585  max mem: 7970\n",
            "Train Epoch: [18]  [  50/2500]  eta: 0:40:21  lr: 0.000045  loss_ita: 6.3983  loss_itm: 0.6511  loss_lm: 4.1244  time: 0.9882  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 100/2500]  eta: 0:38:50  lr: 0.000045  loss_ita: 6.3850  loss_itm: 0.6482  loss_lm: 4.3894  time: 0.9561  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 150/2500]  eta: 0:37:38  lr: 0.000045  loss_ita: 6.2264  loss_itm: 0.6374  loss_lm: 4.5178  time: 0.9203  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 200/2500]  eta: 0:36:30  lr: 0.000045  loss_ita: 5.9527  loss_itm: 0.6499  loss_lm: 4.5807  time: 0.9236  data: 0.0003  max mem: 7982\n",
            "Train Epoch: [18]  [ 250/2500]  eta: 0:35:32  lr: 0.000045  loss_ita: 5.7489  loss_itm: 0.6327  loss_lm: 4.1563  time: 0.9307  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 300/2500]  eta: 0:34:36  lr: 0.000045  loss_ita: 5.7306  loss_itm: 0.6399  loss_lm: 4.6968  time: 0.9242  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 350/2500]  eta: 0:33:43  lr: 0.000045  loss_ita: 5.9946  loss_itm: 0.6380  loss_lm: 4.8402  time: 0.9228  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 400/2500]  eta: 0:32:53  lr: 0.000045  loss_ita: 6.1702  loss_itm: 0.6330  loss_lm: 4.2296  time: 0.9339  data: 0.0004  max mem: 7983\n",
            "Train Epoch: [18]  [ 450/2500]  eta: 0:32:02  lr: 0.000045  loss_ita: 6.1112  loss_itm: 0.6402  loss_lm: 4.3074  time: 0.9217  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 500/2500]  eta: 0:31:12  lr: 0.000045  loss_ita: 6.2875  loss_itm: 0.6396  loss_lm: 4.1692  time: 0.9172  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 550/2500]  eta: 0:30:24  lr: 0.000045  loss_ita: 6.1270  loss_itm: 0.6236  loss_lm: 4.2401  time: 0.9325  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 600/2500]  eta: 0:29:36  lr: 0.000045  loss_ita: 5.8813  loss_itm: 0.6358  loss_lm: 4.6269  time: 0.9270  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 650/2500]  eta: 0:28:47  lr: 0.000045  loss_ita: 5.8662  loss_itm: 0.6497  loss_lm: 4.5022  time: 0.9237  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 700/2500]  eta: 0:27:59  lr: 0.000045  loss_ita: 6.0300  loss_itm: 0.6378  loss_lm: 4.8883  time: 0.9273  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 750/2500]  eta: 0:27:12  lr: 0.000045  loss_ita: 6.1708  loss_itm: 0.6311  loss_lm: 4.5949  time: 0.9273  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 800/2500]  eta: 0:26:24  lr: 0.000045  loss_ita: 6.1851  loss_itm: 0.6380  loss_lm: 4.5566  time: 0.9163  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 850/2500]  eta: 0:25:36  lr: 0.000045  loss_ita: 6.0905  loss_itm: 0.6362  loss_lm: 4.4975  time: 0.9210  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 900/2500]  eta: 0:24:50  lr: 0.000045  loss_ita: 6.1650  loss_itm: 0.6281  loss_lm: 4.4557  time: 0.9308  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 950/2500]  eta: 0:24:02  lr: 0.000045  loss_ita: 6.4208  loss_itm: 0.6424  loss_lm: 4.9033  time: 0.9256  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1000/2500]  eta: 0:23:16  lr: 0.000045  loss_ita: 6.4365  loss_itm: 0.6313  loss_lm: 4.4134  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1050/2500]  eta: 0:22:29  lr: 0.000045  loss_ita: 6.1584  loss_itm: 0.6530  loss_lm: 4.5601  time: 0.9284  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1100/2500]  eta: 0:21:43  lr: 0.000045  loss_ita: 6.0874  loss_itm: 0.6354  loss_lm: 4.9366  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1150/2500]  eta: 0:20:56  lr: 0.000045  loss_ita: 5.9607  loss_itm: 0.6468  loss_lm: 4.2832  time: 0.9209  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1200/2500]  eta: 0:20:09  lr: 0.000045  loss_ita: 5.9632  loss_itm: 0.6471  loss_lm: 4.6797  time: 0.9294  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1250/2500]  eta: 0:19:22  lr: 0.000045  loss_ita: 6.1932  loss_itm: 0.6278  loss_lm: 4.3471  time: 0.9296  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1300/2500]  eta: 0:18:36  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6202  loss_lm: 4.5908  time: 0.9244  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1350/2500]  eta: 0:17:49  lr: 0.000045  loss_ita: 6.0786  loss_itm: 0.6392  loss_lm: 4.4718  time: 0.9198  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1400/2500]  eta: 0:17:02  lr: 0.000045  loss_ita: 6.0546  loss_itm: 0.6346  loss_lm: 4.4181  time: 0.9355  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1450/2500]  eta: 0:16:16  lr: 0.000045  loss_ita: 6.1391  loss_itm: 0.6276  loss_lm: 4.5114  time: 0.9284  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1500/2500]  eta: 0:15:29  lr: 0.000045  loss_ita: 5.7351  loss_itm: 0.6409  loss_lm: 4.5599  time: 0.9248  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [1550/2500]  eta: 0:14:43  lr: 0.000045  loss_ita: 5.2817  loss_itm: 0.6334  loss_lm: 4.4926  time: 0.9290  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1600/2500]  eta: 0:13:56  lr: 0.000045  loss_ita: 5.3891  loss_itm: 0.6325  loss_lm: 4.5553  time: 0.9378  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1650/2500]  eta: 0:13:10  lr: 0.000045  loss_ita: 5.8572  loss_itm: 0.6407  loss_lm: 4.6760  time: 0.9198  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1700/2500]  eta: 0:12:23  lr: 0.000045  loss_ita: 6.2208  loss_itm: 0.6292  loss_lm: 4.2864  time: 0.9233  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1750/2500]  eta: 0:11:37  lr: 0.000045  loss_ita: 6.4253  loss_itm: 0.6383  loss_lm: 4.3046  time: 0.9352  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1800/2500]  eta: 0:10:50  lr: 0.000045  loss_ita: 6.1586  loss_itm: 0.6361  loss_lm: 4.5963  time: 0.9234  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1850/2500]  eta: 0:10:04  lr: 0.000045  loss_ita: 6.1716  loss_itm: 0.6387  loss_lm: 4.4412  time: 0.9251  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1900/2500]  eta: 0:09:17  lr: 0.000045  loss_ita: 6.0535  loss_itm: 0.6367  loss_lm: 4.3708  time: 0.9263  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1950/2500]  eta: 0:08:31  lr: 0.000045  loss_ita: 5.9256  loss_itm: 0.6286  loss_lm: 4.6511  time: 0.9289  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2000/2500]  eta: 0:07:44  lr: 0.000045  loss_ita: 5.7240  loss_itm: 0.6361  loss_lm: 4.3443  time: 0.9228  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2050/2500]  eta: 0:06:58  lr: 0.000045  loss_ita: 5.7294  loss_itm: 0.6406  loss_lm: 4.5127  time: 0.9255  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2100/2500]  eta: 0:06:11  lr: 0.000045  loss_ita: 5.7598  loss_itm: 0.6372  loss_lm: 5.1710  time: 0.9374  data: 0.0006  max mem: 7989\n",
            "Train Epoch: [18]  [2150/2500]  eta: 0:05:25  lr: 0.000045  loss_ita: 6.0122  loss_itm: 0.6364  loss_lm: 4.6644  time: 0.9268  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2200/2500]  eta: 0:04:38  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6409  loss_lm: 3.9354  time: 0.9197  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2250/2500]  eta: 0:03:52  lr: 0.000045  loss_ita: 6.3002  loss_itm: 0.6383  loss_lm: 4.6477  time: 0.9372  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2300/2500]  eta: 0:03:05  lr: 0.000045  loss_ita: 6.2824  loss_itm: 0.6529  loss_lm: 4.5341  time: 0.9280  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2350/2500]  eta: 0:02:19  lr: 0.000045  loss_ita: 6.1060  loss_itm: 0.6350  loss_lm: 4.9516  time: 0.9269  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2400/2500]  eta: 0:01:32  lr: 0.000045  loss_ita: 6.1870  loss_itm: 0.6261  loss_lm: 5.1146  time: 0.9306  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2450/2500]  eta: 0:00:46  lr: 0.000045  loss_ita: 6.0545  loss_itm: 0.6384  loss_lm: 4.0758  time: 0.9333  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2499/2500]  eta: 0:00:00  lr: 0.000045  loss_ita: 6.0094  loss_itm: 0.6447  loss_lm: 4.6467  time: 0.9193  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18] Total time: 0:38:43 (0.9293 s / it)\n",
            "Averaged stats: lr: 0.0000  loss_ita: 6.0553  loss_itm: 0.6362  loss_lm: 4.5673\n",
            "Train Epoch: [19]  [   0/2500]  eta: 1:33:40  lr: 0.000041  loss_ita: 6.0785  loss_itm: 0.6439  loss_lm: 3.9600  time: 2.2481  data: 1.0550  max mem: 7989\n",
            "Train Epoch: [19]  [  50/2500]  eta: 0:40:28  lr: 0.000041  loss_ita: 6.1363  loss_itm: 0.6342  loss_lm: 4.7595  time: 0.9812  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [19]  [ 100/2500]  eta: 0:38:56  lr: 0.000041  loss_ita: 6.1894  loss_itm: 0.6489  loss_lm: 4.6459  time: 0.9512  data: 0.0005  max mem: 7989\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}