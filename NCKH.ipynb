{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "04a4c546-20c1-4485-a726-afcb9dc053d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "8da58f64-960e-46ca-f4c0-a7714ee81829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.25)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cu128)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.12-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.25.0+cu128)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.5)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from cog) (2.32.4)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.41.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.20 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
            "Downloading cog-0.16.12-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.52.1\n",
            "    Uninstalling starlette-0.52.1:\n",
            "      Successfully uninstalled starlette-0.52.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.133.0\n",
            "    Uninstalling fastapi-0.133.0:\n",
            "      Successfully uninstalled fastapi-0.133.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.25.1 requires fastapi<1.0.0,>=0.124.1, but you have fastapi 0.118.3 which is incompatible.\n",
            "google-adk 1.25.1 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "sse-starlette 3.2.0 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.12 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.24.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.16.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.1\n",
            "    Uninstalling huggingface_hub-1.4.1:\n",
            "      Successfully uninstalled huggingface_hub-1.4.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.3 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.25.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.4->timm==0.4.12) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.25\n",
            "    Uninstalling timm-1.0.25:\n",
            "      Successfully uninstalled timm-1.0.25\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.10.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.8.0->fairscale==0.4.4) (1.3.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292936 sha256=8bc4fb0c39b89677d6aed1ce1c1ab0043e4969e9427bac93abdfa2bb6ef8c9a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "# TextVQA (phải chạy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "fec35312-2531-4400-8d94-b1f1017574db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-28 07:53:01--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.76.35, 65.8.76.47, 65.8.76.89, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.76.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M   213MB/s    in 0.5s    \n",
            "\n",
            "2026-02-28 07:53:02 (213 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-02-28 07:53:02--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.76.35, 65.8.76.47, 65.8.76.89, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.76.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-02-28 07:53:02 (189 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-02-28 07:53:02--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.76.35, 65.8.76.47, 65.8.76.89, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.76.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2026-02-28 07:53:03 (189 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "db1108a6-25d1-47d5-d7a9-0ccaab28f5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-28 07:53:03--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.76.35, 65.8.76.47, 65.8.76.89, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.76.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G   240MB/s    in 36s     \n",
            "\n",
            "2026-02-28 07:53:40 (186 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (Dhuy + HNhien)\n"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "990237e9-1cb2-4d51-fdf7-97249401f1d1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 318kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 2.84MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.26MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 4.30MB/s]\n",
            "100% 1.35G/1.35G [00:36<00:00, 39.2MB/s]\n",
            "reshape position embedding from 900 to 196\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "<All keys matched successfully>\n",
            "=> loading checkpoint './output/textVQA/checkpoint_04.pth'\n",
            "=> loaded checkpoint './output/textVQA/checkpoint_04.pth' (epoch 5)\n",
            "Start training\n",
            "Loss: 5.8001\n",
            "==========================================\n",
            "Train Epoch: [5]  [   0/8650]  eta: 13:56:33  lr: 0.000019  loss: 5.8001  time: 5.8027  data: 1.8266  max mem: 6922\n",
            "Loss: 9.716\n",
            "==========================================\n",
            "Loss: 6.8981\n",
            "==========================================\n",
            "Loss: 11.0914\n",
            "==========================================\n",
            "Loss: 7.5408\n",
            "==========================================\n",
            "Loss: 8.2925\n",
            "==========================================\n",
            "Loss: 7.6599\n",
            "==========================================\n",
            "Loss: 7.5884\n",
            "==========================================\n",
            "Loss: 7.7234\n",
            "==========================================\n",
            "Loss: 11.2942\n",
            "==========================================\n",
            "Loss: 10.0351\n",
            "==========================================\n",
            "Loss: 9.1664\n",
            "==========================================\n",
            "Loss: 6.1788\n",
            "==========================================\n",
            "Loss: 9.1275\n",
            "==========================================\n",
            "Loss: 4.9309\n",
            "==========================================\n",
            "Loss: 13.3417\n",
            "==========================================\n",
            "Loss: 8.64\n",
            "==========================================\n",
            "Loss: 6.0007\n",
            "==========================================\n",
            "Loss: 6.9533\n",
            "==========================================\n",
            "Loss: 12.9262\n",
            "==========================================\n",
            "Loss: 14.4791\n",
            "==========================================\n",
            "Loss: 7.4321\n",
            "==========================================\n",
            "Loss: 9.2772\n",
            "==========================================\n",
            "Loss: 9.4475\n",
            "==========================================\n",
            "Loss: 6.576\n",
            "==========================================\n",
            "Loss: 7.4088\n",
            "==========================================\n",
            "Loss: 5.924\n",
            "==========================================\n",
            "Loss: 8.2728\n",
            "==========================================\n",
            "Loss: 5.0004\n",
            "==========================================\n",
            "Loss: 9.5953\n",
            "==========================================\n",
            "Loss: 15.6227\n",
            "==========================================\n",
            "Loss: 14.5672\n",
            "==========================================\n",
            "Loss: 8.6229\n",
            "==========================================\n",
            "Loss: 7.8427\n",
            "==========================================\n",
            "Loss: 6.0376\n",
            "==========================================\n",
            "Loss: 6.5138\n",
            "==========================================\n",
            "Loss: 15.387\n",
            "==========================================\n",
            "Loss: 13.0894\n",
            "==========================================\n",
            "Loss: 5.5321\n",
            "==========================================\n",
            "Loss: 11.557\n",
            "==========================================\n",
            "Loss: 5.2967\n",
            "==========================================\n",
            "Loss: 9.8019\n",
            "==========================================\n",
            "Loss: 7.5374\n",
            "==========================================\n",
            "Loss: 8.2649\n",
            "==========================================\n",
            "Loss: 3.8398\n",
            "==========================================\n",
            "Loss: 8.6245\n",
            "==========================================\n",
            "Loss: 7.2302\n",
            "==========================================\n",
            "Loss: 11.3509\n",
            "==========================================\n",
            "Loss: 7.7063\n",
            "==========================================\n",
            "Loss: 13.948\n",
            "==========================================\n",
            "Train Epoch: [5]  [  50/8650]  eta: 1:23:06  lr: 0.000019  loss: 6.5669  time: 0.5259  data: 0.0004  max mem: 6940\n",
            "Train Epoch: [5]  [ 100/8650]  eta: 1:15:48  lr: 0.000019  loss: 13.0420  time: 0.5086  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 150/8650]  eta: 1:13:07  lr: 0.000019  loss: 7.7572  time: 0.5102  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 200/8650]  eta: 1:11:55  lr: 0.000019  loss: 6.3999  time: 0.5104  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [ 250/8650]  eta: 1:11:38  lr: 0.000019  loss: 6.9745  time: 0.5484  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 300/8650]  eta: 1:11:18  lr: 0.000019  loss: 6.8175  time: 0.5664  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 350/8650]  eta: 1:10:45  lr: 0.000019  loss: 8.9320  time: 0.5760  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 400/8650]  eta: 1:10:11  lr: 0.000019  loss: 7.4691  time: 0.5815  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 450/8650]  eta: 1:09:46  lr: 0.000019  loss: 7.3811  time: 0.5944  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 500/8650]  eta: 1:09:20  lr: 0.000019  loss: 10.0244  time: 0.5978  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [ 550/8650]  eta: 1:08:51  lr: 0.000019  loss: 6.7450  time: 0.6100  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 600/8650]  eta: 1:08:20  lr: 0.000019  loss: 10.3296  time: 0.6078  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [ 650/8650]  eta: 1:07:51  lr: 0.000019  loss: 6.3503  time: 0.6034  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 700/8650]  eta: 1:07:21  lr: 0.000019  loss: 9.6846  time: 0.5871  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [ 750/8650]  eta: 1:06:52  lr: 0.000019  loss: 7.7588  time: 0.5706  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 800/8650]  eta: 1:06:27  lr: 0.000019  loss: 12.9199  time: 0.5448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 850/8650]  eta: 1:05:59  lr: 0.000019  loss: 6.7244  time: 0.5184  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 900/8650]  eta: 1:05:32  lr: 0.000019  loss: 12.3290  time: 0.5120  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [ 950/8650]  eta: 1:05:07  lr: 0.000019  loss: 9.2821  time: 0.5008  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1000/8650]  eta: 1:04:46  lr: 0.000019  loss: 11.6311  time: 0.4925  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [1050/8650]  eta: 1:04:24  lr: 0.000019  loss: 14.2759  time: 0.4724  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1100/8650]  eta: 1:04:04  lr: 0.000019  loss: 11.5028  time: 0.4728  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1150/8650]  eta: 1:03:39  lr: 0.000019  loss: 6.8373  time: 0.4336  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1200/8650]  eta: 1:03:18  lr: 0.000019  loss: 8.3390  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1250/8650]  eta: 1:02:57  lr: 0.000019  loss: 13.0444  time: 0.4186  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1300/8650]  eta: 1:02:38  lr: 0.000019  loss: 5.6193  time: 0.4346  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1350/8650]  eta: 1:02:20  lr: 0.000019  loss: 7.8417  time: 0.4524  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1400/8650]  eta: 1:01:54  lr: 0.000019  loss: 8.8899  time: 0.4293  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1450/8650]  eta: 1:01:27  lr: 0.000019  loss: 4.0739  time: 0.4370  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1500/8650]  eta: 1:01:01  lr: 0.000019  loss: 17.6428  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1550/8650]  eta: 1:00:35  lr: 0.000019  loss: 12.5670  time: 0.4484  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1600/8650]  eta: 1:00:13  lr: 0.000019  loss: 12.3315  time: 0.4765  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1650/8650]  eta: 0:59:50  lr: 0.000019  loss: 12.8724  time: 0.4895  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1700/8650]  eta: 0:59:29  lr: 0.000019  loss: 11.0204  time: 0.5196  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1750/8650]  eta: 0:59:07  lr: 0.000019  loss: 7.0482  time: 0.5089  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [1800/8650]  eta: 0:58:42  lr: 0.000019  loss: 5.9717  time: 0.5100  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [1850/8650]  eta: 0:58:15  lr: 0.000019  loss: 5.3410  time: 0.5197  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [1900/8650]  eta: 0:57:49  lr: 0.000019  loss: 13.1554  time: 0.5389  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1950/8650]  eta: 0:57:23  lr: 0.000019  loss: 10.8936  time: 0.5527  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2000/8650]  eta: 0:56:57  lr: 0.000019  loss: 10.2741  time: 0.5775  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2050/8650]  eta: 0:56:31  lr: 0.000019  loss: 7.2414  time: 0.5873  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2100/8650]  eta: 0:56:03  lr: 0.000019  loss: 8.1972  time: 0.5622  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2150/8650]  eta: 0:55:37  lr: 0.000019  loss: 7.2263  time: 0.5933  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2200/8650]  eta: 0:55:13  lr: 0.000019  loss: 7.5388  time: 0.6028  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [2250/8650]  eta: 0:54:51  lr: 0.000019  loss: 8.6495  time: 0.6497  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [2300/8650]  eta: 0:54:28  lr: 0.000019  loss: 7.3926  time: 0.6350  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [2350/8650]  eta: 0:54:02  lr: 0.000019  loss: 10.0805  time: 0.6181  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [2400/8650]  eta: 0:53:37  lr: 0.000019  loss: 10.5668  time: 0.6124  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2450/8650]  eta: 0:53:10  lr: 0.000019  loss: 9.1756  time: 0.5858  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2500/8650]  eta: 0:52:43  lr: 0.000019  loss: 6.5217  time: 0.5670  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2550/8650]  eta: 0:52:16  lr: 0.000019  loss: 6.7275  time: 0.5485  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2600/8650]  eta: 0:51:49  lr: 0.000019  loss: 10.4781  time: 0.5373  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2650/8650]  eta: 0:51:21  lr: 0.000019  loss: 7.5381  time: 0.5328  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2700/8650]  eta: 0:50:58  lr: 0.000019  loss: 6.9949  time: 0.5363  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2750/8650]  eta: 0:50:33  lr: 0.000019  loss: 5.7036  time: 0.5248  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2800/8650]  eta: 0:50:07  lr: 0.000019  loss: 9.6470  time: 0.5206  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2850/8650]  eta: 0:49:44  lr: 0.000019  loss: 8.1270  time: 0.5238  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2900/8650]  eta: 0:49:18  lr: 0.000019  loss: 7.5041  time: 0.4999  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2950/8650]  eta: 0:48:53  lr: 0.000019  loss: 15.6734  time: 0.4843  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3000/8650]  eta: 0:48:28  lr: 0.000019  loss: 9.4343  time: 0.4923  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3050/8650]  eta: 0:48:02  lr: 0.000019  loss: 8.1490  time: 0.4766  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3100/8650]  eta: 0:47:37  lr: 0.000019  loss: 4.2241  time: 0.4654  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3150/8650]  eta: 0:47:12  lr: 0.000019  loss: 4.9721  time: 0.4393  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3200/8650]  eta: 0:46:47  lr: 0.000019  loss: 11.1032  time: 0.4469  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3250/8650]  eta: 0:46:23  lr: 0.000019  loss: 10.6466  time: 0.4287  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [3300/8650]  eta: 0:45:58  lr: 0.000019  loss: 6.7168  time: 0.4254  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3350/8650]  eta: 0:45:33  lr: 0.000019  loss: 8.0026  time: 0.4346  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [3400/8650]  eta: 0:45:09  lr: 0.000019  loss: 6.1973  time: 0.4417  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3450/8650]  eta: 0:44:44  lr: 0.000019  loss: 9.8794  time: 0.4472  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3500/8650]  eta: 0:44:18  lr: 0.000019  loss: 6.4894  time: 0.4504  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3550/8650]  eta: 0:43:53  lr: 0.000019  loss: 7.6040  time: 0.4544  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3600/8650]  eta: 0:43:29  lr: 0.000019  loss: 15.9298  time: 0.4735  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3650/8650]  eta: 0:43:04  lr: 0.000019  loss: 15.0736  time: 0.4742  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3700/8650]  eta: 0:42:40  lr: 0.000019  loss: 16.7557  time: 0.4924  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3750/8650]  eta: 0:42:15  lr: 0.000019  loss: 6.6279  time: 0.5070  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3800/8650]  eta: 0:41:50  lr: 0.000019  loss: 11.6900  time: 0.5189  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3850/8650]  eta: 0:41:24  lr: 0.000019  loss: 7.2965  time: 0.5285  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3900/8650]  eta: 0:41:00  lr: 0.000019  loss: 8.7488  time: 0.5694  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3950/8650]  eta: 0:40:34  lr: 0.000019  loss: 6.1661  time: 0.5743  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4000/8650]  eta: 0:40:09  lr: 0.000019  loss: 9.7439  time: 0.5927  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [4050/8650]  eta: 0:39:44  lr: 0.000019  loss: 10.9126  time: 0.6131  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4100/8650]  eta: 0:39:19  lr: 0.000019  loss: 10.9178  time: 0.6126  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [4150/8650]  eta: 0:38:52  lr: 0.000019  loss: 11.4628  time: 0.5910  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4200/8650]  eta: 0:38:26  lr: 0.000019  loss: 4.1876  time: 0.6003  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4250/8650]  eta: 0:37:59  lr: 0.000019  loss: 5.8990  time: 0.6130  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [4300/8650]  eta: 0:37:32  lr: 0.000019  loss: 9.9709  time: 0.6150  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [4350/8650]  eta: 0:37:06  lr: 0.000019  loss: 10.3060  time: 0.6194  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4400/8650]  eta: 0:36:40  lr: 0.000019  loss: 8.5342  time: 0.5904  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [4450/8650]  eta: 0:36:13  lr: 0.000019  loss: 13.9831  time: 0.5661  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4500/8650]  eta: 0:35:46  lr: 0.000019  loss: 10.3032  time: 0.5467  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4550/8650]  eta: 0:35:20  lr: 0.000019  loss: 7.5555  time: 0.5394  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [4600/8650]  eta: 0:34:53  lr: 0.000019  loss: 7.5189  time: 0.5232  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [4650/8650]  eta: 0:34:26  lr: 0.000019  loss: 13.0161  time: 0.5114  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4700/8650]  eta: 0:34:00  lr: 0.000019  loss: 5.1385  time: 0.5011  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [4750/8650]  eta: 0:33:33  lr: 0.000019  loss: 9.9114  time: 0.4828  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4800/8650]  eta: 0:33:07  lr: 0.000019  loss: 8.4540  time: 0.4782  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4850/8650]  eta: 0:32:40  lr: 0.000019  loss: 7.5223  time: 0.4580  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4900/8650]  eta: 0:32:14  lr: 0.000019  loss: 6.4215  time: 0.4570  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4950/8650]  eta: 0:31:48  lr: 0.000019  loss: 9.9241  time: 0.4298  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5000/8650]  eta: 0:31:21  lr: 0.000019  loss: 9.1050  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5050/8650]  eta: 0:30:55  lr: 0.000019  loss: 7.5348  time: 0.4142  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5100/8650]  eta: 0:30:29  lr: 0.000019  loss: 13.2453  time: 0.4175  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5150/8650]  eta: 0:30:04  lr: 0.000019  loss: 7.7833  time: 0.4332  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5200/8650]  eta: 0:29:38  lr: 0.000019  loss: 12.0841  time: 0.4438  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5250/8650]  eta: 0:29:13  lr: 0.000019  loss: 8.3652  time: 0.4555  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5300/8650]  eta: 0:28:48  lr: 0.000019  loss: 8.7505  time: 0.4643  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [5350/8650]  eta: 0:28:23  lr: 0.000019  loss: 10.2043  time: 0.4756  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5400/8650]  eta: 0:27:58  lr: 0.000019  loss: 8.3261  time: 0.5006  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [5450/8650]  eta: 0:27:33  lr: 0.000019  loss: 10.4547  time: 0.5227  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [5500/8650]  eta: 0:27:07  lr: 0.000019  loss: 9.9565  time: 0.5320  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [5550/8650]  eta: 0:26:42  lr: 0.000019  loss: 7.7064  time: 0.5513  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5600/8650]  eta: 0:26:17  lr: 0.000019  loss: 10.2593  time: 0.5694  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5650/8650]  eta: 0:25:51  lr: 0.000019  loss: 7.4164  time: 0.5953  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5700/8650]  eta: 0:25:26  lr: 0.000019  loss: 8.8054  time: 0.6096  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5750/8650]  eta: 0:25:00  lr: 0.000019  loss: 9.9639  time: 0.6186  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5800/8650]  eta: 0:24:35  lr: 0.000019  loss: 5.9324  time: 0.6418  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5850/8650]  eta: 0:24:10  lr: 0.000019  loss: 13.9653  time: 0.6556  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5900/8650]  eta: 0:23:44  lr: 0.000019  loss: 6.1673  time: 0.6438  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5950/8650]  eta: 0:23:18  lr: 0.000019  loss: 7.3268  time: 0.6355  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [6000/8650]  eta: 0:22:53  lr: 0.000019  loss: 4.7842  time: 0.6215  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [6050/8650]  eta: 0:22:27  lr: 0.000019  loss: 8.7514  time: 0.5870  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [6100/8650]  eta: 0:22:01  lr: 0.000019  loss: 14.5427  time: 0.5523  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [6150/8650]  eta: 0:21:34  lr: 0.000019  loss: 11.0418  time: 0.5364  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [6200/8650]  eta: 0:21:08  lr: 0.000019  loss: 14.7763  time: 0.5252  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [6250/8650]  eta: 0:20:42  lr: 0.000019  loss: 13.9391  time: 0.5198  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [6300/8650]  eta: 0:20:16  lr: 0.000019  loss: 10.6665  time: 0.5135  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6350/8650]  eta: 0:19:49  lr: 0.000019  loss: 13.5960  time: 0.4958  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6400/8650]  eta: 0:19:24  lr: 0.000019  loss: 13.2406  time: 0.5181  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6450/8650]  eta: 0:18:58  lr: 0.000019  loss: 11.3838  time: 0.5038  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6500/8650]  eta: 0:18:32  lr: 0.000019  loss: 8.4652  time: 0.4820  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6550/8650]  eta: 0:18:06  lr: 0.000019  loss: 10.0051  time: 0.4848  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6600/8650]  eta: 0:17:40  lr: 0.000019  loss: 9.1695  time: 0.4576  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6650/8650]  eta: 0:17:15  lr: 0.000019  loss: 5.9478  time: 0.4333  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6700/8650]  eta: 0:16:49  lr: 0.000019  loss: 11.1746  time: 0.4134  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6750/8650]  eta: 0:16:23  lr: 0.000019  loss: 4.6233  time: 0.4133  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6800/8650]  eta: 0:15:57  lr: 0.000019  loss: 8.4037  time: 0.4134  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6850/8650]  eta: 0:15:31  lr: 0.000019  loss: 11.2519  time: 0.4258  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6900/8650]  eta: 0:15:05  lr: 0.000019  loss: 6.1735  time: 0.4248  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6950/8650]  eta: 0:14:39  lr: 0.000019  loss: 10.5685  time: 0.4271  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7000/8650]  eta: 0:14:13  lr: 0.000019  loss: 13.1472  time: 0.4486  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7050/8650]  eta: 0:13:47  lr: 0.000019  loss: 11.1049  time: 0.4634  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7100/8650]  eta: 0:13:21  lr: 0.000019  loss: 5.6737  time: 0.4829  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7150/8650]  eta: 0:12:55  lr: 0.000019  loss: 11.2305  time: 0.4883  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7200/8650]  eta: 0:12:30  lr: 0.000019  loss: 11.7382  time: 0.5042  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7250/8650]  eta: 0:12:04  lr: 0.000019  loss: 17.4972  time: 0.5311  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7300/8650]  eta: 0:11:38  lr: 0.000019  loss: 6.7846  time: 0.5309  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7350/8650]  eta: 0:11:13  lr: 0.000019  loss: 12.0840  time: 0.5516  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7400/8650]  eta: 0:10:47  lr: 0.000019  loss: 10.3018  time: 0.5663  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [7450/8650]  eta: 0:10:21  lr: 0.000019  loss: 8.5857  time: 0.5824  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7500/8650]  eta: 0:09:55  lr: 0.000019  loss: 9.5705  time: 0.5816  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7550/8650]  eta: 0:09:29  lr: 0.000019  loss: 6.4759  time: 0.5942  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7600/8650]  eta: 0:09:04  lr: 0.000019  loss: 5.9648  time: 0.6044  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7650/8650]  eta: 0:08:38  lr: 0.000019  loss: 9.3134  time: 0.6216  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7700/8650]  eta: 0:08:12  lr: 0.000019  loss: 6.7390  time: 0.6012  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7750/8650]  eta: 0:07:46  lr: 0.000019  loss: 7.9263  time: 0.6254  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7800/8650]  eta: 0:07:20  lr: 0.000019  loss: 9.3983  time: 0.6356  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7850/8650]  eta: 0:06:54  lr: 0.000019  loss: 13.6575  time: 0.6503  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7900/8650]  eta: 0:06:29  lr: 0.000019  loss: 14.2111  time: 0.6448  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7950/8650]  eta: 0:06:03  lr: 0.000019  loss: 9.6984  time: 0.6384  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [5]  [8000/8650]  eta: 0:05:37  lr: 0.000019  loss: 12.9771  time: 0.6075  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [8050/8650]  eta: 0:05:11  lr: 0.000019  loss: 8.3484  time: 0.5863  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8100/8650]  eta: 0:04:45  lr: 0.000019  loss: 12.4707  time: 0.5753  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8150/8650]  eta: 0:04:19  lr: 0.000019  loss: 14.6489  time: 0.5601  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8200/8650]  eta: 0:03:53  lr: 0.000019  loss: 8.5347  time: 0.5685  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8250/8650]  eta: 0:03:27  lr: 0.000019  loss: 7.9667  time: 0.5585  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8300/8650]  eta: 0:03:01  lr: 0.000019  loss: 9.9880  time: 0.5407  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8350/8650]  eta: 0:02:35  lr: 0.000019  loss: 8.9232  time: 0.5252  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8400/8650]  eta: 0:02:09  lr: 0.000019  loss: 16.0629  time: 0.5297  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [8450/8650]  eta: 0:01:43  lr: 0.000019  loss: 7.1970  time: 0.5019  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [8500/8650]  eta: 0:01:17  lr: 0.000019  loss: 11.9363  time: 0.4975  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [8550/8650]  eta: 0:00:51  lr: 0.000019  loss: 17.6459  time: 0.4789  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [8600/8650]  eta: 0:00:25  lr: 0.000019  loss: 10.9015  time: 0.4596  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [8649/8650]  eta: 0:00:00  lr: 0.000019  loss: 7.7335  time: 0.4459  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5] Total time: 1:14:49 (0.5190 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 9.3163\n",
            "Loss: 8.5265\n",
            "==========================================\n",
            "Train Epoch: [6]  [   0/8650]  eta: 6:35:13  lr: 0.000018  loss: 8.5265  time: 2.7414  data: 1.5243  max mem: 9086\n",
            "Loss: 13.2796\n",
            "==========================================\n",
            "Loss: 10.7466\n",
            "==========================================\n",
            "Loss: 12.8492\n",
            "==========================================\n",
            "Loss: 6.5873\n",
            "==========================================\n",
            "Loss: 5.8761\n",
            "==========================================\n",
            "Loss: 11.7641\n",
            "==========================================\n",
            "Loss: 5.3718\n",
            "==========================================\n",
            "Loss: 5.311\n",
            "==========================================\n",
            "Loss: 7.885\n",
            "==========================================\n",
            "Loss: 8.124\n",
            "==========================================\n",
            "Loss: 6.9498\n",
            "==========================================\n",
            "Loss: 8.8575\n",
            "==========================================\n",
            "Loss: 10.786\n",
            "==========================================\n",
            "Loss: 7.7356\n",
            "==========================================\n",
            "Loss: 5.1139\n",
            "==========================================\n",
            "Loss: 9.3516\n",
            "==========================================\n",
            "Loss: 7.1849\n",
            "==========================================\n",
            "Loss: 9.7251\n",
            "==========================================\n",
            "Loss: 15.0498\n",
            "==========================================\n",
            "Loss: 6.34\n",
            "==========================================\n",
            "Loss: 7.4213\n",
            "==========================================\n",
            "Loss: 6.9484\n",
            "==========================================\n",
            "Loss: 10.7638\n",
            "==========================================\n",
            "Loss: 11.435\n",
            "==========================================\n",
            "Loss: 5.8783\n",
            "==========================================\n",
            "Loss: 11.3849\n",
            "==========================================\n",
            "Loss: 10.2778\n",
            "==========================================\n",
            "Loss: 10.2428\n",
            "==========================================\n",
            "Loss: 6.8818\n",
            "==========================================\n",
            "Loss: 6.2717\n",
            "==========================================\n",
            "Loss: 17.319\n",
            "==========================================\n",
            "Loss: 10.3455\n",
            "==========================================\n",
            "Loss: 7.1199\n",
            "==========================================\n",
            "Loss: 7.3411\n",
            "==========================================\n",
            "Loss: 9.74\n",
            "==========================================\n",
            "Loss: 9.8956\n",
            "==========================================\n",
            "Loss: 6.724\n",
            "==========================================\n",
            "Loss: 13.6002\n",
            "==========================================\n",
            "Loss: 7.9394\n",
            "==========================================\n",
            "Loss: 7.7329\n",
            "==========================================\n",
            "Loss: 4.757\n",
            "==========================================\n",
            "Loss: 8.9845\n",
            "==========================================\n",
            "Loss: 10.1575\n",
            "==========================================\n",
            "Loss: 7.9443\n",
            "==========================================\n",
            "Loss: 14.2508\n",
            "==========================================\n",
            "Loss: 7.7219\n",
            "==========================================\n",
            "Loss: 9.8364\n",
            "==========================================\n",
            "Loss: 6.7814\n",
            "==========================================\n",
            "Loss: 9.2709\n",
            "==========================================\n",
            "Train Epoch: [6]  [  50/8650]  eta: 1:45:25  lr: 0.000018  loss: 13.3541  time: 0.8149  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [ 100/8650]  eta: 1:30:57  lr: 0.000018  loss: 11.9387  time: 0.6552  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 150/8650]  eta: 1:25:58  lr: 0.000018  loss: 6.7050  time: 0.6738  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 200/8650]  eta: 1:22:50  lr: 0.000018  loss: 7.3543  time: 0.6405  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 250/8650]  eta: 1:20:45  lr: 0.000018  loss: 8.4298  time: 0.6164  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [ 300/8650]  eta: 1:19:12  lr: 0.000018  loss: 7.1672  time: 0.5955  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [ 350/8650]  eta: 1:17:57  lr: 0.000018  loss: 6.9104  time: 0.5702  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [ 400/8650]  eta: 1:16:22  lr: 0.000018  loss: 4.6862  time: 0.5394  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [ 450/8650]  eta: 1:15:00  lr: 0.000018  loss: 10.0569  time: 0.5256  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 500/8650]  eta: 1:13:54  lr: 0.000018  loss: 7.2182  time: 0.4985  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [ 550/8650]  eta: 1:12:54  lr: 0.000018  loss: 9.0042  time: 0.4912  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 600/8650]  eta: 1:12:00  lr: 0.000018  loss: 4.3407  time: 0.4953  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [ 650/8650]  eta: 1:11:12  lr: 0.000018  loss: 9.1411  time: 0.4847  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [ 700/8650]  eta: 1:10:27  lr: 0.000018  loss: 12.0635  time: 0.4665  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 750/8650]  eta: 1:09:45  lr: 0.000018  loss: 7.3661  time: 0.4568  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 800/8650]  eta: 1:09:04  lr: 0.000018  loss: 7.4012  time: 0.4246  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [ 850/8650]  eta: 1:08:23  lr: 0.000018  loss: 12.2747  time: 0.4119  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [ 900/8650]  eta: 1:07:44  lr: 0.000018  loss: 7.6412  time: 0.4104  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [ 950/8650]  eta: 1:07:09  lr: 0.000018  loss: 9.5868  time: 0.4108  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1000/8650]  eta: 1:06:33  lr: 0.000018  loss: 11.3633  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1050/8650]  eta: 1:06:00  lr: 0.000018  loss: 7.1431  time: 0.4203  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1100/8650]  eta: 1:05:30  lr: 0.000018  loss: 8.5856  time: 0.4390  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1150/8650]  eta: 1:04:59  lr: 0.000018  loss: 6.2946  time: 0.4346  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1200/8650]  eta: 1:04:27  lr: 0.000018  loss: 3.9348  time: 0.4370  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1250/8650]  eta: 1:03:59  lr: 0.000018  loss: 11.4983  time: 0.4590  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [1300/8650]  eta: 1:03:32  lr: 0.000018  loss: 14.4251  time: 0.4642  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1350/8650]  eta: 1:03:04  lr: 0.000018  loss: 9.3808  time: 0.4792  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1400/8650]  eta: 1:02:36  lr: 0.000018  loss: 12.4232  time: 0.4791  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1450/8650]  eta: 1:02:09  lr: 0.000018  loss: 9.9928  time: 0.5014  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [1500/8650]  eta: 1:01:40  lr: 0.000018  loss: 12.5362  time: 0.5059  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [1550/8650]  eta: 1:01:14  lr: 0.000018  loss: 7.3687  time: 0.5299  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1600/8650]  eta: 1:00:56  lr: 0.000018  loss: 9.2208  time: 0.6046  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1650/8650]  eta: 1:00:30  lr: 0.000018  loss: 8.1765  time: 0.5367  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [1700/8650]  eta: 1:00:05  lr: 0.000018  loss: 14.6416  time: 0.5716  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1750/8650]  eta: 0:59:39  lr: 0.000018  loss: 8.7103  time: 0.5795  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [1800/8650]  eta: 0:59:12  lr: 0.000018  loss: 7.5587  time: 0.5784  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [1850/8650]  eta: 0:58:48  lr: 0.000018  loss: 9.9871  time: 0.5810  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [1900/8650]  eta: 0:58:21  lr: 0.000018  loss: 5.9160  time: 0.5888  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [1950/8650]  eta: 0:57:57  lr: 0.000018  loss: 6.9598  time: 0.6066  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2000/8650]  eta: 0:57:29  lr: 0.000018  loss: 10.1281  time: 0.5929  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [2050/8650]  eta: 0:57:01  lr: 0.000018  loss: 11.6278  time: 0.5959  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2100/8650]  eta: 0:56:33  lr: 0.000018  loss: 9.0626  time: 0.6038  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2150/8650]  eta: 0:56:05  lr: 0.000018  loss: 12.1998  time: 0.6106  data: 0.0017  max mem: 9086\n",
            "Train Epoch: [6]  [2200/8650]  eta: 0:55:39  lr: 0.000018  loss: 7.9931  time: 0.5993  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2250/8650]  eta: 0:55:10  lr: 0.000018  loss: 8.4814  time: 0.5742  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2300/8650]  eta: 0:54:42  lr: 0.000018  loss: 8.0301  time: 0.5677  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2350/8650]  eta: 0:54:23  lr: 0.000018  loss: 6.1511  time: 0.6390  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [2400/8650]  eta: 0:53:57  lr: 0.000018  loss: 13.6800  time: 0.5423  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2450/8650]  eta: 0:53:32  lr: 0.000018  loss: 8.6549  time: 0.5547  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2500/8650]  eta: 0:53:07  lr: 0.000018  loss: 13.3026  time: 0.5402  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2550/8650]  eta: 0:52:41  lr: 0.000018  loss: 6.4222  time: 0.5194  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [2600/8650]  eta: 0:52:15  lr: 0.000018  loss: 7.1482  time: 0.5209  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [2650/8650]  eta: 0:51:48  lr: 0.000018  loss: 6.2789  time: 0.4918  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [2700/8650]  eta: 0:51:20  lr: 0.000018  loss: 7.9309  time: 0.4781  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2750/8650]  eta: 0:50:53  lr: 0.000018  loss: 10.2314  time: 0.4525  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2800/8650]  eta: 0:50:26  lr: 0.000018  loss: 10.2724  time: 0.4562  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [2850/8650]  eta: 0:49:58  lr: 0.000018  loss: 10.3259  time: 0.4408  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [2900/8650]  eta: 0:49:31  lr: 0.000018  loss: 8.8291  time: 0.4313  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2950/8650]  eta: 0:49:04  lr: 0.000018  loss: 12.2788  time: 0.4200  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3000/8650]  eta: 0:48:38  lr: 0.000018  loss: 9.9890  time: 0.4126  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3050/8650]  eta: 0:48:11  lr: 0.000018  loss: 6.2730  time: 0.4167  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3100/8650]  eta: 0:47:44  lr: 0.000018  loss: 6.5066  time: 0.4110  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3150/8650]  eta: 0:47:17  lr: 0.000018  loss: 8.5855  time: 0.4259  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3200/8650]  eta: 0:46:50  lr: 0.000018  loss: 13.7952  time: 0.4293  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [3250/8650]  eta: 0:46:24  lr: 0.000018  loss: 12.5473  time: 0.4441  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3300/8650]  eta: 0:45:57  lr: 0.000018  loss: 6.3941  time: 0.4529  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3350/8650]  eta: 0:45:32  lr: 0.000018  loss: 15.7937  time: 0.4848  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [3400/8650]  eta: 0:45:07  lr: 0.000018  loss: 6.9773  time: 0.4940  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3450/8650]  eta: 0:44:41  lr: 0.000018  loss: 10.8067  time: 0.4900  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [3500/8650]  eta: 0:44:17  lr: 0.000018  loss: 10.9630  time: 0.5418  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3550/8650]  eta: 0:43:51  lr: 0.000018  loss: 7.9786  time: 0.5299  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [3600/8650]  eta: 0:43:26  lr: 0.000018  loss: 8.9087  time: 0.5333  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3650/8650]  eta: 0:43:03  lr: 0.000018  loss: 8.1983  time: 0.5778  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3700/8650]  eta: 0:42:39  lr: 0.000018  loss: 7.9853  time: 0.5775  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3750/8650]  eta: 0:42:13  lr: 0.000018  loss: 6.8639  time: 0.5704  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3800/8650]  eta: 0:41:46  lr: 0.000018  loss: 8.8785  time: 0.5567  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [3850/8650]  eta: 0:41:20  lr: 0.000018  loss: 14.1508  time: 0.5823  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [3900/8650]  eta: 0:40:53  lr: 0.000018  loss: 13.1841  time: 0.5716  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3950/8650]  eta: 0:40:27  lr: 0.000018  loss: 6.7390  time: 0.5984  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4000/8650]  eta: 0:40:01  lr: 0.000018  loss: 6.3210  time: 0.6040  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [4050/8650]  eta: 0:39:34  lr: 0.000018  loss: 6.2748  time: 0.6162  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6]  [4100/8650]  eta: 0:39:08  lr: 0.000018  loss: 13.2526  time: 0.6201  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4150/8650]  eta: 0:38:42  lr: 0.000018  loss: 10.2891  time: 0.6091  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [4200/8650]  eta: 0:38:15  lr: 0.000018  loss: 9.6157  time: 0.5907  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4250/8650]  eta: 0:37:49  lr: 0.000018  loss: 6.3298  time: 0.5792  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4300/8650]  eta: 0:37:22  lr: 0.000018  loss: 11.6028  time: 0.5656  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [4350/8650]  eta: 0:36:56  lr: 0.000018  loss: 7.3120  time: 0.5609  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4400/8650]  eta: 0:36:29  lr: 0.000018  loss: 10.4844  time: 0.5484  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4450/8650]  eta: 0:36:03  lr: 0.000018  loss: 9.5898  time: 0.5257  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [4500/8650]  eta: 0:35:37  lr: 0.000018  loss: 13.0422  time: 0.5212  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4550/8650]  eta: 0:35:10  lr: 0.000018  loss: 8.6806  time: 0.5040  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4600/8650]  eta: 0:34:44  lr: 0.000018  loss: 12.9434  time: 0.4998  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4650/8650]  eta: 0:34:18  lr: 0.000018  loss: 9.8023  time: 0.4729  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4700/8650]  eta: 0:33:53  lr: 0.000018  loss: 14.7364  time: 0.4816  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4750/8650]  eta: 0:33:28  lr: 0.000018  loss: 11.4415  time: 0.4635  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4800/8650]  eta: 0:33:03  lr: 0.000018  loss: 9.6640  time: 0.4472  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4850/8650]  eta: 0:32:36  lr: 0.000018  loss: 10.6265  time: 0.4278  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [4900/8650]  eta: 0:32:10  lr: 0.000018  loss: 11.4844  time: 0.4235  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [4950/8650]  eta: 0:31:45  lr: 0.000018  loss: 11.5718  time: 0.4222  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5000/8650]  eta: 0:31:19  lr: 0.000018  loss: 5.2528  time: 0.4250  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5050/8650]  eta: 0:30:53  lr: 0.000018  loss: 7.3147  time: 0.4178  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5100/8650]  eta: 0:30:28  lr: 0.000018  loss: 8.4810  time: 0.4185  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5150/8650]  eta: 0:30:02  lr: 0.000018  loss: 17.4687  time: 0.4217  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [5200/8650]  eta: 0:29:36  lr: 0.000018  loss: 6.3156  time: 0.4260  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5250/8650]  eta: 0:29:11  lr: 0.000018  loss: 10.9219  time: 0.4475  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5300/8650]  eta: 0:28:46  lr: 0.000018  loss: 8.1697  time: 0.4351  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [5350/8650]  eta: 0:28:20  lr: 0.000018  loss: 9.3666  time: 0.4365  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5400/8650]  eta: 0:27:54  lr: 0.000018  loss: 6.4004  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5450/8650]  eta: 0:27:27  lr: 0.000018  loss: 12.0481  time: 0.4506  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5500/8650]  eta: 0:27:02  lr: 0.000018  loss: 13.0492  time: 0.4635  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5550/8650]  eta: 0:26:36  lr: 0.000018  loss: 8.5694  time: 0.4606  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [5600/8650]  eta: 0:26:09  lr: 0.000018  loss: 8.8211  time: 0.4691  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5650/8650]  eta: 0:25:43  lr: 0.000018  loss: 7.3662  time: 0.4750  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5700/8650]  eta: 0:25:18  lr: 0.000018  loss: 7.6419  time: 0.5053  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [5750/8650]  eta: 0:24:52  lr: 0.000018  loss: 6.4156  time: 0.5256  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [5800/8650]  eta: 0:24:26  lr: 0.000018  loss: 13.9443  time: 0.5448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [5850/8650]  eta: 0:24:00  lr: 0.000018  loss: 6.4287  time: 0.5609  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [5900/8650]  eta: 0:23:34  lr: 0.000018  loss: 6.2620  time: 0.5677  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [5950/8650]  eta: 0:23:08  lr: 0.000018  loss: 12.2994  time: 0.5723  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6000/8650]  eta: 0:22:43  lr: 0.000018  loss: 7.4848  time: 0.6484  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6050/8650]  eta: 0:22:17  lr: 0.000018  loss: 10.7408  time: 0.5905  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [6100/8650]  eta: 0:21:52  lr: 0.000018  loss: 8.8045  time: 0.6055  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6150/8650]  eta: 0:21:26  lr: 0.000018  loss: 9.8758  time: 0.6303  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6200/8650]  eta: 0:21:01  lr: 0.000018  loss: 6.4068  time: 0.6324  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6250/8650]  eta: 0:20:34  lr: 0.000018  loss: 7.8222  time: 0.5852  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [6]  [6300/8650]  eta: 0:20:09  lr: 0.000018  loss: 16.7939  time: 0.6969  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6350/8650]  eta: 0:19:44  lr: 0.000018  loss: 10.2340  time: 0.6277  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6400/8650]  eta: 0:19:18  lr: 0.000018  loss: 10.7532  time: 0.6204  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6450/8650]  eta: 0:18:52  lr: 0.000018  loss: 6.0227  time: 0.6059  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6]  [6500/8650]  eta: 0:18:26  lr: 0.000018  loss: 10.8508  time: 0.5858  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6550/8650]  eta: 0:18:00  lr: 0.000018  loss: 5.5268  time: 0.5792  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [6600/8650]  eta: 0:17:34  lr: 0.000018  loss: 12.0824  time: 0.5673  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6650/8650]  eta: 0:17:08  lr: 0.000018  loss: 7.8516  time: 0.5544  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [6700/8650]  eta: 0:16:42  lr: 0.000018  loss: 12.0964  time: 0.5338  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6750/8650]  eta: 0:16:16  lr: 0.000018  loss: 8.9048  time: 0.5193  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [6800/8650]  eta: 0:15:51  lr: 0.000018  loss: 9.2116  time: 0.4996  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [6850/8650]  eta: 0:15:25  lr: 0.000018  loss: 10.3504  time: 0.5092  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [6900/8650]  eta: 0:14:59  lr: 0.000018  loss: 11.3653  time: 0.5142  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [6950/8650]  eta: 0:14:34  lr: 0.000018  loss: 8.4748  time: 0.5056  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [7000/8650]  eta: 0:14:08  lr: 0.000018  loss: 4.6866  time: 0.5153  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [7050/8650]  eta: 0:13:43  lr: 0.000018  loss: 10.3888  time: 0.4957  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7100/8650]  eta: 0:13:17  lr: 0.000018  loss: 7.0534  time: 0.4899  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7150/8650]  eta: 0:12:51  lr: 0.000018  loss: 10.9717  time: 0.4529  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7200/8650]  eta: 0:12:25  lr: 0.000018  loss: 5.4373  time: 0.4433  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [7250/8650]  eta: 0:12:00  lr: 0.000018  loss: 11.2938  time: 0.4248  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7300/8650]  eta: 0:11:34  lr: 0.000018  loss: 10.5430  time: 0.4192  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7350/8650]  eta: 0:11:08  lr: 0.000018  loss: 7.7703  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7400/8650]  eta: 0:10:42  lr: 0.000018  loss: 8.9021  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7450/8650]  eta: 0:10:17  lr: 0.000018  loss: 9.2109  time: 0.4185  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7500/8650]  eta: 0:09:51  lr: 0.000018  loss: 17.5883  time: 0.4323  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7550/8650]  eta: 0:09:25  lr: 0.000018  loss: 8.1363  time: 0.4283  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7600/8650]  eta: 0:08:59  lr: 0.000018  loss: 8.0516  time: 0.4304  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7650/8650]  eta: 0:08:34  lr: 0.000018  loss: 11.3057  time: 0.4404  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7700/8650]  eta: 0:08:08  lr: 0.000018  loss: 9.0966  time: 0.4331  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7750/8650]  eta: 0:07:42  lr: 0.000018  loss: 12.7919  time: 0.4518  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7800/8650]  eta: 0:07:17  lr: 0.000018  loss: 10.5155  time: 0.4815  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7850/8650]  eta: 0:06:51  lr: 0.000018  loss: 7.0916  time: 0.4793  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [7900/8650]  eta: 0:06:25  lr: 0.000018  loss: 8.8058  time: 0.4891  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7950/8650]  eta: 0:05:59  lr: 0.000018  loss: 10.5038  time: 0.4902  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [8000/8650]  eta: 0:05:34  lr: 0.000018  loss: 8.6845  time: 0.5130  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [8050/8650]  eta: 0:05:08  lr: 0.000018  loss: 12.3904  time: 0.5249  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [8100/8650]  eta: 0:04:42  lr: 0.000018  loss: 8.1986  time: 0.5361  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [8150/8650]  eta: 0:04:17  lr: 0.000018  loss: 11.8347  time: 0.5497  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [8200/8650]  eta: 0:03:51  lr: 0.000018  loss: 7.0175  time: 0.5818  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8250/8650]  eta: 0:03:25  lr: 0.000018  loss: 6.0770  time: 0.5743  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [8300/8650]  eta: 0:02:59  lr: 0.000018  loss: 16.2899  time: 0.5704  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [8350/8650]  eta: 0:02:34  lr: 0.000018  loss: 17.2887  time: 0.5879  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8400/8650]  eta: 0:02:08  lr: 0.000018  loss: 5.7699  time: 0.5802  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8450/8650]  eta: 0:01:42  lr: 0.000018  loss: 5.9327  time: 0.5905  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [8500/8650]  eta: 0:01:17  lr: 0.000018  loss: 10.1223  time: 0.5973  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8550/8650]  eta: 0:00:51  lr: 0.000018  loss: 9.3807  time: 0.6159  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [8600/8650]  eta: 0:00:25  lr: 0.000018  loss: 6.4628  time: 0.6374  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8649/8650]  eta: 0:00:00  lr: 0.000018  loss: 4.6984  time: 0.5974  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6] Total time: 1:14:06 (0.5140 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 9.3507\n",
            "Loss: 6.9651\n",
            "==========================================\n",
            "Train Epoch: [7]  [   0/8650]  eta: 4:39:52  lr: 0.000017  loss: 6.9651  time: 1.9413  data: 1.0819  max mem: 9086\n",
            "Loss: 6.277\n",
            "==========================================\n",
            "Loss: 7.9919\n",
            "==========================================\n",
            "Loss: 6.9872\n",
            "==========================================\n",
            "Loss: 11.1805\n",
            "==========================================\n",
            "Loss: 11.1639\n",
            "==========================================\n",
            "Loss: 7.1333\n",
            "==========================================\n",
            "Loss: 9.8996\n",
            "==========================================\n",
            "Loss: 8.6578\n",
            "==========================================\n",
            "Loss: 9.949\n",
            "==========================================\n",
            "Loss: 9.9788\n",
            "==========================================\n",
            "Loss: 11.8081\n",
            "==========================================\n",
            "Loss: 6.0883\n",
            "==========================================\n",
            "Loss: 5.7889\n",
            "==========================================\n",
            "Loss: 5.851\n",
            "==========================================\n",
            "Loss: 8.0898\n",
            "==========================================\n",
            "Loss: 8.8398\n",
            "==========================================\n",
            "Loss: 9.528\n",
            "==========================================\n",
            "Loss: 7.0136\n",
            "==========================================\n",
            "Loss: 8.9172\n",
            "==========================================\n",
            "Loss: 9.6133\n",
            "==========================================\n",
            "Loss: 10.6327\n",
            "==========================================\n",
            "Loss: 11.8055\n",
            "==========================================\n",
            "Loss: 7.1087\n",
            "==========================================\n",
            "Loss: 4.8396\n",
            "==========================================\n",
            "Loss: 9.556\n",
            "==========================================\n",
            "Loss: 5.1909\n",
            "==========================================\n",
            "Loss: 6.8896\n",
            "==========================================\n",
            "Loss: 13.6629\n",
            "==========================================\n",
            "Loss: 10.4571\n",
            "==========================================\n",
            "Loss: 14.1672\n",
            "==========================================\n",
            "Loss: 8.9223\n",
            "==========================================\n",
            "Loss: 7.6069\n",
            "==========================================\n",
            "Loss: 6.8588\n",
            "==========================================\n",
            "Loss: 11.2001\n",
            "==========================================\n",
            "Loss: 4.6759\n",
            "==========================================\n",
            "Loss: 7.478\n",
            "==========================================\n",
            "Loss: 11.3764\n",
            "==========================================\n",
            "Loss: 6.7273\n",
            "==========================================\n",
            "Loss: 5.8912\n",
            "==========================================\n",
            "Loss: 8.2311\n",
            "==========================================\n",
            "Loss: 7.3096\n",
            "==========================================\n",
            "Loss: 7.0351\n",
            "==========================================\n",
            "Loss: 7.0354\n",
            "==========================================\n",
            "Loss: 4.2508\n",
            "==========================================\n",
            "Loss: 9.9736\n",
            "==========================================\n",
            "Loss: 7.7842\n",
            "==========================================\n",
            "Loss: 8.4671\n",
            "==========================================\n",
            "Loss: 9.1234\n",
            "==========================================\n",
            "Loss: 8.9679\n",
            "==========================================\n",
            "Train Epoch: [7]  [  50/8650]  eta: 1:34:31  lr: 0.000017  loss: 9.4938  time: 0.6230  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 100/8650]  eta: 1:26:14  lr: 0.000017  loss: 7.8777  time: 0.5508  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 150/8650]  eta: 1:22:14  lr: 0.000017  loss: 14.1560  time: 0.5329  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [ 200/8650]  eta: 1:20:13  lr: 0.000017  loss: 10.4455  time: 0.5493  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 250/8650]  eta: 1:19:49  lr: 0.000017  loss: 8.0043  time: 0.6273  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [ 300/8650]  eta: 1:18:31  lr: 0.000017  loss: 5.6494  time: 0.5790  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 350/8650]  eta: 1:17:01  lr: 0.000017  loss: 9.2160  time: 0.5757  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 400/8650]  eta: 1:15:50  lr: 0.000017  loss: 5.1543  time: 0.6001  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 450/8650]  eta: 1:14:49  lr: 0.000017  loss: 11.4146  time: 0.5993  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [ 500/8650]  eta: 1:13:50  lr: 0.000017  loss: 11.8593  time: 0.6063  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 550/8650]  eta: 1:13:01  lr: 0.000017  loss: 6.9364  time: 0.6259  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [ 600/8650]  eta: 1:12:12  lr: 0.000017  loss: 14.3055  time: 0.6094  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 650/8650]  eta: 1:11:26  lr: 0.000017  loss: 9.8270  time: 0.6096  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 700/8650]  eta: 1:10:40  lr: 0.000017  loss: 12.9913  time: 0.5873  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 750/8650]  eta: 1:10:02  lr: 0.000017  loss: 8.3696  time: 0.5844  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 800/8650]  eta: 1:09:24  lr: 0.000017  loss: 5.7487  time: 0.5592  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 850/8650]  eta: 1:08:44  lr: 0.000017  loss: 7.6529  time: 0.5524  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 900/8650]  eta: 1:08:06  lr: 0.000017  loss: 8.4002  time: 0.5589  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 950/8650]  eta: 1:07:32  lr: 0.000017  loss: 9.0978  time: 0.5519  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [1000/8650]  eta: 1:06:58  lr: 0.000017  loss: 5.9277  time: 0.5336  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [1050/8650]  eta: 1:06:26  lr: 0.000017  loss: 13.2498  time: 0.5274  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [1100/8650]  eta: 1:05:52  lr: 0.000017  loss: 6.7690  time: 0.5139  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [1150/8650]  eta: 1:05:23  lr: 0.000017  loss: 8.2555  time: 0.5237  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [1200/8650]  eta: 1:04:51  lr: 0.000017  loss: 4.7379  time: 0.4800  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [1250/8650]  eta: 1:04:21  lr: 0.000017  loss: 15.5486  time: 0.4790  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [1300/8650]  eta: 1:03:52  lr: 0.000017  loss: 11.3983  time: 0.4825  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [1350/8650]  eta: 1:03:22  lr: 0.000017  loss: 9.8996  time: 0.4519  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [1400/8650]  eta: 1:02:53  lr: 0.000017  loss: 8.1814  time: 0.4346  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1450/8650]  eta: 1:02:24  lr: 0.000017  loss: 13.3549  time: 0.4237  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1500/8650]  eta: 1:01:55  lr: 0.000017  loss: 7.6535  time: 0.4144  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1550/8650]  eta: 1:01:27  lr: 0.000017  loss: 9.1274  time: 0.4236  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1600/8650]  eta: 1:00:59  lr: 0.000017  loss: 9.0033  time: 0.4282  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1650/8650]  eta: 1:00:30  lr: 0.000017  loss: 8.9940  time: 0.4156  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1700/8650]  eta: 1:00:03  lr: 0.000017  loss: 10.1996  time: 0.4244  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1750/8650]  eta: 0:59:34  lr: 0.000017  loss: 10.9605  time: 0.4208  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1800/8650]  eta: 0:59:08  lr: 0.000017  loss: 12.2057  time: 0.4526  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1850/8650]  eta: 0:58:46  lr: 0.000017  loss: 6.5209  time: 0.4680  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [1900/8650]  eta: 0:58:21  lr: 0.000017  loss: 6.6092  time: 0.4518  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1950/8650]  eta: 0:57:54  lr: 0.000017  loss: 5.7882  time: 0.4555  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [2000/8650]  eta: 0:57:28  lr: 0.000017  loss: 7.4042  time: 0.4774  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [2050/8650]  eta: 0:57:04  lr: 0.000017  loss: 8.5893  time: 0.5080  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [2100/8650]  eta: 0:56:37  lr: 0.000017  loss: 7.0648  time: 0.4843  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [2150/8650]  eta: 0:56:12  lr: 0.000017  loss: 13.8695  time: 0.5001  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [2200/8650]  eta: 0:55:47  lr: 0.000017  loss: 9.3586  time: 0.5120  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [2250/8650]  eta: 0:55:21  lr: 0.000017  loss: 8.6169  time: 0.5239  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [2300/8650]  eta: 0:54:56  lr: 0.000017  loss: 5.0426  time: 0.5447  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [2350/8650]  eta: 0:54:30  lr: 0.000017  loss: 9.3662  time: 0.5527  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [2400/8650]  eta: 0:54:04  lr: 0.000017  loss: 14.6053  time: 0.5687  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2450/8650]  eta: 0:53:37  lr: 0.000017  loss: 6.1953  time: 0.5702  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2500/8650]  eta: 0:53:10  lr: 0.000017  loss: 8.1212  time: 0.5844  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2550/8650]  eta: 0:52:46  lr: 0.000017  loss: 8.0249  time: 0.6375  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2600/8650]  eta: 0:52:23  lr: 0.000017  loss: 10.1520  time: 0.6374  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2650/8650]  eta: 0:51:57  lr: 0.000017  loss: 9.6214  time: 0.6015  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2700/8650]  eta: 0:51:32  lr: 0.000017  loss: 11.4041  time: 0.6424  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [2750/8650]  eta: 0:51:06  lr: 0.000017  loss: 7.8220  time: 0.6314  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2800/8650]  eta: 0:50:40  lr: 0.000017  loss: 8.8305  time: 0.6213  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2850/8650]  eta: 0:50:12  lr: 0.000017  loss: 8.1588  time: 0.5955  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2900/8650]  eta: 0:49:46  lr: 0.000017  loss: 10.8983  time: 0.5787  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2950/8650]  eta: 0:49:19  lr: 0.000017  loss: 10.6230  time: 0.5855  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3000/8650]  eta: 0:48:52  lr: 0.000017  loss: 8.6140  time: 0.5613  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [3050/8650]  eta: 0:48:25  lr: 0.000017  loss: 7.5831  time: 0.5498  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3100/8650]  eta: 0:47:59  lr: 0.000017  loss: 8.7567  time: 0.5587  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [3150/8650]  eta: 0:47:35  lr: 0.000017  loss: 9.7593  time: 0.5770  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [3200/8650]  eta: 0:47:08  lr: 0.000017  loss: 6.4451  time: 0.5219  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3250/8650]  eta: 0:46:42  lr: 0.000017  loss: 5.4734  time: 0.5256  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [3300/8650]  eta: 0:46:16  lr: 0.000017  loss: 5.7588  time: 0.5242  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3350/8650]  eta: 0:45:50  lr: 0.000017  loss: 12.5059  time: 0.5164  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [3400/8650]  eta: 0:45:24  lr: 0.000017  loss: 8.6301  time: 0.5013  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3450/8650]  eta: 0:44:58  lr: 0.000017  loss: 8.5589  time: 0.4717  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3500/8650]  eta: 0:44:31  lr: 0.000017  loss: 8.1924  time: 0.4652  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3550/8650]  eta: 0:44:05  lr: 0.000017  loss: 5.1823  time: 0.4546  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3600/8650]  eta: 0:43:38  lr: 0.000017  loss: 5.7140  time: 0.4241  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3650/8650]  eta: 0:43:12  lr: 0.000017  loss: 6.6396  time: 0.4168  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3700/8650]  eta: 0:42:45  lr: 0.000017  loss: 11.1873  time: 0.4169  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3750/8650]  eta: 0:42:19  lr: 0.000017  loss: 6.4641  time: 0.4297  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3800/8650]  eta: 0:41:53  lr: 0.000017  loss: 6.9850  time: 0.4224  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3850/8650]  eta: 0:41:27  lr: 0.000017  loss: 10.8541  time: 0.4471  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3900/8650]  eta: 0:41:01  lr: 0.000017  loss: 9.5829  time: 0.4361  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3950/8650]  eta: 0:40:36  lr: 0.000017  loss: 7.1696  time: 0.4541  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [4000/8650]  eta: 0:40:11  lr: 0.000017  loss: 12.0305  time: 0.4386  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4050/8650]  eta: 0:39:46  lr: 0.000017  loss: 9.6940  time: 0.4528  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4100/8650]  eta: 0:39:22  lr: 0.000017  loss: 11.6511  time: 0.4497  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4150/8650]  eta: 0:38:56  lr: 0.000017  loss: 10.2520  time: 0.4409  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4200/8650]  eta: 0:38:29  lr: 0.000017  loss: 7.6948  time: 0.4517  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4250/8650]  eta: 0:38:03  lr: 0.000017  loss: 6.1810  time: 0.4637  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4300/8650]  eta: 0:37:37  lr: 0.000017  loss: 11.4234  time: 0.4674  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [4350/8650]  eta: 0:37:11  lr: 0.000017  loss: 11.9392  time: 0.4679  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4400/8650]  eta: 0:36:45  lr: 0.000017  loss: 8.1058  time: 0.4949  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4450/8650]  eta: 0:36:19  lr: 0.000017  loss: 12.1712  time: 0.4813  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4500/8650]  eta: 0:35:54  lr: 0.000017  loss: 8.4324  time: 0.5065  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [4550/8650]  eta: 0:35:29  lr: 0.000017  loss: 9.6282  time: 0.5226  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [4600/8650]  eta: 0:35:02  lr: 0.000017  loss: 8.9540  time: 0.5151  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4650/8650]  eta: 0:34:36  lr: 0.000017  loss: 9.9487  time: 0.5369  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [4700/8650]  eta: 0:34:12  lr: 0.000017  loss: 8.7334  time: 0.6207  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [4750/8650]  eta: 0:33:47  lr: 0.000017  loss: 10.8660  time: 0.5867  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4800/8650]  eta: 0:33:21  lr: 0.000017  loss: 7.2010  time: 0.5622  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [4850/8650]  eta: 0:32:55  lr: 0.000017  loss: 5.4725  time: 0.5826  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4900/8650]  eta: 0:32:29  lr: 0.000017  loss: 8.2172  time: 0.5801  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [4950/8650]  eta: 0:32:03  lr: 0.000017  loss: 8.2459  time: 0.5813  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5000/8650]  eta: 0:31:36  lr: 0.000017  loss: 5.4753  time: 0.5755  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [5050/8650]  eta: 0:31:10  lr: 0.000017  loss: 10.0510  time: 0.5747  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5100/8650]  eta: 0:30:44  lr: 0.000017  loss: 12.5888  time: 0.5814  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5150/8650]  eta: 0:30:18  lr: 0.000017  loss: 12.9244  time: 0.6302  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [5200/8650]  eta: 0:29:52  lr: 0.000017  loss: 9.7436  time: 0.6222  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5250/8650]  eta: 0:29:27  lr: 0.000017  loss: 9.1175  time: 0.6645  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5300/8650]  eta: 0:29:02  lr: 0.000017  loss: 11.3688  time: 0.6732  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [5350/8650]  eta: 0:28:36  lr: 0.000017  loss: 9.0789  time: 0.6369  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5400/8650]  eta: 0:28:10  lr: 0.000017  loss: 6.5241  time: 0.6165  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5450/8650]  eta: 0:27:44  lr: 0.000017  loss: 7.6953  time: 0.6139  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5500/8650]  eta: 0:27:17  lr: 0.000017  loss: 6.1571  time: 0.5971  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [5550/8650]  eta: 0:26:51  lr: 0.000017  loss: 8.9268  time: 0.5849  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [5600/8650]  eta: 0:26:25  lr: 0.000017  loss: 11.8150  time: 0.5549  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5650/8650]  eta: 0:25:59  lr: 0.000017  loss: 8.3074  time: 0.5961  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5700/8650]  eta: 0:25:33  lr: 0.000017  loss: 9.2889  time: 0.5790  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5750/8650]  eta: 0:25:08  lr: 0.000017  loss: 8.9520  time: 0.5517  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5800/8650]  eta: 0:24:41  lr: 0.000017  loss: 9.8666  time: 0.5160  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [5850/8650]  eta: 0:24:15  lr: 0.000017  loss: 8.5201  time: 0.4992  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [5900/8650]  eta: 0:23:49  lr: 0.000017  loss: 6.8197  time: 0.4864  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [5950/8650]  eta: 0:23:23  lr: 0.000017  loss: 11.0724  time: 0.4756  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [6000/8650]  eta: 0:22:56  lr: 0.000017  loss: 8.1376  time: 0.4561  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [6050/8650]  eta: 0:22:30  lr: 0.000017  loss: 9.8285  time: 0.4536  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6100/8650]  eta: 0:22:04  lr: 0.000017  loss: 9.5402  time: 0.4467  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6150/8650]  eta: 0:21:38  lr: 0.000017  loss: 9.6295  time: 0.4354  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6200/8650]  eta: 0:21:11  lr: 0.000017  loss: 13.6655  time: 0.4298  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6250/8650]  eta: 0:20:45  lr: 0.000017  loss: 8.1217  time: 0.4218  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6300/8650]  eta: 0:20:19  lr: 0.000017  loss: 7.9863  time: 0.4284  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6350/8650]  eta: 0:19:53  lr: 0.000017  loss: 7.6919  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6400/8650]  eta: 0:19:27  lr: 0.000017  loss: 6.8911  time: 0.4321  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6450/8650]  eta: 0:19:01  lr: 0.000017  loss: 8.7889  time: 0.4349  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6500/8650]  eta: 0:18:35  lr: 0.000017  loss: 12.0165  time: 0.4221  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6550/8650]  eta: 0:18:10  lr: 0.000017  loss: 6.9514  time: 0.4402  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6600/8650]  eta: 0:17:44  lr: 0.000017  loss: 29.2547  time: 0.4394  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6650/8650]  eta: 0:17:18  lr: 0.000017  loss: 7.7061  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6700/8650]  eta: 0:16:52  lr: 0.000017  loss: 10.6540  time: 0.4390  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6750/8650]  eta: 0:16:26  lr: 0.000017  loss: 10.7660  time: 0.4469  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6800/8650]  eta: 0:16:00  lr: 0.000017  loss: 7.9884  time: 0.4509  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6850/8650]  eta: 0:15:34  lr: 0.000017  loss: 6.7048  time: 0.4553  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6900/8650]  eta: 0:15:08  lr: 0.000017  loss: 6.1937  time: 0.4667  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6950/8650]  eta: 0:14:42  lr: 0.000017  loss: 6.4485  time: 0.4743  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [7000/8650]  eta: 0:14:16  lr: 0.000017  loss: 8.9652  time: 0.4897  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7050/8650]  eta: 0:13:50  lr: 0.000017  loss: 8.9966  time: 0.5076  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [7100/8650]  eta: 0:13:24  lr: 0.000017  loss: 11.1333  time: 0.5001  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7150/8650]  eta: 0:12:58  lr: 0.000017  loss: 7.2718  time: 0.5193  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7200/8650]  eta: 0:12:32  lr: 0.000017  loss: 7.2616  time: 0.5358  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [7250/8650]  eta: 0:12:06  lr: 0.000017  loss: 9.2809  time: 0.5684  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7300/8650]  eta: 0:11:40  lr: 0.000017  loss: 9.9385  time: 0.5748  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [7350/8650]  eta: 0:11:14  lr: 0.000017  loss: 9.3384  time: 0.5898  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [7400/8650]  eta: 0:10:48  lr: 0.000017  loss: 8.5349  time: 0.5810  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7450/8650]  eta: 0:10:22  lr: 0.000017  loss: 6.0092  time: 0.5981  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7500/8650]  eta: 0:09:56  lr: 0.000017  loss: 6.2190  time: 0.5970  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [7550/8650]  eta: 0:09:30  lr: 0.000017  loss: 9.1826  time: 0.6046  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7600/8650]  eta: 0:09:04  lr: 0.000017  loss: 9.4467  time: 0.6040  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [7650/8650]  eta: 0:08:38  lr: 0.000017  loss: 11.4337  time: 0.6365  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [7700/8650]  eta: 0:08:12  lr: 0.000017  loss: 10.3359  time: 0.6564  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7750/8650]  eta: 0:07:47  lr: 0.000017  loss: 8.2989  time: 0.6516  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7800/8650]  eta: 0:07:21  lr: 0.000017  loss: 8.5611  time: 0.6185  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7850/8650]  eta: 0:06:55  lr: 0.000017  loss: 9.2754  time: 0.6173  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7900/8650]  eta: 0:06:29  lr: 0.000017  loss: 12.2691  time: 0.5928  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [7950/8650]  eta: 0:06:03  lr: 0.000017  loss: 10.6156  time: 0.5733  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [8000/8650]  eta: 0:05:37  lr: 0.000017  loss: 4.2285  time: 0.5846  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8050/8650]  eta: 0:05:11  lr: 0.000017  loss: 5.5360  time: 0.5643  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8100/8650]  eta: 0:04:45  lr: 0.000017  loss: 7.0580  time: 0.5732  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [8150/8650]  eta: 0:04:19  lr: 0.000017  loss: 8.3257  time: 0.5528  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [8200/8650]  eta: 0:03:53  lr: 0.000017  loss: 9.1498  time: 0.5513  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8250/8650]  eta: 0:03:27  lr: 0.000017  loss: 15.8532  time: 0.5605  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [8300/8650]  eta: 0:03:01  lr: 0.000017  loss: 10.0324  time: 0.5310  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [8350/8650]  eta: 0:02:35  lr: 0.000017  loss: 11.5032  time: 0.4999  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8400/8650]  eta: 0:02:09  lr: 0.000017  loss: 12.5117  time: 0.4729  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [8450/8650]  eta: 0:01:43  lr: 0.000017  loss: 7.2170  time: 0.4714  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8500/8650]  eta: 0:01:17  lr: 0.000017  loss: 9.3437  time: 0.4530  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8550/8650]  eta: 0:00:51  lr: 0.000017  loss: 6.9807  time: 0.4476  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [8600/8650]  eta: 0:00:25  lr: 0.000017  loss: 3.6678  time: 0.4363  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8649/8650]  eta: 0:00:00  lr: 0.000017  loss: 11.4466  time: 0.4419  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7] Total time: 1:14:45 (0.5186 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 8.6721\n",
            "Loss: 8.5887\n",
            "==========================================\n",
            "Train Epoch: [8]  [   0/8650]  eta: 6:18:40  lr: 0.000017  loss: 8.5887  time: 2.6267  data: 1.2901  max mem: 9086\n",
            "Loss: 8.9799\n",
            "==========================================\n",
            "Loss: 11.2287\n",
            "==========================================\n",
            "Loss: 7.6762\n",
            "==========================================\n",
            "Loss: 7.8738\n",
            "==========================================\n",
            "Loss: 5.6159\n",
            "==========================================\n",
            "Loss: 8.7384\n",
            "==========================================\n",
            "Loss: 8.9461\n",
            "==========================================\n",
            "Loss: 8.6301\n",
            "==========================================\n",
            "Loss: 7.6243\n",
            "==========================================\n",
            "Loss: 8.4243\n",
            "==========================================\n",
            "Loss: 8.9897\n",
            "==========================================\n",
            "Loss: 5.8718\n",
            "==========================================\n",
            "Loss: 7.3765\n",
            "==========================================\n",
            "Loss: 8.9717\n",
            "==========================================\n",
            "Loss: 7.6177\n",
            "==========================================\n",
            "Loss: 15.5338\n",
            "==========================================\n",
            "Loss: 9.7865\n",
            "==========================================\n",
            "Loss: 6.7609\n",
            "==========================================\n",
            "Loss: 6.5124\n",
            "==========================================\n",
            "Loss: 7.5373\n",
            "==========================================\n",
            "Loss: 8.0037\n",
            "==========================================\n",
            "Loss: 23.1815\n",
            "==========================================\n",
            "Loss: 6.5641\n",
            "==========================================\n",
            "Loss: 7.6523\n",
            "==========================================\n",
            "Loss: 9.24\n",
            "==========================================\n",
            "Loss: 11.1977\n",
            "==========================================\n",
            "Loss: 6.5934\n",
            "==========================================\n",
            "Loss: 8.002\n",
            "==========================================\n",
            "Loss: 8.6218\n",
            "==========================================\n",
            "Loss: 6.9175\n",
            "==========================================\n",
            "Loss: 6.9062\n",
            "==========================================\n",
            "Loss: 8.6392\n",
            "==========================================\n",
            "Loss: 7.4255\n",
            "==========================================\n",
            "Loss: 8.9188\n",
            "==========================================\n",
            "Loss: 5.6186\n",
            "==========================================\n",
            "Loss: 7.8037\n",
            "==========================================\n",
            "Loss: 5.7252\n",
            "==========================================\n",
            "Loss: 7.6388\n",
            "==========================================\n",
            "Loss: 6.9086\n",
            "==========================================\n",
            "Loss: 8.3265\n",
            "==========================================\n",
            "Loss: 7.7741\n",
            "==========================================\n",
            "Loss: 13.4817\n",
            "==========================================\n",
            "Loss: 9.5316\n",
            "==========================================\n",
            "Loss: 6.1108\n",
            "==========================================\n",
            "Loss: 11.6081\n",
            "==========================================\n",
            "Loss: 7.7169\n",
            "==========================================\n",
            "Loss: 10.3857\n",
            "==========================================\n",
            "Loss: 6.4361\n",
            "==========================================\n",
            "Loss: 6.0008\n",
            "==========================================\n",
            "Train Epoch: [8]  [  50/8650]  eta: 1:30:06  lr: 0.000017  loss: 7.1514  time: 0.6325  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [8]  [ 100/8650]  eta: 1:23:42  lr: 0.000017  loss: 5.6420  time: 0.5567  data: 0.0012  max mem: 9086\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA/checkpoint_04.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train textVQA + pretrain_animal (Khoi)"
      ],
      "metadata": {
        "id": "i9pDWKQLVYL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHhslGC2SmK",
        "collapsed": true,
        "outputId": "7995485d-a751-4606-abde-d7bd9596059c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 277kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 774kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 4.62MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 4.86MB/s]\n",
            "load checkpoint from /content/drive/MyDrive/BLIP/output/pretrain_animals/checkpoint_29.pth\n",
            "_IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_proj_m.weight', 'text_proj_m.bias'])\n",
            "=> loading checkpoint './output/textVQA_pretrain_animals/checkpoint_16.pth'\n",
            "=> loaded checkpoint './output/textVQA_pretrain_animals/checkpoint_16.pth' (epoch 17)\n",
            "Start training\n",
            "Loss: 12.8918\n",
            "==========================================\n",
            "Train Epoch: [17]  [   0/8650]  eta: 10:42:57  lr: 0.000008  loss: 12.8918  time: 4.4598  data: 1.6528  max mem: 6922\n",
            "Loss: 17.6764\n",
            "==========================================\n",
            "Loss: 13.086\n",
            "==========================================\n",
            "Loss: 17.5797\n",
            "==========================================\n",
            "Loss: 16.2041\n",
            "==========================================\n",
            "Loss: 16.1703\n",
            "==========================================\n",
            "Loss: 12.6891\n",
            "==========================================\n",
            "Loss: 17.8512\n",
            "==========================================\n",
            "Loss: 17.532\n",
            "==========================================\n",
            "Loss: 21.466\n",
            "==========================================\n",
            "Loss: 15.8552\n",
            "==========================================\n",
            "Loss: 18.6346\n",
            "==========================================\n",
            "Loss: 12.3462\n",
            "==========================================\n",
            "Loss: 22.282\n",
            "==========================================\n",
            "Loss: 8.6822\n",
            "==========================================\n",
            "Loss: 27.4381\n",
            "==========================================\n",
            "Loss: 13.8011\n",
            "==========================================\n",
            "Loss: 10.2274\n",
            "==========================================\n",
            "Loss: 16.3855\n",
            "==========================================\n",
            "Loss: 29.7147\n",
            "==========================================\n",
            "Loss: 20.4318\n",
            "==========================================\n",
            "Loss: 13.5378\n",
            "==========================================\n",
            "Loss: 20.1847\n",
            "==========================================\n",
            "Loss: 18.484\n",
            "==========================================\n",
            "Loss: 12.4033\n",
            "==========================================\n",
            "Loss: 12.3889\n",
            "==========================================\n",
            "Loss: 14.1558\n",
            "==========================================\n",
            "Loss: 17.1968\n",
            "==========================================\n",
            "Loss: 12.3343\n",
            "==========================================\n",
            "Loss: 18.2867\n",
            "==========================================\n",
            "Loss: 36.4367\n",
            "==========================================\n",
            "Loss: 26.8627\n",
            "==========================================\n",
            "Loss: 14.7644\n",
            "==========================================\n",
            "Loss: 16.1029\n",
            "==========================================\n",
            "Loss: 12.5201\n",
            "==========================================\n",
            "Loss: 12.0129\n",
            "==========================================\n",
            "Loss: 29.4921\n",
            "==========================================\n",
            "Loss: 27.0708\n",
            "==========================================\n",
            "Loss: 16.4506\n",
            "==========================================\n",
            "Loss: 18.1092\n",
            "==========================================\n",
            "Loss: 10.7178\n",
            "==========================================\n",
            "Loss: 22.4364\n",
            "==========================================\n",
            "Loss: 14.023\n",
            "==========================================\n",
            "Loss: 13.7221\n",
            "==========================================\n",
            "Loss: 6.9992\n",
            "==========================================\n",
            "Loss: 14.1729\n",
            "==========================================\n",
            "Loss: 13.4682\n",
            "==========================================\n",
            "Loss: 19.8436\n",
            "==========================================\n",
            "Loss: 9.6216\n",
            "==========================================\n",
            "Loss: 23.7954\n",
            "==========================================\n",
            "Train Epoch: [17]  [  50/8650]  eta: 1:17:58  lr: 0.000008  loss: 8.9493  time: 0.5096  data: 0.0005  max mem: 6940\n",
            "Train Epoch: [17]  [ 100/8650]  eta: 1:12:50  lr: 0.000008  loss: 19.4453  time: 0.5425  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 150/8650]  eta: 1:10:27  lr: 0.000008  loss: 16.0444  time: 0.5598  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 200/8650]  eta: 1:09:23  lr: 0.000008  loss: 15.2044  time: 0.5708  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [ 250/8650]  eta: 1:09:01  lr: 0.000008  loss: 11.9606  time: 0.5746  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [ 300/8650]  eta: 1:08:36  lr: 0.000008  loss: 10.4570  time: 0.5586  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [ 350/8650]  eta: 1:08:04  lr: 0.000008  loss: 18.3770  time: 0.5625  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 400/8650]  eta: 1:07:34  lr: 0.000008  loss: 16.8992  time: 0.5690  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 450/8650]  eta: 1:07:04  lr: 0.000008  loss: 13.6435  time: 0.5697  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 500/8650]  eta: 1:06:39  lr: 0.000008  loss: 17.8929  time: 0.5655  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 550/8650]  eta: 1:06:16  lr: 0.000008  loss: 9.7501  time: 0.5775  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 600/8650]  eta: 1:05:50  lr: 0.000008  loss: 16.9335  time: 0.5654  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 650/8650]  eta: 1:05:25  lr: 0.000008  loss: 9.8833  time: 0.5625  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 700/8650]  eta: 1:04:58  lr: 0.000008  loss: 16.8314  time: 0.5677  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [ 750/8650]  eta: 1:04:31  lr: 0.000008  loss: 17.0048  time: 0.5559  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 800/8650]  eta: 1:04:09  lr: 0.000008  loss: 19.5837  time: 0.5520  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [ 850/8650]  eta: 1:03:43  lr: 0.000008  loss: 11.2518  time: 0.5571  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [ 900/8650]  eta: 1:03:18  lr: 0.000008  loss: 21.0643  time: 0.5555  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [ 950/8650]  eta: 1:02:54  lr: 0.000008  loss: 14.9246  time: 0.5536  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [1000/8650]  eta: 1:02:29  lr: 0.000008  loss: 30.3496  time: 0.5454  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [1050/8650]  eta: 1:02:06  lr: 0.000008  loss: 32.5235  time: 0.5691  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1100/8650]  eta: 1:01:41  lr: 0.000008  loss: 19.6854  time: 0.5358  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [1150/8650]  eta: 1:01:15  lr: 0.000008  loss: 16.6725  time: 0.5313  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [1200/8650]  eta: 1:00:49  lr: 0.000008  loss: 16.7629  time: 0.5330  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1250/8650]  eta: 1:00:27  lr: 0.000008  loss: 24.9831  time: 0.5552  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1300/8650]  eta: 1:00:02  lr: 0.000008  loss: 9.4191  time: 0.5345  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [17]  [1350/8650]  eta: 0:59:37  lr: 0.000008  loss: 14.7763  time: 0.5381  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1400/8650]  eta: 0:59:12  lr: 0.000008  loss: 23.0453  time: 0.5199  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1450/8650]  eta: 0:58:47  lr: 0.000008  loss: 8.9445  time: 0.5201  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [1500/8650]  eta: 0:58:23  lr: 0.000008  loss: 32.0468  time: 0.5196  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [1550/8650]  eta: 0:57:59  lr: 0.000008  loss: 21.7527  time: 0.5238  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1600/8650]  eta: 0:57:34  lr: 0.000008  loss: 22.0330  time: 0.5127  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1650/8650]  eta: 0:57:10  lr: 0.000008  loss: 22.3962  time: 0.5036  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [1700/8650]  eta: 0:56:45  lr: 0.000008  loss: 15.8880  time: 0.4979  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1750/8650]  eta: 0:56:20  lr: 0.000008  loss: 12.6650  time: 0.4991  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [1800/8650]  eta: 0:55:55  lr: 0.000008  loss: 9.4096  time: 0.4863  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [1850/8650]  eta: 0:55:31  lr: 0.000008  loss: 11.8423  time: 0.4870  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [1900/8650]  eta: 0:55:06  lr: 0.000008  loss: 21.6272  time: 0.4806  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [1950/8650]  eta: 0:54:41  lr: 0.000008  loss: 22.5922  time: 0.4811  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [2000/8650]  eta: 0:54:16  lr: 0.000008  loss: 16.8833  time: 0.4777  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [2050/8650]  eta: 0:53:51  lr: 0.000008  loss: 14.2174  time: 0.4786  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [2100/8650]  eta: 0:53:26  lr: 0.000008  loss: 15.0512  time: 0.4712  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [2150/8650]  eta: 0:53:02  lr: 0.000008  loss: 10.9257  time: 0.4705  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [2200/8650]  eta: 0:52:37  lr: 0.000008  loss: 14.6800  time: 0.4744  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2250/8650]  eta: 0:52:13  lr: 0.000008  loss: 13.2121  time: 0.4782  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [2300/8650]  eta: 0:51:49  lr: 0.000008  loss: 18.5317  time: 0.4689  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [2350/8650]  eta: 0:51:25  lr: 0.000008  loss: 17.2915  time: 0.4650  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [2400/8650]  eta: 0:51:01  lr: 0.000008  loss: 15.8548  time: 0.4597  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2450/8650]  eta: 0:50:37  lr: 0.000008  loss: 14.3151  time: 0.4567  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2500/8650]  eta: 0:50:12  lr: 0.000008  loss: 11.7747  time: 0.4578  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [2550/8650]  eta: 0:49:47  lr: 0.000008  loss: 11.5188  time: 0.4522  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [2600/8650]  eta: 0:49:22  lr: 0.000008  loss: 19.3584  time: 0.4496  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [2650/8650]  eta: 0:48:58  lr: 0.000008  loss: 15.8257  time: 0.4506  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2700/8650]  eta: 0:48:33  lr: 0.000008  loss: 11.7182  time: 0.4504  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2750/8650]  eta: 0:48:09  lr: 0.000008  loss: 10.6134  time: 0.4441  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [2800/8650]  eta: 0:47:44  lr: 0.000008  loss: 17.6498  time: 0.4436  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2850/8650]  eta: 0:47:20  lr: 0.000008  loss: 14.2270  time: 0.4395  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2900/8650]  eta: 0:46:55  lr: 0.000008  loss: 12.3817  time: 0.4482  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [2950/8650]  eta: 0:46:30  lr: 0.000008  loss: 32.0968  time: 0.4356  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3000/8650]  eta: 0:46:06  lr: 0.000008  loss: 17.9526  time: 0.4372  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3050/8650]  eta: 0:45:41  lr: 0.000008  loss: 15.6875  time: 0.4387  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3100/8650]  eta: 0:45:17  lr: 0.000008  loss: 8.7606  time: 0.4478  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [3150/8650]  eta: 0:44:53  lr: 0.000008  loss: 7.6811  time: 0.4268  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3200/8650]  eta: 0:44:28  lr: 0.000008  loss: 19.6428  time: 0.4280  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [3250/8650]  eta: 0:44:04  lr: 0.000008  loss: 20.6653  time: 0.4225  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [3300/8650]  eta: 0:43:39  lr: 0.000008  loss: 9.3791  time: 0.4279  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3350/8650]  eta: 0:43:14  lr: 0.000008  loss: 14.2784  time: 0.4271  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3400/8650]  eta: 0:42:50  lr: 0.000008  loss: 15.6102  time: 0.4302  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3450/8650]  eta: 0:42:25  lr: 0.000008  loss: 17.7686  time: 0.4286  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3500/8650]  eta: 0:42:00  lr: 0.000008  loss: 10.6941  time: 0.4269  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [3550/8650]  eta: 0:41:35  lr: 0.000008  loss: 12.0642  time: 0.4233  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3600/8650]  eta: 0:41:11  lr: 0.000008  loss: 24.4511  time: 0.4272  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3650/8650]  eta: 0:40:46  lr: 0.000008  loss: 24.1698  time: 0.4260  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [3700/8650]  eta: 0:40:22  lr: 0.000008  loss: 37.8931  time: 0.4306  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3750/8650]  eta: 0:39:57  lr: 0.000008  loss: 12.3126  time: 0.4174  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [3800/8650]  eta: 0:39:32  lr: 0.000008  loss: 20.9188  time: 0.4186  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [3850/8650]  eta: 0:39:07  lr: 0.000008  loss: 9.8846  time: 0.4275  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3900/8650]  eta: 0:38:43  lr: 0.000008  loss: 17.9013  time: 0.4361  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [3950/8650]  eta: 0:38:18  lr: 0.000008  loss: 12.3737  time: 0.4224  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4000/8650]  eta: 0:37:54  lr: 0.000008  loss: 16.5538  time: 0.4157  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4050/8650]  eta: 0:37:30  lr: 0.000008  loss: 19.1424  time: 0.4173  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4100/8650]  eta: 0:37:05  lr: 0.000008  loss: 19.2239  time: 0.4087  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4150/8650]  eta: 0:36:40  lr: 0.000008  loss: 23.9594  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4200/8650]  eta: 0:36:16  lr: 0.000008  loss: 7.2572  time: 0.4244  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4250/8650]  eta: 0:35:51  lr: 0.000008  loss: 12.6940  time: 0.4168  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [4300/8650]  eta: 0:35:27  lr: 0.000008  loss: 18.0215  time: 0.4143  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4350/8650]  eta: 0:35:03  lr: 0.000008  loss: 16.9637  time: 0.4099  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4400/8650]  eta: 0:34:38  lr: 0.000008  loss: 15.2451  time: 0.4055  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4450/8650]  eta: 0:34:13  lr: 0.000008  loss: 21.9668  time: 0.4095  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4500/8650]  eta: 0:33:49  lr: 0.000008  loss: 15.2831  time: 0.4118  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4550/8650]  eta: 0:33:24  lr: 0.000008  loss: 20.5254  time: 0.4151  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4600/8650]  eta: 0:33:00  lr: 0.000008  loss: 15.2327  time: 0.4104  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4650/8650]  eta: 0:32:35  lr: 0.000008  loss: 21.1157  time: 0.4101  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4700/8650]  eta: 0:32:11  lr: 0.000008  loss: 12.6377  time: 0.4083  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4750/8650]  eta: 0:31:46  lr: 0.000008  loss: 18.9210  time: 0.4107  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4800/8650]  eta: 0:31:22  lr: 0.000008  loss: 15.8913  time: 0.4148  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4850/8650]  eta: 0:30:57  lr: 0.000008  loss: 14.1183  time: 0.4115  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4900/8650]  eta: 0:30:33  lr: 0.000008  loss: 12.2200  time: 0.4153  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [4950/8650]  eta: 0:30:08  lr: 0.000008  loss: 19.0881  time: 0.4069  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [5000/8650]  eta: 0:29:44  lr: 0.000008  loss: 16.4440  time: 0.4136  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5050/8650]  eta: 0:29:19  lr: 0.000008  loss: 15.7131  time: 0.4080  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5100/8650]  eta: 0:28:55  lr: 0.000008  loss: 24.0128  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5150/8650]  eta: 0:28:30  lr: 0.000008  loss: 14.6235  time: 0.4092  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5200/8650]  eta: 0:28:06  lr: 0.000008  loss: 22.5209  time: 0.4116  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5250/8650]  eta: 0:27:41  lr: 0.000008  loss: 17.0619  time: 0.4109  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5300/8650]  eta: 0:27:17  lr: 0.000008  loss: 17.2837  time: 0.4131  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5350/8650]  eta: 0:26:53  lr: 0.000008  loss: 16.4837  time: 0.4129  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5400/8650]  eta: 0:26:28  lr: 0.000008  loss: 15.6386  time: 0.4100  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5450/8650]  eta: 0:26:04  lr: 0.000008  loss: 18.9610  time: 0.4145  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5500/8650]  eta: 0:25:39  lr: 0.000008  loss: 20.0702  time: 0.4099  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5550/8650]  eta: 0:25:15  lr: 0.000008  loss: 15.2921  time: 0.4084  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5600/8650]  eta: 0:24:50  lr: 0.000008  loss: 17.2581  time: 0.4080  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5650/8650]  eta: 0:24:26  lr: 0.000008  loss: 12.9252  time: 0.4108  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5700/8650]  eta: 0:24:01  lr: 0.000008  loss: 19.2961  time: 0.4094  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5750/8650]  eta: 0:23:36  lr: 0.000008  loss: 20.0940  time: 0.4072  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5800/8650]  eta: 0:23:12  lr: 0.000008  loss: 10.7731  time: 0.4118  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5850/8650]  eta: 0:22:47  lr: 0.000008  loss: 23.6580  time: 0.4129  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5900/8650]  eta: 0:22:23  lr: 0.000008  loss: 16.0884  time: 0.4137  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [5950/8650]  eta: 0:21:58  lr: 0.000008  loss: 13.2904  time: 0.4076  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6000/8650]  eta: 0:21:34  lr: 0.000008  loss: 11.7654  time: 0.4103  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6050/8650]  eta: 0:21:10  lr: 0.000008  loss: 15.5153  time: 0.4062  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6100/8650]  eta: 0:20:45  lr: 0.000008  loss: 36.2247  time: 0.4169  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [6150/8650]  eta: 0:20:21  lr: 0.000008  loss: 17.6320  time: 0.4144  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6200/8650]  eta: 0:19:56  lr: 0.000008  loss: 30.4716  time: 0.4124  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6250/8650]  eta: 0:19:32  lr: 0.000008  loss: 25.9428  time: 0.4124  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6300/8650]  eta: 0:19:08  lr: 0.000008  loss: 17.4570  time: 0.4091  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6350/8650]  eta: 0:18:43  lr: 0.000008  loss: 19.4545  time: 0.4106  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6400/8650]  eta: 0:18:19  lr: 0.000008  loss: 24.0049  time: 0.4103  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6450/8650]  eta: 0:17:54  lr: 0.000008  loss: 23.4020  time: 0.4116  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6500/8650]  eta: 0:17:30  lr: 0.000008  loss: 14.3161  time: 0.4183  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6550/8650]  eta: 0:17:05  lr: 0.000008  loss: 18.3010  time: 0.4085  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6600/8650]  eta: 0:16:41  lr: 0.000008  loss: 22.9869  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6650/8650]  eta: 0:16:17  lr: 0.000008  loss: 12.2483  time: 0.4079  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6700/8650]  eta: 0:15:52  lr: 0.000008  loss: 21.1211  time: 0.4140  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6750/8650]  eta: 0:15:28  lr: 0.000008  loss: 7.2327  time: 0.4176  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [6800/8650]  eta: 0:15:03  lr: 0.000008  loss: 17.4482  time: 0.4176  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [6850/8650]  eta: 0:14:39  lr: 0.000008  loss: 15.2043  time: 0.4164  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6900/8650]  eta: 0:14:14  lr: 0.000008  loss: 11.4115  time: 0.4064  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [6950/8650]  eta: 0:13:50  lr: 0.000008  loss: 16.4838  time: 0.4079  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7000/8650]  eta: 0:13:25  lr: 0.000008  loss: 21.7638  time: 0.4106  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7050/8650]  eta: 0:13:01  lr: 0.000008  loss: 20.1505  time: 0.4101  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7100/8650]  eta: 0:12:37  lr: 0.000008  loss: 9.0087  time: 0.4175  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7150/8650]  eta: 0:12:12  lr: 0.000008  loss: 13.9548  time: 0.4184  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [7200/8650]  eta: 0:11:48  lr: 0.000008  loss: 24.9474  time: 0.4173  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7250/8650]  eta: 0:11:23  lr: 0.000008  loss: 30.5203  time: 0.4322  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7300/8650]  eta: 0:10:59  lr: 0.000008  loss: 16.7209  time: 0.4258  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7350/8650]  eta: 0:10:34  lr: 0.000008  loss: 23.2837  time: 0.4306  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7400/8650]  eta: 0:10:10  lr: 0.000008  loss: 15.1857  time: 0.4235  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7450/8650]  eta: 0:09:46  lr: 0.000008  loss: 18.1192  time: 0.4290  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [17]  [7500/8650]  eta: 0:09:21  lr: 0.000008  loss: 21.9308  time: 0.4337  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [7550/8650]  eta: 0:08:57  lr: 0.000008  loss: 13.7209  time: 0.4338  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [7600/8650]  eta: 0:08:32  lr: 0.000008  loss: 16.1060  time: 0.4320  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [7650/8650]  eta: 0:08:08  lr: 0.000008  loss: 16.3322  time: 0.4296  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7700/8650]  eta: 0:07:43  lr: 0.000008  loss: 15.9735  time: 0.4369  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [17]  [7750/8650]  eta: 0:07:19  lr: 0.000008  loss: 15.4747  time: 0.4535  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7800/8650]  eta: 0:06:55  lr: 0.000008  loss: 17.7374  time: 0.4604  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [7850/8650]  eta: 0:06:30  lr: 0.000008  loss: 18.0506  time: 0.4696  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7900/8650]  eta: 0:06:06  lr: 0.000008  loss: 23.0745  time: 0.4602  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [7950/8650]  eta: 0:05:41  lr: 0.000008  loss: 25.2026  time: 0.4605  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [8000/8650]  eta: 0:05:17  lr: 0.000008  loss: 21.0752  time: 0.4675  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [17]  [8050/8650]  eta: 0:04:53  lr: 0.000008  loss: 15.8585  time: 0.5015  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [17]  [8100/8650]  eta: 0:04:28  lr: 0.000008  loss: 22.5042  time: 0.4920  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8150/8650]  eta: 0:04:04  lr: 0.000008  loss: 23.9285  time: 0.4898  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [8200/8650]  eta: 0:03:39  lr: 0.000008  loss: 17.3197  time: 0.4852  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8250/8650]  eta: 0:03:15  lr: 0.000008  loss: 12.3811  time: 0.4956  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8300/8650]  eta: 0:02:51  lr: 0.000008  loss: 17.7524  time: 0.4968  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [17]  [8350/8650]  eta: 0:02:26  lr: 0.000008  loss: 15.1325  time: 0.4992  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8400/8650]  eta: 0:02:02  lr: 0.000008  loss: 32.0818  time: 0.5174  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8450/8650]  eta: 0:01:37  lr: 0.000008  loss: 11.2324  time: 0.5034  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8500/8650]  eta: 0:01:13  lr: 0.000008  loss: 15.9250  time: 0.5063  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8550/8650]  eta: 0:00:48  lr: 0.000008  loss: 29.3400  time: 0.4988  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8600/8650]  eta: 0:00:24  lr: 0.000008  loss: 20.2129  time: 0.5030  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [17]  [8649/8650]  eta: 0:00:00  lr: 0.000008  loss: 12.3575  time: 0.5151  data: 0.0027  max mem: 9086\n",
            "Train Epoch: [17] Total time: 1:10:28 (0.4889 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 17.2264\n",
            "Loss: 17.2405\n",
            "==========================================\n",
            "Train Epoch: [18]  [   0/8650]  eta: 5:21:44  lr: 0.000007  loss: 17.2405  time: 2.2318  data: 1.2665  max mem: 9086\n",
            "Loss: 24.2062\n",
            "==========================================\n",
            "Loss: 20.7421\n",
            "==========================================\n",
            "Loss: 23.3395\n",
            "==========================================\n",
            "Loss: 13.7498\n",
            "==========================================\n",
            "Loss: 10.9578\n",
            "==========================================\n",
            "Loss: 20.6052\n",
            "==========================================\n",
            "Loss: 8.3601\n",
            "==========================================\n",
            "Loss: 10.0254\n",
            "==========================================\n",
            "Loss: 17.0516\n",
            "==========================================\n",
            "Loss: 17.2338\n",
            "==========================================\n",
            "Loss: 8.134\n",
            "==========================================\n",
            "Loss: 16.6184\n",
            "==========================================\n",
            "Loss: 20.0202\n",
            "==========================================\n",
            "Loss: 13.1447\n",
            "==========================================\n",
            "Loss: 10.2131\n",
            "==========================================\n",
            "Loss: 21.219\n",
            "==========================================\n",
            "Loss: 15.9546\n",
            "==========================================\n",
            "Loss: 12.5848\n",
            "==========================================\n",
            "Loss: 27.9697\n",
            "==========================================\n",
            "Loss: 12.8662\n",
            "==========================================\n",
            "Loss: 14.6465\n",
            "==========================================\n",
            "Loss: 15.4678\n",
            "==========================================\n",
            "Loss: 19.5018\n",
            "==========================================\n",
            "Loss: 22.6686\n",
            "==========================================\n",
            "Loss: 15.1438\n",
            "==========================================\n",
            "Loss: 25.4942\n",
            "==========================================\n",
            "Loss: 20.102\n",
            "==========================================\n",
            "Loss: 14.475\n",
            "==========================================\n",
            "Loss: 11.1375\n",
            "==========================================\n",
            "Loss: 15.6817\n",
            "==========================================\n",
            "Loss: 31.7189\n",
            "==========================================\n",
            "Loss: 14.077\n",
            "==========================================\n",
            "Loss: 11.9571\n",
            "==========================================\n",
            "Loss: 12.0068\n",
            "==========================================\n",
            "Loss: 14.8387\n",
            "==========================================\n",
            "Loss: 14.3692\n",
            "==========================================\n",
            "Loss: 15.0549\n",
            "==========================================\n",
            "Loss: 24.225\n",
            "==========================================\n",
            "Loss: 18.3049\n",
            "==========================================\n",
            "Loss: 16.2849\n",
            "==========================================\n",
            "Loss: 11.1518\n",
            "==========================================\n",
            "Loss: 18.3821\n",
            "==========================================\n",
            "Loss: 27.6118\n",
            "==========================================\n",
            "Loss: 10.0139\n",
            "==========================================\n",
            "Loss: 19.1485\n",
            "==========================================\n",
            "Loss: 15.4868\n",
            "==========================================\n",
            "Loss: 18.939\n",
            "==========================================\n",
            "Loss: 9.9645\n",
            "==========================================\n",
            "Loss: 16.7623\n",
            "==========================================\n",
            "Train Epoch: [18]  [  50/8650]  eta: 1:26:38  lr: 0.000007  loss: 33.3382  time: 0.5555  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 100/8650]  eta: 1:20:33  lr: 0.000007  loss: 22.8189  time: 0.4788  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 150/8650]  eta: 1:16:38  lr: 0.000007  loss: 13.0765  time: 0.4840  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [ 200/8650]  eta: 1:14:14  lr: 0.000007  loss: 18.1160  time: 0.4683  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 250/8650]  eta: 1:12:41  lr: 0.000007  loss: 17.3498  time: 0.4569  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [18]  [ 300/8650]  eta: 1:11:39  lr: 0.000007  loss: 18.8965  time: 0.4622  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 350/8650]  eta: 1:10:41  lr: 0.000007  loss: 12.6438  time: 0.4590  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 400/8650]  eta: 1:09:51  lr: 0.000007  loss: 6.2167  time: 0.4538  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 450/8650]  eta: 1:09:09  lr: 0.000007  loss: 19.1681  time: 0.4555  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [ 500/8650]  eta: 1:08:33  lr: 0.000007  loss: 12.3796  time: 0.4578  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 550/8650]  eta: 1:07:58  lr: 0.000007  loss: 14.9352  time: 0.4604  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 600/8650]  eta: 1:07:25  lr: 0.000007  loss: 8.7310  time: 0.4640  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 650/8650]  eta: 1:06:54  lr: 0.000007  loss: 18.4171  time: 0.4714  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 700/8650]  eta: 1:06:22  lr: 0.000007  loss: 19.5834  time: 0.4641  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 750/8650]  eta: 1:06:00  lr: 0.000007  loss: 15.0041  time: 0.4804  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 800/8650]  eta: 1:05:34  lr: 0.000007  loss: 19.1787  time: 0.4704  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 850/8650]  eta: 1:05:14  lr: 0.000007  loss: 21.7787  time: 0.4810  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [ 900/8650]  eta: 1:04:45  lr: 0.000007  loss: 13.3346  time: 0.4593  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [ 950/8650]  eta: 1:04:19  lr: 0.000007  loss: 17.9067  time: 0.4570  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1000/8650]  eta: 1:04:06  lr: 0.000007  loss: 20.2583  time: 0.4643  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [1050/8650]  eta: 1:03:45  lr: 0.000007  loss: 9.7109  time: 0.4840  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1100/8650]  eta: 1:03:25  lr: 0.000007  loss: 12.8939  time: 0.4894  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1150/8650]  eta: 1:03:04  lr: 0.000007  loss: 11.2717  time: 0.5180  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [1200/8650]  eta: 1:02:44  lr: 0.000007  loss: 9.1660  time: 0.4956  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [1250/8650]  eta: 1:02:21  lr: 0.000007  loss: 17.7906  time: 0.5130  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1300/8650]  eta: 1:01:53  lr: 0.000007  loss: 26.8741  time: 0.4884  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1350/8650]  eta: 1:01:25  lr: 0.000007  loss: 19.3665  time: 0.4740  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1400/8650]  eta: 1:00:58  lr: 0.000007  loss: 24.7927  time: 0.4707  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1450/8650]  eta: 1:00:30  lr: 0.000007  loss: 19.7724  time: 0.4793  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1500/8650]  eta: 1:00:01  lr: 0.000007  loss: 24.3296  time: 0.4672  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1550/8650]  eta: 0:59:32  lr: 0.000007  loss: 11.5178  time: 0.4571  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1600/8650]  eta: 0:59:03  lr: 0.000007  loss: 16.9683  time: 0.4555  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1650/8650]  eta: 0:58:35  lr: 0.000007  loss: 14.5328  time: 0.4451  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1700/8650]  eta: 0:58:08  lr: 0.000007  loss: 24.6650  time: 0.4506  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1750/8650]  eta: 0:57:41  lr: 0.000007  loss: 15.2466  time: 0.4582  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [1800/8650]  eta: 0:57:12  lr: 0.000007  loss: 13.9744  time: 0.4445  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [1850/8650]  eta: 0:56:44  lr: 0.000007  loss: 24.3014  time: 0.4297  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [1900/8650]  eta: 0:56:18  lr: 0.000007  loss: 10.0722  time: 0.4342  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [1950/8650]  eta: 0:55:51  lr: 0.000007  loss: 11.1070  time: 0.4382  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2000/8650]  eta: 0:55:25  lr: 0.000007  loss: 24.3722  time: 0.4391  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2050/8650]  eta: 0:54:58  lr: 0.000007  loss: 25.9493  time: 0.4436  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2100/8650]  eta: 0:54:33  lr: 0.000007  loss: 16.3180  time: 0.4350  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2150/8650]  eta: 0:54:06  lr: 0.000007  loss: 25.5537  time: 0.4326  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2200/8650]  eta: 0:53:41  lr: 0.000007  loss: 13.0683  time: 0.4406  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2250/8650]  eta: 0:53:15  lr: 0.000007  loss: 14.3240  time: 0.4401  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2300/8650]  eta: 0:52:48  lr: 0.000007  loss: 19.8128  time: 0.4378  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2350/8650]  eta: 0:52:22  lr: 0.000007  loss: 16.1499  time: 0.4330  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2400/8650]  eta: 0:51:55  lr: 0.000007  loss: 31.0691  time: 0.4289  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2450/8650]  eta: 0:51:29  lr: 0.000007  loss: 18.7501  time: 0.4323  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2500/8650]  eta: 0:51:03  lr: 0.000007  loss: 31.1990  time: 0.4374  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2550/8650]  eta: 0:50:38  lr: 0.000007  loss: 9.7068  time: 0.4441  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2600/8650]  eta: 0:50:12  lr: 0.000007  loss: 16.0400  time: 0.4425  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [2650/8650]  eta: 0:49:46  lr: 0.000007  loss: 21.1997  time: 0.4399  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2700/8650]  eta: 0:49:20  lr: 0.000007  loss: 16.2688  time: 0.4384  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2750/8650]  eta: 0:48:54  lr: 0.000007  loss: 19.8088  time: 0.4396  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [2800/8650]  eta: 0:48:29  lr: 0.000007  loss: 12.5878  time: 0.4433  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [2850/8650]  eta: 0:48:04  lr: 0.000007  loss: 20.7861  time: 0.4384  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [2900/8650]  eta: 0:47:38  lr: 0.000007  loss: 15.4084  time: 0.4491  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [2950/8650]  eta: 0:47:12  lr: 0.000007  loss: 22.8783  time: 0.4387  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [3000/8650]  eta: 0:46:46  lr: 0.000007  loss: 17.6690  time: 0.4276  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [18]  [3050/8650]  eta: 0:46:21  lr: 0.000007  loss: 12.6199  time: 0.4438  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [3100/8650]  eta: 0:45:57  lr: 0.000007  loss: 16.1628  time: 0.4498  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [18]  [3150/8650]  eta: 0:45:37  lr: 0.000007  loss: 19.0693  time: 0.5222  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [18]  [3200/8650]  eta: 0:45:16  lr: 0.000007  loss: 19.3221  time: 0.4991  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [18]  [3250/8650]  eta: 0:44:51  lr: 0.000007  loss: 24.8521  time: 0.4786  data: 0.0005  max mem: 9086\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_animals.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_animals \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA_pretrain_animals/checkpoint_16.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Textvqa + PretrainTextCaps"
      ],
      "metadata": {
        "id": "c1Xp2adpkVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_Textcaps.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_textcaps \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA_pretrain_textcaps/checkpoint_07.pth"
      ],
      "metadata": {
        "id": "9RnYLAjmkcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA + resnet"
      ],
      "metadata": {
        "id": "ZU3eOQrIfwJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/resnet_textvqa.yaml \\\n",
        "    --output_dir ./output/resnet_textvqa \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/resnet_textvqa/checkpoint_01.pth"
      ],
      "metadata": {
        "id": "km8oueVNiPf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/dataset_animals"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ekwts1oGwoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain.py --config configs/pre_animals.yaml --output_dir output/pretrain_animals --checkpoint output/pretrain_animals/checkpoint_22.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5gFi5qxQmH",
        "outputId": "41cbfa9f-fb90-4d33-abf7-62afd1e11954",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating dataset\n",
            "Đang tải dữ liệu từ: /content/dataset_animals/dataset.json\n",
            "number of training samples: 20000\n",
            "Creating model\n",
            "/embeddings/word_embeddings is tied\n",
            "/embeddings/position_embeddings is tied\n",
            "/embeddings/LayerNorm is tied\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "/encoder/layer/2/crossattention/self/query is tied\n",
            "/encoder/layer/2/crossattention/self/key is tied\n",
            "/encoder/layer/2/crossattention/self/value is tied\n",
            "/encoder/layer/2/crossattention/output/dense is tied\n",
            "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/2/intermediate/dense is tied\n",
            "/encoder/layer/2/output/dense is tied\n",
            "/encoder/layer/2/output/LayerNorm is tied\n",
            "/encoder/layer/3/crossattention/self/query is tied\n",
            "/encoder/layer/3/crossattention/self/key is tied\n",
            "/encoder/layer/3/crossattention/self/value is tied\n",
            "/encoder/layer/3/crossattention/output/dense is tied\n",
            "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/3/intermediate/dense is tied\n",
            "/encoder/layer/3/output/dense is tied\n",
            "/encoder/layer/3/output/LayerNorm is tied\n",
            "/encoder/layer/4/crossattention/self/query is tied\n",
            "/encoder/layer/4/crossattention/self/key is tied\n",
            "/encoder/layer/4/crossattention/self/value is tied\n",
            "/encoder/layer/4/crossattention/output/dense is tied\n",
            "/encoder/layer/4/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/4/intermediate/dense is tied\n",
            "/encoder/layer/4/output/dense is tied\n",
            "/encoder/layer/4/output/LayerNorm is tied\n",
            "/encoder/layer/5/crossattention/self/query is tied\n",
            "/encoder/layer/5/crossattention/self/key is tied\n",
            "/encoder/layer/5/crossattention/self/value is tied\n",
            "/encoder/layer/5/crossattention/output/dense is tied\n",
            "/encoder/layer/5/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/5/intermediate/dense is tied\n",
            "/encoder/layer/5/output/dense is tied\n",
            "/encoder/layer/5/output/LayerNorm is tied\n",
            "/encoder/layer/6/crossattention/self/query is tied\n",
            "/encoder/layer/6/crossattention/self/key is tied\n",
            "/encoder/layer/6/crossattention/self/value is tied\n",
            "/encoder/layer/6/crossattention/output/dense is tied\n",
            "/encoder/layer/6/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/6/intermediate/dense is tied\n",
            "/encoder/layer/6/output/dense is tied\n",
            "/encoder/layer/6/output/LayerNorm is tied\n",
            "/encoder/layer/7/crossattention/self/query is tied\n",
            "/encoder/layer/7/crossattention/self/key is tied\n",
            "/encoder/layer/7/crossattention/self/value is tied\n",
            "/encoder/layer/7/crossattention/output/dense is tied\n",
            "/encoder/layer/7/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/7/intermediate/dense is tied\n",
            "/encoder/layer/7/output/dense is tied\n",
            "/encoder/layer/7/output/LayerNorm is tied\n",
            "/encoder/layer/8/crossattention/self/query is tied\n",
            "/encoder/layer/8/crossattention/self/key is tied\n",
            "/encoder/layer/8/crossattention/self/value is tied\n",
            "/encoder/layer/8/crossattention/output/dense is tied\n",
            "/encoder/layer/8/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/8/intermediate/dense is tied\n",
            "/encoder/layer/8/output/dense is tied\n",
            "/encoder/layer/8/output/LayerNorm is tied\n",
            "/encoder/layer/9/crossattention/self/query is tied\n",
            "/encoder/layer/9/crossattention/self/key is tied\n",
            "/encoder/layer/9/crossattention/self/value is tied\n",
            "/encoder/layer/9/crossattention/output/dense is tied\n",
            "/encoder/layer/9/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/9/intermediate/dense is tied\n",
            "/encoder/layer/9/output/dense is tied\n",
            "/encoder/layer/9/output/LayerNorm is tied\n",
            "/encoder/layer/10/crossattention/self/query is tied\n",
            "/encoder/layer/10/crossattention/self/key is tied\n",
            "/encoder/layer/10/crossattention/self/value is tied\n",
            "/encoder/layer/10/crossattention/output/dense is tied\n",
            "/encoder/layer/10/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/10/intermediate/dense is tied\n",
            "/encoder/layer/10/output/dense is tied\n",
            "/encoder/layer/10/output/LayerNorm is tied\n",
            "/encoder/layer/11/crossattention/self/query is tied\n",
            "/encoder/layer/11/crossattention/self/key is tied\n",
            "/encoder/layer/11/crossattention/self/value is tied\n",
            "/encoder/layer/11/crossattention/output/dense is tied\n",
            "/encoder/layer/11/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/11/intermediate/dense is tied\n",
            "/encoder/layer/11/output/dense is tied\n",
            "/encoder/layer/11/output/LayerNorm is tied\n",
            "resume checkpoint from output/pretrain_animals/checkpoint_16.pth\n",
            "Start training\n",
            "Train Epoch: [17]  [   0/2500]  eta: 3:19:38  lr: 0.000050  loss_ita: 6.1392  loss_itm: 0.6385  loss_lm: 4.7927  time: 4.7912  data: 1.5059  max mem: 7121\n",
            "Train Epoch: [17]  [  50/2500]  eta: 0:39:44  lr: 0.000050  loss_ita: 5.9841  loss_itm: 0.6401  loss_lm: 4.4402  time: 0.8945  data: 0.0004  max mem: 7129\n",
            "Train Epoch: [17]  [ 100/2500]  eta: 0:38:02  lr: 0.000050  loss_ita: 5.9510  loss_itm: 0.6436  loss_lm: 4.7203  time: 0.9362  data: 0.0003  max mem: 7129\n",
            "Train Epoch: [17]  [ 150/2500]  eta: 0:36:56  lr: 0.000050  loss_ita: 5.8191  loss_itm: 0.6290  loss_lm: 4.2306  time: 0.9299  data: 0.0006  max mem: 7132\n",
            "Train Epoch: [17]  [ 200/2500]  eta: 0:36:00  lr: 0.000050  loss_ita: 5.8432  loss_itm: 0.6357  loss_lm: 4.4590  time: 0.9304  data: 0.0005  max mem: 7132\n",
            "Train Epoch: [17]  [ 250/2500]  eta: 0:35:06  lr: 0.000050  loss_ita: 6.0463  loss_itm: 0.6259  loss_lm: 4.6369  time: 0.9230  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 300/2500]  eta: 0:34:17  lr: 0.000050  loss_ita: 5.9681  loss_itm: 0.6204  loss_lm: 4.2531  time: 0.9307  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 350/2500]  eta: 0:33:28  lr: 0.000050  loss_ita: 5.8556  loss_itm: 0.6375  loss_lm: 4.7831  time: 0.9301  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 400/2500]  eta: 0:32:40  lr: 0.000050  loss_ita: 6.1228  loss_itm: 0.6355  loss_lm: 4.8984  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 450/2500]  eta: 0:31:51  lr: 0.000050  loss_ita: 6.3495  loss_itm: 0.6393  loss_lm: 4.6135  time: 0.9222  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 500/2500]  eta: 0:31:04  lr: 0.000050  loss_ita: 6.1634  loss_itm: 0.6500  loss_lm: 4.8925  time: 0.9304  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 550/2500]  eta: 0:30:17  lr: 0.000050  loss_ita: 5.9357  loss_itm: 0.6350  loss_lm: 4.1571  time: 0.9271  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 600/2500]  eta: 0:29:30  lr: 0.000050  loss_ita: 5.8463  loss_itm: 0.6394  loss_lm: 4.1982  time: 0.9227  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [ 650/2500]  eta: 0:28:43  lr: 0.000050  loss_ita: 6.0158  loss_itm: 0.6351  loss_lm: 4.7299  time: 0.9328  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 700/2500]  eta: 0:27:57  lr: 0.000050  loss_ita: 6.1428  loss_itm: 0.6314  loss_lm: 4.6872  time: 0.9312  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 750/2500]  eta: 0:27:10  lr: 0.000050  loss_ita: 6.2539  loss_itm: 0.6385  loss_lm: 4.5769  time: 0.9238  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 800/2500]  eta: 0:26:23  lr: 0.000050  loss_ita: 6.0947  loss_itm: 0.6348  loss_lm: 4.4382  time: 0.9340  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 850/2500]  eta: 0:25:36  lr: 0.000050  loss_ita: 5.9637  loss_itm: 0.6348  loss_lm: 4.9964  time: 0.9370  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 900/2500]  eta: 0:24:49  lr: 0.000050  loss_ita: 6.2077  loss_itm: 0.6503  loss_lm: 4.5971  time: 0.9215  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 950/2500]  eta: 0:24:03  lr: 0.000050  loss_ita: 6.2782  loss_itm: 0.6304  loss_lm: 4.1598  time: 0.9229  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1000/2500]  eta: 0:23:16  lr: 0.000050  loss_ita: 6.1663  loss_itm: 0.6368  loss_lm: 4.6378  time: 0.9374  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1050/2500]  eta: 0:22:30  lr: 0.000050  loss_ita: 6.2009  loss_itm: 0.6334  loss_lm: 4.4458  time: 0.9369  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1100/2500]  eta: 0:21:43  lr: 0.000050  loss_ita: 6.1823  loss_itm: 0.6355  loss_lm: 4.9945  time: 0.9254  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1150/2500]  eta: 0:20:56  lr: 0.000050  loss_ita: 6.0765  loss_itm: 0.6280  loss_lm: 4.5967  time: 0.9312  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1200/2500]  eta: 0:20:10  lr: 0.000050  loss_ita: 6.0619  loss_itm: 0.6410  loss_lm: 4.8511  time: 0.9367  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [1250/2500]  eta: 0:19:23  lr: 0.000050  loss_ita: 6.0537  loss_itm: 0.6422  loss_lm: 5.3150  time: 0.9227  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1300/2500]  eta: 0:18:36  lr: 0.000050  loss_ita: 5.9791  loss_itm: 0.6471  loss_lm: 4.5806  time: 0.9206  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1350/2500]  eta: 0:17:50  lr: 0.000050  loss_ita: 5.7339  loss_itm: 0.6316  loss_lm: 4.2935  time: 0.9418  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1400/2500]  eta: 0:17:03  lr: 0.000050  loss_ita: 5.8574  loss_itm: 0.6248  loss_lm: 4.8669  time: 0.9227  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1450/2500]  eta: 0:16:17  lr: 0.000050  loss_ita: 5.8236  loss_itm: 0.6454  loss_lm: 4.4377  time: 0.9271  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1500/2500]  eta: 0:15:30  lr: 0.000050  loss_ita: 5.6852  loss_itm: 0.6495  loss_lm: 4.7744  time: 0.9329  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1550/2500]  eta: 0:14:44  lr: 0.000050  loss_ita: 5.4990  loss_itm: 0.6276  loss_lm: 4.4243  time: 0.9361  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1600/2500]  eta: 0:13:57  lr: 0.000050  loss_ita: 5.6231  loss_itm: 0.6459  loss_lm: 4.4868  time: 0.9241  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1650/2500]  eta: 0:13:10  lr: 0.000050  loss_ita: 5.7932  loss_itm: 0.6374  loss_lm: 4.5288  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1700/2500]  eta: 0:12:24  lr: 0.000050  loss_ita: 5.9085  loss_itm: 0.6344  loss_lm: 4.4765  time: 0.9344  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1750/2500]  eta: 0:11:37  lr: 0.000050  loss_ita: 6.1626  loss_itm: 0.6300  loss_lm: 4.4793  time: 0.9218  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1800/2500]  eta: 0:10:51  lr: 0.000050  loss_ita: 6.1059  loss_itm: 0.6371  loss_lm: 4.8303  time: 0.9242  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1850/2500]  eta: 0:10:04  lr: 0.000050  loss_ita: 5.7550  loss_itm: 0.6354  loss_lm: 4.7988  time: 0.9292  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [1900/2500]  eta: 0:09:18  lr: 0.000050  loss_ita: 5.6715  loss_itm: 0.6439  loss_lm: 4.9443  time: 0.9322  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [1950/2500]  eta: 0:08:31  lr: 0.000050  loss_ita: 5.9078  loss_itm: 0.6322  loss_lm: 4.9832  time: 0.9235  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2000/2500]  eta: 0:07:45  lr: 0.000050  loss_ita: 6.1245  loss_itm: 0.6394  loss_lm: 4.4271  time: 0.9317  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2050/2500]  eta: 0:06:58  lr: 0.000050  loss_ita: 6.3317  loss_itm: 0.6358  loss_lm: 4.5954  time: 0.9336  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2100/2500]  eta: 0:06:12  lr: 0.000050  loss_ita: 6.1765  loss_itm: 0.6298  loss_lm: 4.4585  time: 0.9223  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2150/2500]  eta: 0:05:25  lr: 0.000050  loss_ita: 5.8163  loss_itm: 0.6358  loss_lm: 4.6149  time: 0.9204  data: 0.0002  max mem: 7134\n",
            "Train Epoch: [17]  [2200/2500]  eta: 0:04:39  lr: 0.000050  loss_ita: 5.7070  loss_itm: 0.6437  loss_lm: 4.4813  time: 0.9407  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2250/2500]  eta: 0:03:52  lr: 0.000050  loss_ita: 5.9511  loss_itm: 0.6427  loss_lm: 4.4372  time: 0.9190  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2300/2500]  eta: 0:03:05  lr: 0.000050  loss_ita: 6.0164  loss_itm: 0.6342  loss_lm: 4.4307  time: 0.9265  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2350/2500]  eta: 0:02:19  lr: 0.000050  loss_ita: 5.9607  loss_itm: 0.6364  loss_lm: 4.5868  time: 0.9313  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2400/2500]  eta: 0:01:32  lr: 0.000050  loss_ita: 6.0198  loss_itm: 0.6424  loss_lm: 4.5925  time: 0.9271  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2450/2500]  eta: 0:00:46  lr: 0.000050  loss_ita: 6.2225  loss_itm: 0.6522  loss_lm: 4.5737  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2499/2500]  eta: 0:00:00  lr: 0.000050  loss_ita: 6.3974  loss_itm: 0.6369  loss_lm: 5.1242  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17] Total time: 0:38:44 (0.9298 s / it)\n",
            "Averaged stats: lr: 0.0001  loss_ita: 5.9935  loss_itm: 0.6356  loss_lm: 4.6061\n",
            "Train Epoch: [18]  [   0/2500]  eta: 1:20:04  lr: 0.000045  loss_ita: 6.2714  loss_itm: 0.6457  loss_lm: 4.7524  time: 1.9218  data: 0.8585  max mem: 7970\n",
            "Train Epoch: [18]  [  50/2500]  eta: 0:40:21  lr: 0.000045  loss_ita: 6.3983  loss_itm: 0.6511  loss_lm: 4.1244  time: 0.9882  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 100/2500]  eta: 0:38:50  lr: 0.000045  loss_ita: 6.3850  loss_itm: 0.6482  loss_lm: 4.3894  time: 0.9561  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 150/2500]  eta: 0:37:38  lr: 0.000045  loss_ita: 6.2264  loss_itm: 0.6374  loss_lm: 4.5178  time: 0.9203  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 200/2500]  eta: 0:36:30  lr: 0.000045  loss_ita: 5.9527  loss_itm: 0.6499  loss_lm: 4.5807  time: 0.9236  data: 0.0003  max mem: 7982\n",
            "Train Epoch: [18]  [ 250/2500]  eta: 0:35:32  lr: 0.000045  loss_ita: 5.7489  loss_itm: 0.6327  loss_lm: 4.1563  time: 0.9307  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 300/2500]  eta: 0:34:36  lr: 0.000045  loss_ita: 5.7306  loss_itm: 0.6399  loss_lm: 4.6968  time: 0.9242  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 350/2500]  eta: 0:33:43  lr: 0.000045  loss_ita: 5.9946  loss_itm: 0.6380  loss_lm: 4.8402  time: 0.9228  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 400/2500]  eta: 0:32:53  lr: 0.000045  loss_ita: 6.1702  loss_itm: 0.6330  loss_lm: 4.2296  time: 0.9339  data: 0.0004  max mem: 7983\n",
            "Train Epoch: [18]  [ 450/2500]  eta: 0:32:02  lr: 0.000045  loss_ita: 6.1112  loss_itm: 0.6402  loss_lm: 4.3074  time: 0.9217  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 500/2500]  eta: 0:31:12  lr: 0.000045  loss_ita: 6.2875  loss_itm: 0.6396  loss_lm: 4.1692  time: 0.9172  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 550/2500]  eta: 0:30:24  lr: 0.000045  loss_ita: 6.1270  loss_itm: 0.6236  loss_lm: 4.2401  time: 0.9325  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 600/2500]  eta: 0:29:36  lr: 0.000045  loss_ita: 5.8813  loss_itm: 0.6358  loss_lm: 4.6269  time: 0.9270  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 650/2500]  eta: 0:28:47  lr: 0.000045  loss_ita: 5.8662  loss_itm: 0.6497  loss_lm: 4.5022  time: 0.9237  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 700/2500]  eta: 0:27:59  lr: 0.000045  loss_ita: 6.0300  loss_itm: 0.6378  loss_lm: 4.8883  time: 0.9273  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 750/2500]  eta: 0:27:12  lr: 0.000045  loss_ita: 6.1708  loss_itm: 0.6311  loss_lm: 4.5949  time: 0.9273  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 800/2500]  eta: 0:26:24  lr: 0.000045  loss_ita: 6.1851  loss_itm: 0.6380  loss_lm: 4.5566  time: 0.9163  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 850/2500]  eta: 0:25:36  lr: 0.000045  loss_ita: 6.0905  loss_itm: 0.6362  loss_lm: 4.4975  time: 0.9210  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 900/2500]  eta: 0:24:50  lr: 0.000045  loss_ita: 6.1650  loss_itm: 0.6281  loss_lm: 4.4557  time: 0.9308  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 950/2500]  eta: 0:24:02  lr: 0.000045  loss_ita: 6.4208  loss_itm: 0.6424  loss_lm: 4.9033  time: 0.9256  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1000/2500]  eta: 0:23:16  lr: 0.000045  loss_ita: 6.4365  loss_itm: 0.6313  loss_lm: 4.4134  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1050/2500]  eta: 0:22:29  lr: 0.000045  loss_ita: 6.1584  loss_itm: 0.6530  loss_lm: 4.5601  time: 0.9284  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1100/2500]  eta: 0:21:43  lr: 0.000045  loss_ita: 6.0874  loss_itm: 0.6354  loss_lm: 4.9366  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1150/2500]  eta: 0:20:56  lr: 0.000045  loss_ita: 5.9607  loss_itm: 0.6468  loss_lm: 4.2832  time: 0.9209  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1200/2500]  eta: 0:20:09  lr: 0.000045  loss_ita: 5.9632  loss_itm: 0.6471  loss_lm: 4.6797  time: 0.9294  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1250/2500]  eta: 0:19:22  lr: 0.000045  loss_ita: 6.1932  loss_itm: 0.6278  loss_lm: 4.3471  time: 0.9296  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1300/2500]  eta: 0:18:36  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6202  loss_lm: 4.5908  time: 0.9244  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1350/2500]  eta: 0:17:49  lr: 0.000045  loss_ita: 6.0786  loss_itm: 0.6392  loss_lm: 4.4718  time: 0.9198  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1400/2500]  eta: 0:17:02  lr: 0.000045  loss_ita: 6.0546  loss_itm: 0.6346  loss_lm: 4.4181  time: 0.9355  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1450/2500]  eta: 0:16:16  lr: 0.000045  loss_ita: 6.1391  loss_itm: 0.6276  loss_lm: 4.5114  time: 0.9284  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1500/2500]  eta: 0:15:29  lr: 0.000045  loss_ita: 5.7351  loss_itm: 0.6409  loss_lm: 4.5599  time: 0.9248  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [1550/2500]  eta: 0:14:43  lr: 0.000045  loss_ita: 5.2817  loss_itm: 0.6334  loss_lm: 4.4926  time: 0.9290  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1600/2500]  eta: 0:13:56  lr: 0.000045  loss_ita: 5.3891  loss_itm: 0.6325  loss_lm: 4.5553  time: 0.9378  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1650/2500]  eta: 0:13:10  lr: 0.000045  loss_ita: 5.8572  loss_itm: 0.6407  loss_lm: 4.6760  time: 0.9198  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1700/2500]  eta: 0:12:23  lr: 0.000045  loss_ita: 6.2208  loss_itm: 0.6292  loss_lm: 4.2864  time: 0.9233  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1750/2500]  eta: 0:11:37  lr: 0.000045  loss_ita: 6.4253  loss_itm: 0.6383  loss_lm: 4.3046  time: 0.9352  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1800/2500]  eta: 0:10:50  lr: 0.000045  loss_ita: 6.1586  loss_itm: 0.6361  loss_lm: 4.5963  time: 0.9234  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1850/2500]  eta: 0:10:04  lr: 0.000045  loss_ita: 6.1716  loss_itm: 0.6387  loss_lm: 4.4412  time: 0.9251  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1900/2500]  eta: 0:09:17  lr: 0.000045  loss_ita: 6.0535  loss_itm: 0.6367  loss_lm: 4.3708  time: 0.9263  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1950/2500]  eta: 0:08:31  lr: 0.000045  loss_ita: 5.9256  loss_itm: 0.6286  loss_lm: 4.6511  time: 0.9289  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2000/2500]  eta: 0:07:44  lr: 0.000045  loss_ita: 5.7240  loss_itm: 0.6361  loss_lm: 4.3443  time: 0.9228  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2050/2500]  eta: 0:06:58  lr: 0.000045  loss_ita: 5.7294  loss_itm: 0.6406  loss_lm: 4.5127  time: 0.9255  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2100/2500]  eta: 0:06:11  lr: 0.000045  loss_ita: 5.7598  loss_itm: 0.6372  loss_lm: 5.1710  time: 0.9374  data: 0.0006  max mem: 7989\n",
            "Train Epoch: [18]  [2150/2500]  eta: 0:05:25  lr: 0.000045  loss_ita: 6.0122  loss_itm: 0.6364  loss_lm: 4.6644  time: 0.9268  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2200/2500]  eta: 0:04:38  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6409  loss_lm: 3.9354  time: 0.9197  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2250/2500]  eta: 0:03:52  lr: 0.000045  loss_ita: 6.3002  loss_itm: 0.6383  loss_lm: 4.6477  time: 0.9372  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2300/2500]  eta: 0:03:05  lr: 0.000045  loss_ita: 6.2824  loss_itm: 0.6529  loss_lm: 4.5341  time: 0.9280  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2350/2500]  eta: 0:02:19  lr: 0.000045  loss_ita: 6.1060  loss_itm: 0.6350  loss_lm: 4.9516  time: 0.9269  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2400/2500]  eta: 0:01:32  lr: 0.000045  loss_ita: 6.1870  loss_itm: 0.6261  loss_lm: 5.1146  time: 0.9306  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2450/2500]  eta: 0:00:46  lr: 0.000045  loss_ita: 6.0545  loss_itm: 0.6384  loss_lm: 4.0758  time: 0.9333  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2499/2500]  eta: 0:00:00  lr: 0.000045  loss_ita: 6.0094  loss_itm: 0.6447  loss_lm: 4.6467  time: 0.9193  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18] Total time: 0:38:43 (0.9293 s / it)\n",
            "Averaged stats: lr: 0.0000  loss_ita: 6.0553  loss_itm: 0.6362  loss_lm: 4.5673\n",
            "Train Epoch: [19]  [   0/2500]  eta: 1:33:40  lr: 0.000041  loss_ita: 6.0785  loss_itm: 0.6439  loss_lm: 3.9600  time: 2.2481  data: 1.0550  max mem: 7989\n",
            "Train Epoch: [19]  [  50/2500]  eta: 0:40:28  lr: 0.000041  loss_ita: 6.1363  loss_itm: 0.6342  loss_lm: 4.7595  time: 0.9812  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [19]  [ 100/2500]  eta: 0:38:56  lr: 0.000041  loss_ita: 6.1894  loss_itm: 0.6489  loss_lm: 4.6459  time: 0.9512  data: 0.0005  max mem: 7989\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}