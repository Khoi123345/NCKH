{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "f89c9bd4-52f8-4613-c248-652d0c9b5f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "baddef30-d1af-4830-a8fc-42b4e9842535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cu128)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.12-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.25.0+cu128)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from cog) (2.32.4)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.41.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.20 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
            "Downloading cog-0.16.12-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.52.1\n",
            "    Uninstalling starlette-0.52.1:\n",
            "      Successfully uninstalled starlette-0.52.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.129.0\n",
            "    Uninstalling fastapi-0.129.0:\n",
            "      Successfully uninstalled fastapi-0.129.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.25.0 requires fastapi<1.0.0,>=0.124.1, but you have fastapi 0.118.3 which is incompatible.\n",
            "google-adk 1.25.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "sse-starlette 3.2.0 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.12 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.24.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.16.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.1\n",
            "    Uninstalling huggingface_hub-1.4.1:\n",
            "      Successfully uninstalled huggingface_hub-1.4.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.3 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.25.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.4->timm==0.4.12) (1.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.10.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.8.0->fairscale==0.4.4) (1.3.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292936 sha256=9a5265cf412f3fd0ce5197d88cf07732f9c5b92063c3c15a4890535fea130908\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "# TextVQA (phải chạy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "1807c9a7-7896-4c4a-ecf7-f29545ca876c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-25 07:18:52--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.182.81, 13.249.182.39, 13.249.182.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.182.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M   365MB/s    in 0.3s    \n",
            "\n",
            "2026-02-25 07:18:52 (365 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-02-25 07:18:52--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.182.81, 13.249.182.39, 13.249.182.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.182.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  71.8MB/s    in 0.2s    \n",
            "\n",
            "2026-02-25 07:18:53 (71.8 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-02-25 07:18:53--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.182.81, 13.249.182.39, 13.249.182.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.182.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  59.2MB/s    in 0.2s    \n",
            "\n",
            "2026-02-25 07:18:53 (59.2 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "3feea3da-bb4d-47b9-d2c2-6c829cd0b678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-25 07:18:53--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.182.81, 13.249.182.39, 13.249.182.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.182.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "images.zip           41%[=======>            ]   2.74G   232MB/s    eta 28s    "
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (Dhuy + HNhien)\n"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "990237e9-1cb2-4d51-fdf7-97249401f1d1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 318kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 2.84MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.26MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 4.30MB/s]\n",
            "100% 1.35G/1.35G [00:36<00:00, 39.2MB/s]\n",
            "reshape position embedding from 900 to 196\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "<All keys matched successfully>\n",
            "=> loading checkpoint './output/textVQA/checkpoint_04.pth'\n",
            "=> loaded checkpoint './output/textVQA/checkpoint_04.pth' (epoch 5)\n",
            "Start training\n",
            "Loss: 5.8001\n",
            "==========================================\n",
            "Train Epoch: [5]  [   0/8650]  eta: 13:56:33  lr: 0.000019  loss: 5.8001  time: 5.8027  data: 1.8266  max mem: 6922\n",
            "Loss: 9.716\n",
            "==========================================\n",
            "Loss: 6.8981\n",
            "==========================================\n",
            "Loss: 11.0914\n",
            "==========================================\n",
            "Loss: 7.5408\n",
            "==========================================\n",
            "Loss: 8.2925\n",
            "==========================================\n",
            "Loss: 7.6599\n",
            "==========================================\n",
            "Loss: 7.5884\n",
            "==========================================\n",
            "Loss: 7.7234\n",
            "==========================================\n",
            "Loss: 11.2942\n",
            "==========================================\n",
            "Loss: 10.0351\n",
            "==========================================\n",
            "Loss: 9.1664\n",
            "==========================================\n",
            "Loss: 6.1788\n",
            "==========================================\n",
            "Loss: 9.1275\n",
            "==========================================\n",
            "Loss: 4.9309\n",
            "==========================================\n",
            "Loss: 13.3417\n",
            "==========================================\n",
            "Loss: 8.64\n",
            "==========================================\n",
            "Loss: 6.0007\n",
            "==========================================\n",
            "Loss: 6.9533\n",
            "==========================================\n",
            "Loss: 12.9262\n",
            "==========================================\n",
            "Loss: 14.4791\n",
            "==========================================\n",
            "Loss: 7.4321\n",
            "==========================================\n",
            "Loss: 9.2772\n",
            "==========================================\n",
            "Loss: 9.4475\n",
            "==========================================\n",
            "Loss: 6.576\n",
            "==========================================\n",
            "Loss: 7.4088\n",
            "==========================================\n",
            "Loss: 5.924\n",
            "==========================================\n",
            "Loss: 8.2728\n",
            "==========================================\n",
            "Loss: 5.0004\n",
            "==========================================\n",
            "Loss: 9.5953\n",
            "==========================================\n",
            "Loss: 15.6227\n",
            "==========================================\n",
            "Loss: 14.5672\n",
            "==========================================\n",
            "Loss: 8.6229\n",
            "==========================================\n",
            "Loss: 7.8427\n",
            "==========================================\n",
            "Loss: 6.0376\n",
            "==========================================\n",
            "Loss: 6.5138\n",
            "==========================================\n",
            "Loss: 15.387\n",
            "==========================================\n",
            "Loss: 13.0894\n",
            "==========================================\n",
            "Loss: 5.5321\n",
            "==========================================\n",
            "Loss: 11.557\n",
            "==========================================\n",
            "Loss: 5.2967\n",
            "==========================================\n",
            "Loss: 9.8019\n",
            "==========================================\n",
            "Loss: 7.5374\n",
            "==========================================\n",
            "Loss: 8.2649\n",
            "==========================================\n",
            "Loss: 3.8398\n",
            "==========================================\n",
            "Loss: 8.6245\n",
            "==========================================\n",
            "Loss: 7.2302\n",
            "==========================================\n",
            "Loss: 11.3509\n",
            "==========================================\n",
            "Loss: 7.7063\n",
            "==========================================\n",
            "Loss: 13.948\n",
            "==========================================\n",
            "Train Epoch: [5]  [  50/8650]  eta: 1:23:06  lr: 0.000019  loss: 6.5669  time: 0.5259  data: 0.0004  max mem: 6940\n",
            "Train Epoch: [5]  [ 100/8650]  eta: 1:15:48  lr: 0.000019  loss: 13.0420  time: 0.5086  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 150/8650]  eta: 1:13:07  lr: 0.000019  loss: 7.7572  time: 0.5102  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 200/8650]  eta: 1:11:55  lr: 0.000019  loss: 6.3999  time: 0.5104  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [ 250/8650]  eta: 1:11:38  lr: 0.000019  loss: 6.9745  time: 0.5484  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 300/8650]  eta: 1:11:18  lr: 0.000019  loss: 6.8175  time: 0.5664  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 350/8650]  eta: 1:10:45  lr: 0.000019  loss: 8.9320  time: 0.5760  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 400/8650]  eta: 1:10:11  lr: 0.000019  loss: 7.4691  time: 0.5815  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 450/8650]  eta: 1:09:46  lr: 0.000019  loss: 7.3811  time: 0.5944  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 500/8650]  eta: 1:09:20  lr: 0.000019  loss: 10.0244  time: 0.5978  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [ 550/8650]  eta: 1:08:51  lr: 0.000019  loss: 6.7450  time: 0.6100  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 600/8650]  eta: 1:08:20  lr: 0.000019  loss: 10.3296  time: 0.6078  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [ 650/8650]  eta: 1:07:51  lr: 0.000019  loss: 6.3503  time: 0.6034  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 700/8650]  eta: 1:07:21  lr: 0.000019  loss: 9.6846  time: 0.5871  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [ 750/8650]  eta: 1:06:52  lr: 0.000019  loss: 7.7588  time: 0.5706  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 800/8650]  eta: 1:06:27  lr: 0.000019  loss: 12.9199  time: 0.5448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 850/8650]  eta: 1:05:59  lr: 0.000019  loss: 6.7244  time: 0.5184  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 900/8650]  eta: 1:05:32  lr: 0.000019  loss: 12.3290  time: 0.5120  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [ 950/8650]  eta: 1:05:07  lr: 0.000019  loss: 9.2821  time: 0.5008  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1000/8650]  eta: 1:04:46  lr: 0.000019  loss: 11.6311  time: 0.4925  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [1050/8650]  eta: 1:04:24  lr: 0.000019  loss: 14.2759  time: 0.4724  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1100/8650]  eta: 1:04:04  lr: 0.000019  loss: 11.5028  time: 0.4728  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [1150/8650]  eta: 1:03:39  lr: 0.000019  loss: 6.8373  time: 0.4336  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1200/8650]  eta: 1:03:18  lr: 0.000019  loss: 8.3390  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1250/8650]  eta: 1:02:57  lr: 0.000019  loss: 13.0444  time: 0.4186  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1300/8650]  eta: 1:02:38  lr: 0.000019  loss: 5.6193  time: 0.4346  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1350/8650]  eta: 1:02:20  lr: 0.000019  loss: 7.8417  time: 0.4524  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1400/8650]  eta: 1:01:54  lr: 0.000019  loss: 8.8899  time: 0.4293  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1450/8650]  eta: 1:01:27  lr: 0.000019  loss: 4.0739  time: 0.4370  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1500/8650]  eta: 1:01:01  lr: 0.000019  loss: 17.6428  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1550/8650]  eta: 1:00:35  lr: 0.000019  loss: 12.5670  time: 0.4484  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [1600/8650]  eta: 1:00:13  lr: 0.000019  loss: 12.3315  time: 0.4765  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [1650/8650]  eta: 0:59:50  lr: 0.000019  loss: 12.8724  time: 0.4895  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1700/8650]  eta: 0:59:29  lr: 0.000019  loss: 11.0204  time: 0.5196  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1750/8650]  eta: 0:59:07  lr: 0.000019  loss: 7.0482  time: 0.5089  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [1800/8650]  eta: 0:58:42  lr: 0.000019  loss: 5.9717  time: 0.5100  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [1850/8650]  eta: 0:58:15  lr: 0.000019  loss: 5.3410  time: 0.5197  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [1900/8650]  eta: 0:57:49  lr: 0.000019  loss: 13.1554  time: 0.5389  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1950/8650]  eta: 0:57:23  lr: 0.000019  loss: 10.8936  time: 0.5527  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2000/8650]  eta: 0:56:57  lr: 0.000019  loss: 10.2741  time: 0.5775  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2050/8650]  eta: 0:56:31  lr: 0.000019  loss: 7.2414  time: 0.5873  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2100/8650]  eta: 0:56:03  lr: 0.000019  loss: 8.1972  time: 0.5622  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2150/8650]  eta: 0:55:37  lr: 0.000019  loss: 7.2263  time: 0.5933  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2200/8650]  eta: 0:55:13  lr: 0.000019  loss: 7.5388  time: 0.6028  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [2250/8650]  eta: 0:54:51  lr: 0.000019  loss: 8.6495  time: 0.6497  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [2300/8650]  eta: 0:54:28  lr: 0.000019  loss: 7.3926  time: 0.6350  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [2350/8650]  eta: 0:54:02  lr: 0.000019  loss: 10.0805  time: 0.6181  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [2400/8650]  eta: 0:53:37  lr: 0.000019  loss: 10.5668  time: 0.6124  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2450/8650]  eta: 0:53:10  lr: 0.000019  loss: 9.1756  time: 0.5858  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2500/8650]  eta: 0:52:43  lr: 0.000019  loss: 6.5217  time: 0.5670  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2550/8650]  eta: 0:52:16  lr: 0.000019  loss: 6.7275  time: 0.5485  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2600/8650]  eta: 0:51:49  lr: 0.000019  loss: 10.4781  time: 0.5373  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2650/8650]  eta: 0:51:21  lr: 0.000019  loss: 7.5381  time: 0.5328  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2700/8650]  eta: 0:50:58  lr: 0.000019  loss: 6.9949  time: 0.5363  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2750/8650]  eta: 0:50:33  lr: 0.000019  loss: 5.7036  time: 0.5248  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2800/8650]  eta: 0:50:07  lr: 0.000019  loss: 9.6470  time: 0.5206  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2850/8650]  eta: 0:49:44  lr: 0.000019  loss: 8.1270  time: 0.5238  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2900/8650]  eta: 0:49:18  lr: 0.000019  loss: 7.5041  time: 0.4999  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2950/8650]  eta: 0:48:53  lr: 0.000019  loss: 15.6734  time: 0.4843  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3000/8650]  eta: 0:48:28  lr: 0.000019  loss: 9.4343  time: 0.4923  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3050/8650]  eta: 0:48:02  lr: 0.000019  loss: 8.1490  time: 0.4766  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3100/8650]  eta: 0:47:37  lr: 0.000019  loss: 4.2241  time: 0.4654  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3150/8650]  eta: 0:47:12  lr: 0.000019  loss: 4.9721  time: 0.4393  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3200/8650]  eta: 0:46:47  lr: 0.000019  loss: 11.1032  time: 0.4469  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3250/8650]  eta: 0:46:23  lr: 0.000019  loss: 10.6466  time: 0.4287  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [3300/8650]  eta: 0:45:58  lr: 0.000019  loss: 6.7168  time: 0.4254  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3350/8650]  eta: 0:45:33  lr: 0.000019  loss: 8.0026  time: 0.4346  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [3400/8650]  eta: 0:45:09  lr: 0.000019  loss: 6.1973  time: 0.4417  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3450/8650]  eta: 0:44:44  lr: 0.000019  loss: 9.8794  time: 0.4472  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3500/8650]  eta: 0:44:18  lr: 0.000019  loss: 6.4894  time: 0.4504  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3550/8650]  eta: 0:43:53  lr: 0.000019  loss: 7.6040  time: 0.4544  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3600/8650]  eta: 0:43:29  lr: 0.000019  loss: 15.9298  time: 0.4735  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [3650/8650]  eta: 0:43:04  lr: 0.000019  loss: 15.0736  time: 0.4742  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3700/8650]  eta: 0:42:40  lr: 0.000019  loss: 16.7557  time: 0.4924  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3750/8650]  eta: 0:42:15  lr: 0.000019  loss: 6.6279  time: 0.5070  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [3800/8650]  eta: 0:41:50  lr: 0.000019  loss: 11.6900  time: 0.5189  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3850/8650]  eta: 0:41:24  lr: 0.000019  loss: 7.2965  time: 0.5285  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [3900/8650]  eta: 0:41:00  lr: 0.000019  loss: 8.7488  time: 0.5694  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3950/8650]  eta: 0:40:34  lr: 0.000019  loss: 6.1661  time: 0.5743  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4000/8650]  eta: 0:40:09  lr: 0.000019  loss: 9.7439  time: 0.5927  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [4050/8650]  eta: 0:39:44  lr: 0.000019  loss: 10.9126  time: 0.6131  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4100/8650]  eta: 0:39:19  lr: 0.000019  loss: 10.9178  time: 0.6126  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [4150/8650]  eta: 0:38:52  lr: 0.000019  loss: 11.4628  time: 0.5910  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4200/8650]  eta: 0:38:26  lr: 0.000019  loss: 4.1876  time: 0.6003  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4250/8650]  eta: 0:37:59  lr: 0.000019  loss: 5.8990  time: 0.6130  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [4300/8650]  eta: 0:37:32  lr: 0.000019  loss: 9.9709  time: 0.6150  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [4350/8650]  eta: 0:37:06  lr: 0.000019  loss: 10.3060  time: 0.6194  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4400/8650]  eta: 0:36:40  lr: 0.000019  loss: 8.5342  time: 0.5904  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [4450/8650]  eta: 0:36:13  lr: 0.000019  loss: 13.9831  time: 0.5661  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4500/8650]  eta: 0:35:46  lr: 0.000019  loss: 10.3032  time: 0.5467  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [4550/8650]  eta: 0:35:20  lr: 0.000019  loss: 7.5555  time: 0.5394  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [4600/8650]  eta: 0:34:53  lr: 0.000019  loss: 7.5189  time: 0.5232  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [4650/8650]  eta: 0:34:26  lr: 0.000019  loss: 13.0161  time: 0.5114  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4700/8650]  eta: 0:34:00  lr: 0.000019  loss: 5.1385  time: 0.5011  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [4750/8650]  eta: 0:33:33  lr: 0.000019  loss: 9.9114  time: 0.4828  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4800/8650]  eta: 0:33:07  lr: 0.000019  loss: 8.4540  time: 0.4782  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4850/8650]  eta: 0:32:40  lr: 0.000019  loss: 7.5223  time: 0.4580  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4900/8650]  eta: 0:32:14  lr: 0.000019  loss: 6.4215  time: 0.4570  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4950/8650]  eta: 0:31:48  lr: 0.000019  loss: 9.9241  time: 0.4298  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5000/8650]  eta: 0:31:21  lr: 0.000019  loss: 9.1050  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5050/8650]  eta: 0:30:55  lr: 0.000019  loss: 7.5348  time: 0.4142  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5100/8650]  eta: 0:30:29  lr: 0.000019  loss: 13.2453  time: 0.4175  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5150/8650]  eta: 0:30:04  lr: 0.000019  loss: 7.7833  time: 0.4332  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5200/8650]  eta: 0:29:38  lr: 0.000019  loss: 12.0841  time: 0.4438  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5250/8650]  eta: 0:29:13  lr: 0.000019  loss: 8.3652  time: 0.4555  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5300/8650]  eta: 0:28:48  lr: 0.000019  loss: 8.7505  time: 0.4643  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [5350/8650]  eta: 0:28:23  lr: 0.000019  loss: 10.2043  time: 0.4756  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5400/8650]  eta: 0:27:58  lr: 0.000019  loss: 8.3261  time: 0.5006  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [5450/8650]  eta: 0:27:33  lr: 0.000019  loss: 10.4547  time: 0.5227  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [5500/8650]  eta: 0:27:07  lr: 0.000019  loss: 9.9565  time: 0.5320  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [5550/8650]  eta: 0:26:42  lr: 0.000019  loss: 7.7064  time: 0.5513  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5600/8650]  eta: 0:26:17  lr: 0.000019  loss: 10.2593  time: 0.5694  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5650/8650]  eta: 0:25:51  lr: 0.000019  loss: 7.4164  time: 0.5953  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5700/8650]  eta: 0:25:26  lr: 0.000019  loss: 8.8054  time: 0.6096  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5750/8650]  eta: 0:25:00  lr: 0.000019  loss: 9.9639  time: 0.6186  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [5800/8650]  eta: 0:24:35  lr: 0.000019  loss: 5.9324  time: 0.6418  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5850/8650]  eta: 0:24:10  lr: 0.000019  loss: 13.9653  time: 0.6556  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5900/8650]  eta: 0:23:44  lr: 0.000019  loss: 6.1673  time: 0.6438  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [5950/8650]  eta: 0:23:18  lr: 0.000019  loss: 7.3268  time: 0.6355  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [6000/8650]  eta: 0:22:53  lr: 0.000019  loss: 4.7842  time: 0.6215  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [6050/8650]  eta: 0:22:27  lr: 0.000019  loss: 8.7514  time: 0.5870  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [6100/8650]  eta: 0:22:01  lr: 0.000019  loss: 14.5427  time: 0.5523  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [6150/8650]  eta: 0:21:34  lr: 0.000019  loss: 11.0418  time: 0.5364  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [6200/8650]  eta: 0:21:08  lr: 0.000019  loss: 14.7763  time: 0.5252  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [6250/8650]  eta: 0:20:42  lr: 0.000019  loss: 13.9391  time: 0.5198  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [6300/8650]  eta: 0:20:16  lr: 0.000019  loss: 10.6665  time: 0.5135  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6350/8650]  eta: 0:19:49  lr: 0.000019  loss: 13.5960  time: 0.4958  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6400/8650]  eta: 0:19:24  lr: 0.000019  loss: 13.2406  time: 0.5181  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6450/8650]  eta: 0:18:58  lr: 0.000019  loss: 11.3838  time: 0.5038  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6500/8650]  eta: 0:18:32  lr: 0.000019  loss: 8.4652  time: 0.4820  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6550/8650]  eta: 0:18:06  lr: 0.000019  loss: 10.0051  time: 0.4848  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6600/8650]  eta: 0:17:40  lr: 0.000019  loss: 9.1695  time: 0.4576  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6650/8650]  eta: 0:17:15  lr: 0.000019  loss: 5.9478  time: 0.4333  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6700/8650]  eta: 0:16:49  lr: 0.000019  loss: 11.1746  time: 0.4134  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6750/8650]  eta: 0:16:23  lr: 0.000019  loss: 4.6233  time: 0.4133  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6800/8650]  eta: 0:15:57  lr: 0.000019  loss: 8.4037  time: 0.4134  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6850/8650]  eta: 0:15:31  lr: 0.000019  loss: 11.2519  time: 0.4258  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6900/8650]  eta: 0:15:05  lr: 0.000019  loss: 6.1735  time: 0.4248  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6950/8650]  eta: 0:14:39  lr: 0.000019  loss: 10.5685  time: 0.4271  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7000/8650]  eta: 0:14:13  lr: 0.000019  loss: 13.1472  time: 0.4486  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7050/8650]  eta: 0:13:47  lr: 0.000019  loss: 11.1049  time: 0.4634  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [7100/8650]  eta: 0:13:21  lr: 0.000019  loss: 5.6737  time: 0.4829  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7150/8650]  eta: 0:12:55  lr: 0.000019  loss: 11.2305  time: 0.4883  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7200/8650]  eta: 0:12:30  lr: 0.000019  loss: 11.7382  time: 0.5042  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7250/8650]  eta: 0:12:04  lr: 0.000019  loss: 17.4972  time: 0.5311  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7300/8650]  eta: 0:11:38  lr: 0.000019  loss: 6.7846  time: 0.5309  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7350/8650]  eta: 0:11:13  lr: 0.000019  loss: 12.0840  time: 0.5516  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7400/8650]  eta: 0:10:47  lr: 0.000019  loss: 10.3018  time: 0.5663  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [7450/8650]  eta: 0:10:21  lr: 0.000019  loss: 8.5857  time: 0.5824  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7500/8650]  eta: 0:09:55  lr: 0.000019  loss: 9.5705  time: 0.5816  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7550/8650]  eta: 0:09:29  lr: 0.000019  loss: 6.4759  time: 0.5942  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [7600/8650]  eta: 0:09:04  lr: 0.000019  loss: 5.9648  time: 0.6044  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7650/8650]  eta: 0:08:38  lr: 0.000019  loss: 9.3134  time: 0.6216  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7700/8650]  eta: 0:08:12  lr: 0.000019  loss: 6.7390  time: 0.6012  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7750/8650]  eta: 0:07:46  lr: 0.000019  loss: 7.9263  time: 0.6254  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7800/8650]  eta: 0:07:20  lr: 0.000019  loss: 9.3983  time: 0.6356  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7850/8650]  eta: 0:06:54  lr: 0.000019  loss: 13.6575  time: 0.6503  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7900/8650]  eta: 0:06:29  lr: 0.000019  loss: 14.2111  time: 0.6448  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7950/8650]  eta: 0:06:03  lr: 0.000019  loss: 9.6984  time: 0.6384  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [5]  [8000/8650]  eta: 0:05:37  lr: 0.000019  loss: 12.9771  time: 0.6075  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [8050/8650]  eta: 0:05:11  lr: 0.000019  loss: 8.3484  time: 0.5863  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8100/8650]  eta: 0:04:45  lr: 0.000019  loss: 12.4707  time: 0.5753  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8150/8650]  eta: 0:04:19  lr: 0.000019  loss: 14.6489  time: 0.5601  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8200/8650]  eta: 0:03:53  lr: 0.000019  loss: 8.5347  time: 0.5685  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8250/8650]  eta: 0:03:27  lr: 0.000019  loss: 7.9667  time: 0.5585  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8300/8650]  eta: 0:03:01  lr: 0.000019  loss: 9.9880  time: 0.5407  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8350/8650]  eta: 0:02:35  lr: 0.000019  loss: 8.9232  time: 0.5252  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8400/8650]  eta: 0:02:09  lr: 0.000019  loss: 16.0629  time: 0.5297  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [8450/8650]  eta: 0:01:43  lr: 0.000019  loss: 7.1970  time: 0.5019  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [8500/8650]  eta: 0:01:17  lr: 0.000019  loss: 11.9363  time: 0.4975  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [8550/8650]  eta: 0:00:51  lr: 0.000019  loss: 17.6459  time: 0.4789  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [8600/8650]  eta: 0:00:25  lr: 0.000019  loss: 10.9015  time: 0.4596  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [8649/8650]  eta: 0:00:00  lr: 0.000019  loss: 7.7335  time: 0.4459  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5] Total time: 1:14:49 (0.5190 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 9.3163\n",
            "Loss: 8.5265\n",
            "==========================================\n",
            "Train Epoch: [6]  [   0/8650]  eta: 6:35:13  lr: 0.000018  loss: 8.5265  time: 2.7414  data: 1.5243  max mem: 9086\n",
            "Loss: 13.2796\n",
            "==========================================\n",
            "Loss: 10.7466\n",
            "==========================================\n",
            "Loss: 12.8492\n",
            "==========================================\n",
            "Loss: 6.5873\n",
            "==========================================\n",
            "Loss: 5.8761\n",
            "==========================================\n",
            "Loss: 11.7641\n",
            "==========================================\n",
            "Loss: 5.3718\n",
            "==========================================\n",
            "Loss: 5.311\n",
            "==========================================\n",
            "Loss: 7.885\n",
            "==========================================\n",
            "Loss: 8.124\n",
            "==========================================\n",
            "Loss: 6.9498\n",
            "==========================================\n",
            "Loss: 8.8575\n",
            "==========================================\n",
            "Loss: 10.786\n",
            "==========================================\n",
            "Loss: 7.7356\n",
            "==========================================\n",
            "Loss: 5.1139\n",
            "==========================================\n",
            "Loss: 9.3516\n",
            "==========================================\n",
            "Loss: 7.1849\n",
            "==========================================\n",
            "Loss: 9.7251\n",
            "==========================================\n",
            "Loss: 15.0498\n",
            "==========================================\n",
            "Loss: 6.34\n",
            "==========================================\n",
            "Loss: 7.4213\n",
            "==========================================\n",
            "Loss: 6.9484\n",
            "==========================================\n",
            "Loss: 10.7638\n",
            "==========================================\n",
            "Loss: 11.435\n",
            "==========================================\n",
            "Loss: 5.8783\n",
            "==========================================\n",
            "Loss: 11.3849\n",
            "==========================================\n",
            "Loss: 10.2778\n",
            "==========================================\n",
            "Loss: 10.2428\n",
            "==========================================\n",
            "Loss: 6.8818\n",
            "==========================================\n",
            "Loss: 6.2717\n",
            "==========================================\n",
            "Loss: 17.319\n",
            "==========================================\n",
            "Loss: 10.3455\n",
            "==========================================\n",
            "Loss: 7.1199\n",
            "==========================================\n",
            "Loss: 7.3411\n",
            "==========================================\n",
            "Loss: 9.74\n",
            "==========================================\n",
            "Loss: 9.8956\n",
            "==========================================\n",
            "Loss: 6.724\n",
            "==========================================\n",
            "Loss: 13.6002\n",
            "==========================================\n",
            "Loss: 7.9394\n",
            "==========================================\n",
            "Loss: 7.7329\n",
            "==========================================\n",
            "Loss: 4.757\n",
            "==========================================\n",
            "Loss: 8.9845\n",
            "==========================================\n",
            "Loss: 10.1575\n",
            "==========================================\n",
            "Loss: 7.9443\n",
            "==========================================\n",
            "Loss: 14.2508\n",
            "==========================================\n",
            "Loss: 7.7219\n",
            "==========================================\n",
            "Loss: 9.8364\n",
            "==========================================\n",
            "Loss: 6.7814\n",
            "==========================================\n",
            "Loss: 9.2709\n",
            "==========================================\n",
            "Train Epoch: [6]  [  50/8650]  eta: 1:45:25  lr: 0.000018  loss: 13.3541  time: 0.8149  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [ 100/8650]  eta: 1:30:57  lr: 0.000018  loss: 11.9387  time: 0.6552  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 150/8650]  eta: 1:25:58  lr: 0.000018  loss: 6.7050  time: 0.6738  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 200/8650]  eta: 1:22:50  lr: 0.000018  loss: 7.3543  time: 0.6405  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 250/8650]  eta: 1:20:45  lr: 0.000018  loss: 8.4298  time: 0.6164  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [ 300/8650]  eta: 1:19:12  lr: 0.000018  loss: 7.1672  time: 0.5955  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [ 350/8650]  eta: 1:17:57  lr: 0.000018  loss: 6.9104  time: 0.5702  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [ 400/8650]  eta: 1:16:22  lr: 0.000018  loss: 4.6862  time: 0.5394  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [ 450/8650]  eta: 1:15:00  lr: 0.000018  loss: 10.0569  time: 0.5256  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [ 500/8650]  eta: 1:13:54  lr: 0.000018  loss: 7.2182  time: 0.4985  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [ 550/8650]  eta: 1:12:54  lr: 0.000018  loss: 9.0042  time: 0.4912  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 600/8650]  eta: 1:12:00  lr: 0.000018  loss: 4.3407  time: 0.4953  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [ 650/8650]  eta: 1:11:12  lr: 0.000018  loss: 9.1411  time: 0.4847  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [ 700/8650]  eta: 1:10:27  lr: 0.000018  loss: 12.0635  time: 0.4665  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 750/8650]  eta: 1:09:45  lr: 0.000018  loss: 7.3661  time: 0.4568  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [ 800/8650]  eta: 1:09:04  lr: 0.000018  loss: 7.4012  time: 0.4246  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [ 850/8650]  eta: 1:08:23  lr: 0.000018  loss: 12.2747  time: 0.4119  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [ 900/8650]  eta: 1:07:44  lr: 0.000018  loss: 7.6412  time: 0.4104  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [ 950/8650]  eta: 1:07:09  lr: 0.000018  loss: 9.5868  time: 0.4108  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1000/8650]  eta: 1:06:33  lr: 0.000018  loss: 11.3633  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1050/8650]  eta: 1:06:00  lr: 0.000018  loss: 7.1431  time: 0.4203  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1100/8650]  eta: 1:05:30  lr: 0.000018  loss: 8.5856  time: 0.4390  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1150/8650]  eta: 1:04:59  lr: 0.000018  loss: 6.2946  time: 0.4346  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1200/8650]  eta: 1:04:27  lr: 0.000018  loss: 3.9348  time: 0.4370  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [1250/8650]  eta: 1:03:59  lr: 0.000018  loss: 11.4983  time: 0.4590  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [1300/8650]  eta: 1:03:32  lr: 0.000018  loss: 14.4251  time: 0.4642  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1350/8650]  eta: 1:03:04  lr: 0.000018  loss: 9.3808  time: 0.4792  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1400/8650]  eta: 1:02:36  lr: 0.000018  loss: 12.4232  time: 0.4791  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [1450/8650]  eta: 1:02:09  lr: 0.000018  loss: 9.9928  time: 0.5014  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [1500/8650]  eta: 1:01:40  lr: 0.000018  loss: 12.5362  time: 0.5059  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [1550/8650]  eta: 1:01:14  lr: 0.000018  loss: 7.3687  time: 0.5299  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1600/8650]  eta: 1:00:56  lr: 0.000018  loss: 9.2208  time: 0.6046  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1650/8650]  eta: 1:00:30  lr: 0.000018  loss: 8.1765  time: 0.5367  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [1700/8650]  eta: 1:00:05  lr: 0.000018  loss: 14.6416  time: 0.5716  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [1750/8650]  eta: 0:59:39  lr: 0.000018  loss: 8.7103  time: 0.5795  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [1800/8650]  eta: 0:59:12  lr: 0.000018  loss: 7.5587  time: 0.5784  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [1850/8650]  eta: 0:58:48  lr: 0.000018  loss: 9.9871  time: 0.5810  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [1900/8650]  eta: 0:58:21  lr: 0.000018  loss: 5.9160  time: 0.5888  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [1950/8650]  eta: 0:57:57  lr: 0.000018  loss: 6.9598  time: 0.6066  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2000/8650]  eta: 0:57:29  lr: 0.000018  loss: 10.1281  time: 0.5929  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [2050/8650]  eta: 0:57:01  lr: 0.000018  loss: 11.6278  time: 0.5959  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2100/8650]  eta: 0:56:33  lr: 0.000018  loss: 9.0626  time: 0.6038  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2150/8650]  eta: 0:56:05  lr: 0.000018  loss: 12.1998  time: 0.6106  data: 0.0017  max mem: 9086\n",
            "Train Epoch: [6]  [2200/8650]  eta: 0:55:39  lr: 0.000018  loss: 7.9931  time: 0.5993  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2250/8650]  eta: 0:55:10  lr: 0.000018  loss: 8.4814  time: 0.5742  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [2300/8650]  eta: 0:54:42  lr: 0.000018  loss: 8.0301  time: 0.5677  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2350/8650]  eta: 0:54:23  lr: 0.000018  loss: 6.1511  time: 0.6390  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [2400/8650]  eta: 0:53:57  lr: 0.000018  loss: 13.6800  time: 0.5423  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2450/8650]  eta: 0:53:32  lr: 0.000018  loss: 8.6549  time: 0.5547  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [2500/8650]  eta: 0:53:07  lr: 0.000018  loss: 13.3026  time: 0.5402  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [2550/8650]  eta: 0:52:41  lr: 0.000018  loss: 6.4222  time: 0.5194  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [2600/8650]  eta: 0:52:15  lr: 0.000018  loss: 7.1482  time: 0.5209  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [2650/8650]  eta: 0:51:48  lr: 0.000018  loss: 6.2789  time: 0.4918  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [2700/8650]  eta: 0:51:20  lr: 0.000018  loss: 7.9309  time: 0.4781  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2750/8650]  eta: 0:50:53  lr: 0.000018  loss: 10.2314  time: 0.4525  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2800/8650]  eta: 0:50:26  lr: 0.000018  loss: 10.2724  time: 0.4562  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [2850/8650]  eta: 0:49:58  lr: 0.000018  loss: 10.3259  time: 0.4408  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [2900/8650]  eta: 0:49:31  lr: 0.000018  loss: 8.8291  time: 0.4313  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [2950/8650]  eta: 0:49:04  lr: 0.000018  loss: 12.2788  time: 0.4200  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3000/8650]  eta: 0:48:38  lr: 0.000018  loss: 9.9890  time: 0.4126  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3050/8650]  eta: 0:48:11  lr: 0.000018  loss: 6.2730  time: 0.4167  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3100/8650]  eta: 0:47:44  lr: 0.000018  loss: 6.5066  time: 0.4110  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3150/8650]  eta: 0:47:17  lr: 0.000018  loss: 8.5855  time: 0.4259  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3200/8650]  eta: 0:46:50  lr: 0.000018  loss: 13.7952  time: 0.4293  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [3250/8650]  eta: 0:46:24  lr: 0.000018  loss: 12.5473  time: 0.4441  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3300/8650]  eta: 0:45:57  lr: 0.000018  loss: 6.3941  time: 0.4529  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [3350/8650]  eta: 0:45:32  lr: 0.000018  loss: 15.7937  time: 0.4848  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [3400/8650]  eta: 0:45:07  lr: 0.000018  loss: 6.9773  time: 0.4940  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [3450/8650]  eta: 0:44:41  lr: 0.000018  loss: 10.8067  time: 0.4900  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [3500/8650]  eta: 0:44:17  lr: 0.000018  loss: 10.9630  time: 0.5418  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3550/8650]  eta: 0:43:51  lr: 0.000018  loss: 7.9786  time: 0.5299  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [3600/8650]  eta: 0:43:26  lr: 0.000018  loss: 8.9087  time: 0.5333  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3650/8650]  eta: 0:43:03  lr: 0.000018  loss: 8.1983  time: 0.5778  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3700/8650]  eta: 0:42:39  lr: 0.000018  loss: 7.9853  time: 0.5775  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3750/8650]  eta: 0:42:13  lr: 0.000018  loss: 6.8639  time: 0.5704  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [3800/8650]  eta: 0:41:46  lr: 0.000018  loss: 8.8785  time: 0.5567  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [3850/8650]  eta: 0:41:20  lr: 0.000018  loss: 14.1508  time: 0.5823  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [3900/8650]  eta: 0:40:53  lr: 0.000018  loss: 13.1841  time: 0.5716  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [3950/8650]  eta: 0:40:27  lr: 0.000018  loss: 6.7390  time: 0.5984  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4000/8650]  eta: 0:40:01  lr: 0.000018  loss: 6.3210  time: 0.6040  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [4050/8650]  eta: 0:39:34  lr: 0.000018  loss: 6.2748  time: 0.6162  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6]  [4100/8650]  eta: 0:39:08  lr: 0.000018  loss: 13.2526  time: 0.6201  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4150/8650]  eta: 0:38:42  lr: 0.000018  loss: 10.2891  time: 0.6091  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [4200/8650]  eta: 0:38:15  lr: 0.000018  loss: 9.6157  time: 0.5907  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4250/8650]  eta: 0:37:49  lr: 0.000018  loss: 6.3298  time: 0.5792  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4300/8650]  eta: 0:37:22  lr: 0.000018  loss: 11.6028  time: 0.5656  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [6]  [4350/8650]  eta: 0:36:56  lr: 0.000018  loss: 7.3120  time: 0.5609  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4400/8650]  eta: 0:36:29  lr: 0.000018  loss: 10.4844  time: 0.5484  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [4450/8650]  eta: 0:36:03  lr: 0.000018  loss: 9.5898  time: 0.5257  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [4500/8650]  eta: 0:35:37  lr: 0.000018  loss: 13.0422  time: 0.5212  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4550/8650]  eta: 0:35:10  lr: 0.000018  loss: 8.6806  time: 0.5040  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [4600/8650]  eta: 0:34:44  lr: 0.000018  loss: 12.9434  time: 0.4998  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4650/8650]  eta: 0:34:18  lr: 0.000018  loss: 9.8023  time: 0.4729  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4700/8650]  eta: 0:33:53  lr: 0.000018  loss: 14.7364  time: 0.4816  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4750/8650]  eta: 0:33:28  lr: 0.000018  loss: 11.4415  time: 0.4635  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [4800/8650]  eta: 0:33:03  lr: 0.000018  loss: 9.6640  time: 0.4472  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [4850/8650]  eta: 0:32:36  lr: 0.000018  loss: 10.6265  time: 0.4278  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [4900/8650]  eta: 0:32:10  lr: 0.000018  loss: 11.4844  time: 0.4235  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [4950/8650]  eta: 0:31:45  lr: 0.000018  loss: 11.5718  time: 0.4222  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5000/8650]  eta: 0:31:19  lr: 0.000018  loss: 5.2528  time: 0.4250  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5050/8650]  eta: 0:30:53  lr: 0.000018  loss: 7.3147  time: 0.4178  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5100/8650]  eta: 0:30:28  lr: 0.000018  loss: 8.4810  time: 0.4185  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5150/8650]  eta: 0:30:02  lr: 0.000018  loss: 17.4687  time: 0.4217  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [5200/8650]  eta: 0:29:36  lr: 0.000018  loss: 6.3156  time: 0.4260  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5250/8650]  eta: 0:29:11  lr: 0.000018  loss: 10.9219  time: 0.4475  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5300/8650]  eta: 0:28:46  lr: 0.000018  loss: 8.1697  time: 0.4351  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [5350/8650]  eta: 0:28:20  lr: 0.000018  loss: 9.3666  time: 0.4365  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5400/8650]  eta: 0:27:54  lr: 0.000018  loss: 6.4004  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [5450/8650]  eta: 0:27:27  lr: 0.000018  loss: 12.0481  time: 0.4506  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5500/8650]  eta: 0:27:02  lr: 0.000018  loss: 13.0492  time: 0.4635  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5550/8650]  eta: 0:26:36  lr: 0.000018  loss: 8.5694  time: 0.4606  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [5600/8650]  eta: 0:26:09  lr: 0.000018  loss: 8.8211  time: 0.4691  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5650/8650]  eta: 0:25:43  lr: 0.000018  loss: 7.3662  time: 0.4750  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [5700/8650]  eta: 0:25:18  lr: 0.000018  loss: 7.6419  time: 0.5053  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [5750/8650]  eta: 0:24:52  lr: 0.000018  loss: 6.4156  time: 0.5256  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [5800/8650]  eta: 0:24:26  lr: 0.000018  loss: 13.9443  time: 0.5448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [5850/8650]  eta: 0:24:00  lr: 0.000018  loss: 6.4287  time: 0.5609  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [5900/8650]  eta: 0:23:34  lr: 0.000018  loss: 6.2620  time: 0.5677  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [5950/8650]  eta: 0:23:08  lr: 0.000018  loss: 12.2994  time: 0.5723  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6000/8650]  eta: 0:22:43  lr: 0.000018  loss: 7.4848  time: 0.6484  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6050/8650]  eta: 0:22:17  lr: 0.000018  loss: 10.7408  time: 0.5905  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [6100/8650]  eta: 0:21:52  lr: 0.000018  loss: 8.8045  time: 0.6055  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6150/8650]  eta: 0:21:26  lr: 0.000018  loss: 9.8758  time: 0.6303  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6200/8650]  eta: 0:21:01  lr: 0.000018  loss: 6.4068  time: 0.6324  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6250/8650]  eta: 0:20:34  lr: 0.000018  loss: 7.8222  time: 0.5852  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [6]  [6300/8650]  eta: 0:20:09  lr: 0.000018  loss: 16.7939  time: 0.6969  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6350/8650]  eta: 0:19:44  lr: 0.000018  loss: 10.2340  time: 0.6277  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6400/8650]  eta: 0:19:18  lr: 0.000018  loss: 10.7532  time: 0.6204  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [6450/8650]  eta: 0:18:52  lr: 0.000018  loss: 6.0227  time: 0.6059  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6]  [6500/8650]  eta: 0:18:26  lr: 0.000018  loss: 10.8508  time: 0.5858  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [6]  [6550/8650]  eta: 0:18:00  lr: 0.000018  loss: 5.5268  time: 0.5792  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [6600/8650]  eta: 0:17:34  lr: 0.000018  loss: 12.0824  time: 0.5673  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6650/8650]  eta: 0:17:08  lr: 0.000018  loss: 7.8516  time: 0.5544  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [6700/8650]  eta: 0:16:42  lr: 0.000018  loss: 12.0964  time: 0.5338  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [6750/8650]  eta: 0:16:16  lr: 0.000018  loss: 8.9048  time: 0.5193  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [6800/8650]  eta: 0:15:51  lr: 0.000018  loss: 9.2116  time: 0.4996  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [6850/8650]  eta: 0:15:25  lr: 0.000018  loss: 10.3504  time: 0.5092  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [6900/8650]  eta: 0:14:59  lr: 0.000018  loss: 11.3653  time: 0.5142  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [6950/8650]  eta: 0:14:34  lr: 0.000018  loss: 8.4748  time: 0.5056  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [7000/8650]  eta: 0:14:08  lr: 0.000018  loss: 4.6866  time: 0.5153  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [7050/8650]  eta: 0:13:43  lr: 0.000018  loss: 10.3888  time: 0.4957  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7100/8650]  eta: 0:13:17  lr: 0.000018  loss: 7.0534  time: 0.4899  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7150/8650]  eta: 0:12:51  lr: 0.000018  loss: 10.9717  time: 0.4529  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7200/8650]  eta: 0:12:25  lr: 0.000018  loss: 5.4373  time: 0.4433  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [6]  [7250/8650]  eta: 0:12:00  lr: 0.000018  loss: 11.2938  time: 0.4248  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7300/8650]  eta: 0:11:34  lr: 0.000018  loss: 10.5430  time: 0.4192  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7350/8650]  eta: 0:11:08  lr: 0.000018  loss: 7.7703  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7400/8650]  eta: 0:10:42  lr: 0.000018  loss: 8.9021  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7450/8650]  eta: 0:10:17  lr: 0.000018  loss: 9.2109  time: 0.4185  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7500/8650]  eta: 0:09:51  lr: 0.000018  loss: 17.5883  time: 0.4323  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7550/8650]  eta: 0:09:25  lr: 0.000018  loss: 8.1363  time: 0.4283  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7600/8650]  eta: 0:08:59  lr: 0.000018  loss: 8.0516  time: 0.4304  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7650/8650]  eta: 0:08:34  lr: 0.000018  loss: 11.3057  time: 0.4404  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [6]  [7700/8650]  eta: 0:08:08  lr: 0.000018  loss: 9.0966  time: 0.4331  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7750/8650]  eta: 0:07:42  lr: 0.000018  loss: 12.7919  time: 0.4518  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7800/8650]  eta: 0:07:17  lr: 0.000018  loss: 10.5155  time: 0.4815  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [7850/8650]  eta: 0:06:51  lr: 0.000018  loss: 7.0916  time: 0.4793  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [7900/8650]  eta: 0:06:25  lr: 0.000018  loss: 8.8058  time: 0.4891  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [7950/8650]  eta: 0:05:59  lr: 0.000018  loss: 10.5038  time: 0.4902  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [6]  [8000/8650]  eta: 0:05:34  lr: 0.000018  loss: 8.6845  time: 0.5130  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [6]  [8050/8650]  eta: 0:05:08  lr: 0.000018  loss: 12.3904  time: 0.5249  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [6]  [8100/8650]  eta: 0:04:42  lr: 0.000018  loss: 8.1986  time: 0.5361  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [8150/8650]  eta: 0:04:17  lr: 0.000018  loss: 11.8347  time: 0.5497  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [8200/8650]  eta: 0:03:51  lr: 0.000018  loss: 7.0175  time: 0.5818  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8250/8650]  eta: 0:03:25  lr: 0.000018  loss: 6.0770  time: 0.5743  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [6]  [8300/8650]  eta: 0:02:59  lr: 0.000018  loss: 16.2899  time: 0.5704  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [8350/8650]  eta: 0:02:34  lr: 0.000018  loss: 17.2887  time: 0.5879  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8400/8650]  eta: 0:02:08  lr: 0.000018  loss: 5.7699  time: 0.5802  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8450/8650]  eta: 0:01:42  lr: 0.000018  loss: 5.9327  time: 0.5905  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [6]  [8500/8650]  eta: 0:01:17  lr: 0.000018  loss: 10.1223  time: 0.5973  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8550/8650]  eta: 0:00:51  lr: 0.000018  loss: 9.3807  time: 0.6159  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [6]  [8600/8650]  eta: 0:00:25  lr: 0.000018  loss: 6.4628  time: 0.6374  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [6]  [8649/8650]  eta: 0:00:00  lr: 0.000018  loss: 4.6984  time: 0.5974  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [6] Total time: 1:14:06 (0.5140 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 9.3507\n",
            "Loss: 6.9651\n",
            "==========================================\n",
            "Train Epoch: [7]  [   0/8650]  eta: 4:39:52  lr: 0.000017  loss: 6.9651  time: 1.9413  data: 1.0819  max mem: 9086\n",
            "Loss: 6.277\n",
            "==========================================\n",
            "Loss: 7.9919\n",
            "==========================================\n",
            "Loss: 6.9872\n",
            "==========================================\n",
            "Loss: 11.1805\n",
            "==========================================\n",
            "Loss: 11.1639\n",
            "==========================================\n",
            "Loss: 7.1333\n",
            "==========================================\n",
            "Loss: 9.8996\n",
            "==========================================\n",
            "Loss: 8.6578\n",
            "==========================================\n",
            "Loss: 9.949\n",
            "==========================================\n",
            "Loss: 9.9788\n",
            "==========================================\n",
            "Loss: 11.8081\n",
            "==========================================\n",
            "Loss: 6.0883\n",
            "==========================================\n",
            "Loss: 5.7889\n",
            "==========================================\n",
            "Loss: 5.851\n",
            "==========================================\n",
            "Loss: 8.0898\n",
            "==========================================\n",
            "Loss: 8.8398\n",
            "==========================================\n",
            "Loss: 9.528\n",
            "==========================================\n",
            "Loss: 7.0136\n",
            "==========================================\n",
            "Loss: 8.9172\n",
            "==========================================\n",
            "Loss: 9.6133\n",
            "==========================================\n",
            "Loss: 10.6327\n",
            "==========================================\n",
            "Loss: 11.8055\n",
            "==========================================\n",
            "Loss: 7.1087\n",
            "==========================================\n",
            "Loss: 4.8396\n",
            "==========================================\n",
            "Loss: 9.556\n",
            "==========================================\n",
            "Loss: 5.1909\n",
            "==========================================\n",
            "Loss: 6.8896\n",
            "==========================================\n",
            "Loss: 13.6629\n",
            "==========================================\n",
            "Loss: 10.4571\n",
            "==========================================\n",
            "Loss: 14.1672\n",
            "==========================================\n",
            "Loss: 8.9223\n",
            "==========================================\n",
            "Loss: 7.6069\n",
            "==========================================\n",
            "Loss: 6.8588\n",
            "==========================================\n",
            "Loss: 11.2001\n",
            "==========================================\n",
            "Loss: 4.6759\n",
            "==========================================\n",
            "Loss: 7.478\n",
            "==========================================\n",
            "Loss: 11.3764\n",
            "==========================================\n",
            "Loss: 6.7273\n",
            "==========================================\n",
            "Loss: 5.8912\n",
            "==========================================\n",
            "Loss: 8.2311\n",
            "==========================================\n",
            "Loss: 7.3096\n",
            "==========================================\n",
            "Loss: 7.0351\n",
            "==========================================\n",
            "Loss: 7.0354\n",
            "==========================================\n",
            "Loss: 4.2508\n",
            "==========================================\n",
            "Loss: 9.9736\n",
            "==========================================\n",
            "Loss: 7.7842\n",
            "==========================================\n",
            "Loss: 8.4671\n",
            "==========================================\n",
            "Loss: 9.1234\n",
            "==========================================\n",
            "Loss: 8.9679\n",
            "==========================================\n",
            "Train Epoch: [7]  [  50/8650]  eta: 1:34:31  lr: 0.000017  loss: 9.4938  time: 0.6230  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 100/8650]  eta: 1:26:14  lr: 0.000017  loss: 7.8777  time: 0.5508  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 150/8650]  eta: 1:22:14  lr: 0.000017  loss: 14.1560  time: 0.5329  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [ 200/8650]  eta: 1:20:13  lr: 0.000017  loss: 10.4455  time: 0.5493  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [ 250/8650]  eta: 1:19:49  lr: 0.000017  loss: 8.0043  time: 0.6273  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [ 300/8650]  eta: 1:18:31  lr: 0.000017  loss: 5.6494  time: 0.5790  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 350/8650]  eta: 1:17:01  lr: 0.000017  loss: 9.2160  time: 0.5757  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 400/8650]  eta: 1:15:50  lr: 0.000017  loss: 5.1543  time: 0.6001  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 450/8650]  eta: 1:14:49  lr: 0.000017  loss: 11.4146  time: 0.5993  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [ 500/8650]  eta: 1:13:50  lr: 0.000017  loss: 11.8593  time: 0.6063  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 550/8650]  eta: 1:13:01  lr: 0.000017  loss: 6.9364  time: 0.6259  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [ 600/8650]  eta: 1:12:12  lr: 0.000017  loss: 14.3055  time: 0.6094  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 650/8650]  eta: 1:11:26  lr: 0.000017  loss: 9.8270  time: 0.6096  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 700/8650]  eta: 1:10:40  lr: 0.000017  loss: 12.9913  time: 0.5873  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 750/8650]  eta: 1:10:02  lr: 0.000017  loss: 8.3696  time: 0.5844  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 800/8650]  eta: 1:09:24  lr: 0.000017  loss: 5.7487  time: 0.5592  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [ 850/8650]  eta: 1:08:44  lr: 0.000017  loss: 7.6529  time: 0.5524  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [ 900/8650]  eta: 1:08:06  lr: 0.000017  loss: 8.4002  time: 0.5589  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [ 950/8650]  eta: 1:07:32  lr: 0.000017  loss: 9.0978  time: 0.5519  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [1000/8650]  eta: 1:06:58  lr: 0.000017  loss: 5.9277  time: 0.5336  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [1050/8650]  eta: 1:06:26  lr: 0.000017  loss: 13.2498  time: 0.5274  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [1100/8650]  eta: 1:05:52  lr: 0.000017  loss: 6.7690  time: 0.5139  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [1150/8650]  eta: 1:05:23  lr: 0.000017  loss: 8.2555  time: 0.5237  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [1200/8650]  eta: 1:04:51  lr: 0.000017  loss: 4.7379  time: 0.4800  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [1250/8650]  eta: 1:04:21  lr: 0.000017  loss: 15.5486  time: 0.4790  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [1300/8650]  eta: 1:03:52  lr: 0.000017  loss: 11.3983  time: 0.4825  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [1350/8650]  eta: 1:03:22  lr: 0.000017  loss: 9.8996  time: 0.4519  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [1400/8650]  eta: 1:02:53  lr: 0.000017  loss: 8.1814  time: 0.4346  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1450/8650]  eta: 1:02:24  lr: 0.000017  loss: 13.3549  time: 0.4237  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1500/8650]  eta: 1:01:55  lr: 0.000017  loss: 7.6535  time: 0.4144  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1550/8650]  eta: 1:01:27  lr: 0.000017  loss: 9.1274  time: 0.4236  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1600/8650]  eta: 1:00:59  lr: 0.000017  loss: 9.0033  time: 0.4282  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1650/8650]  eta: 1:00:30  lr: 0.000017  loss: 8.9940  time: 0.4156  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1700/8650]  eta: 1:00:03  lr: 0.000017  loss: 10.1996  time: 0.4244  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1750/8650]  eta: 0:59:34  lr: 0.000017  loss: 10.9605  time: 0.4208  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [1800/8650]  eta: 0:59:08  lr: 0.000017  loss: 12.2057  time: 0.4526  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1850/8650]  eta: 0:58:46  lr: 0.000017  loss: 6.5209  time: 0.4680  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [1900/8650]  eta: 0:58:21  lr: 0.000017  loss: 6.6092  time: 0.4518  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [1950/8650]  eta: 0:57:54  lr: 0.000017  loss: 5.7882  time: 0.4555  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [2000/8650]  eta: 0:57:28  lr: 0.000017  loss: 7.4042  time: 0.4774  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [2050/8650]  eta: 0:57:04  lr: 0.000017  loss: 8.5893  time: 0.5080  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [2100/8650]  eta: 0:56:37  lr: 0.000017  loss: 7.0648  time: 0.4843  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [2150/8650]  eta: 0:56:12  lr: 0.000017  loss: 13.8695  time: 0.5001  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [2200/8650]  eta: 0:55:47  lr: 0.000017  loss: 9.3586  time: 0.5120  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [2250/8650]  eta: 0:55:21  lr: 0.000017  loss: 8.6169  time: 0.5239  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [2300/8650]  eta: 0:54:56  lr: 0.000017  loss: 5.0426  time: 0.5447  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [2350/8650]  eta: 0:54:30  lr: 0.000017  loss: 9.3662  time: 0.5527  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [2400/8650]  eta: 0:54:04  lr: 0.000017  loss: 14.6053  time: 0.5687  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2450/8650]  eta: 0:53:37  lr: 0.000017  loss: 6.1953  time: 0.5702  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2500/8650]  eta: 0:53:10  lr: 0.000017  loss: 8.1212  time: 0.5844  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2550/8650]  eta: 0:52:46  lr: 0.000017  loss: 8.0249  time: 0.6375  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2600/8650]  eta: 0:52:23  lr: 0.000017  loss: 10.1520  time: 0.6374  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2650/8650]  eta: 0:51:57  lr: 0.000017  loss: 9.6214  time: 0.6015  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2700/8650]  eta: 0:51:32  lr: 0.000017  loss: 11.4041  time: 0.6424  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [2750/8650]  eta: 0:51:06  lr: 0.000017  loss: 7.8220  time: 0.6314  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [2800/8650]  eta: 0:50:40  lr: 0.000017  loss: 8.8305  time: 0.6213  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [2850/8650]  eta: 0:50:12  lr: 0.000017  loss: 8.1588  time: 0.5955  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2900/8650]  eta: 0:49:46  lr: 0.000017  loss: 10.8983  time: 0.5787  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [2950/8650]  eta: 0:49:19  lr: 0.000017  loss: 10.6230  time: 0.5855  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3000/8650]  eta: 0:48:52  lr: 0.000017  loss: 8.6140  time: 0.5613  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [3050/8650]  eta: 0:48:25  lr: 0.000017  loss: 7.5831  time: 0.5498  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3100/8650]  eta: 0:47:59  lr: 0.000017  loss: 8.7567  time: 0.5587  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [3150/8650]  eta: 0:47:35  lr: 0.000017  loss: 9.7593  time: 0.5770  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [3200/8650]  eta: 0:47:08  lr: 0.000017  loss: 6.4451  time: 0.5219  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3250/8650]  eta: 0:46:42  lr: 0.000017  loss: 5.4734  time: 0.5256  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [3300/8650]  eta: 0:46:16  lr: 0.000017  loss: 5.7588  time: 0.5242  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [3350/8650]  eta: 0:45:50  lr: 0.000017  loss: 12.5059  time: 0.5164  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [3400/8650]  eta: 0:45:24  lr: 0.000017  loss: 8.6301  time: 0.5013  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3450/8650]  eta: 0:44:58  lr: 0.000017  loss: 8.5589  time: 0.4717  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3500/8650]  eta: 0:44:31  lr: 0.000017  loss: 8.1924  time: 0.4652  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3550/8650]  eta: 0:44:05  lr: 0.000017  loss: 5.1823  time: 0.4546  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [3600/8650]  eta: 0:43:38  lr: 0.000017  loss: 5.7140  time: 0.4241  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3650/8650]  eta: 0:43:12  lr: 0.000017  loss: 6.6396  time: 0.4168  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3700/8650]  eta: 0:42:45  lr: 0.000017  loss: 11.1873  time: 0.4169  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3750/8650]  eta: 0:42:19  lr: 0.000017  loss: 6.4641  time: 0.4297  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3800/8650]  eta: 0:41:53  lr: 0.000017  loss: 6.9850  time: 0.4224  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3850/8650]  eta: 0:41:27  lr: 0.000017  loss: 10.8541  time: 0.4471  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [3900/8650]  eta: 0:41:01  lr: 0.000017  loss: 9.5829  time: 0.4361  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [3950/8650]  eta: 0:40:36  lr: 0.000017  loss: 7.1696  time: 0.4541  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [4000/8650]  eta: 0:40:11  lr: 0.000017  loss: 12.0305  time: 0.4386  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4050/8650]  eta: 0:39:46  lr: 0.000017  loss: 9.6940  time: 0.4528  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4100/8650]  eta: 0:39:22  lr: 0.000017  loss: 11.6511  time: 0.4497  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4150/8650]  eta: 0:38:56  lr: 0.000017  loss: 10.2520  time: 0.4409  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [4200/8650]  eta: 0:38:29  lr: 0.000017  loss: 7.6948  time: 0.4517  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4250/8650]  eta: 0:38:03  lr: 0.000017  loss: 6.1810  time: 0.4637  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4300/8650]  eta: 0:37:37  lr: 0.000017  loss: 11.4234  time: 0.4674  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [4350/8650]  eta: 0:37:11  lr: 0.000017  loss: 11.9392  time: 0.4679  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4400/8650]  eta: 0:36:45  lr: 0.000017  loss: 8.1058  time: 0.4949  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [4450/8650]  eta: 0:36:19  lr: 0.000017  loss: 12.1712  time: 0.4813  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [4500/8650]  eta: 0:35:54  lr: 0.000017  loss: 8.4324  time: 0.5065  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [4550/8650]  eta: 0:35:29  lr: 0.000017  loss: 9.6282  time: 0.5226  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [4600/8650]  eta: 0:35:02  lr: 0.000017  loss: 8.9540  time: 0.5151  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4650/8650]  eta: 0:34:36  lr: 0.000017  loss: 9.9487  time: 0.5369  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [4700/8650]  eta: 0:34:12  lr: 0.000017  loss: 8.7334  time: 0.6207  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [4750/8650]  eta: 0:33:47  lr: 0.000017  loss: 10.8660  time: 0.5867  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4800/8650]  eta: 0:33:21  lr: 0.000017  loss: 7.2010  time: 0.5622  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [4850/8650]  eta: 0:32:55  lr: 0.000017  loss: 5.4725  time: 0.5826  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [4900/8650]  eta: 0:32:29  lr: 0.000017  loss: 8.2172  time: 0.5801  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [4950/8650]  eta: 0:32:03  lr: 0.000017  loss: 8.2459  time: 0.5813  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5000/8650]  eta: 0:31:36  lr: 0.000017  loss: 5.4753  time: 0.5755  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [5050/8650]  eta: 0:31:10  lr: 0.000017  loss: 10.0510  time: 0.5747  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5100/8650]  eta: 0:30:44  lr: 0.000017  loss: 12.5888  time: 0.5814  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5150/8650]  eta: 0:30:18  lr: 0.000017  loss: 12.9244  time: 0.6302  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [5200/8650]  eta: 0:29:52  lr: 0.000017  loss: 9.7436  time: 0.6222  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5250/8650]  eta: 0:29:27  lr: 0.000017  loss: 9.1175  time: 0.6645  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5300/8650]  eta: 0:29:02  lr: 0.000017  loss: 11.3688  time: 0.6732  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [7]  [5350/8650]  eta: 0:28:36  lr: 0.000017  loss: 9.0789  time: 0.6369  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5400/8650]  eta: 0:28:10  lr: 0.000017  loss: 6.5241  time: 0.6165  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [7]  [5450/8650]  eta: 0:27:44  lr: 0.000017  loss: 7.6953  time: 0.6139  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5500/8650]  eta: 0:27:17  lr: 0.000017  loss: 6.1571  time: 0.5971  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [5550/8650]  eta: 0:26:51  lr: 0.000017  loss: 8.9268  time: 0.5849  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [5600/8650]  eta: 0:26:25  lr: 0.000017  loss: 11.8150  time: 0.5549  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5650/8650]  eta: 0:25:59  lr: 0.000017  loss: 8.3074  time: 0.5961  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5700/8650]  eta: 0:25:33  lr: 0.000017  loss: 9.2889  time: 0.5790  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [5750/8650]  eta: 0:25:08  lr: 0.000017  loss: 8.9520  time: 0.5517  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [5800/8650]  eta: 0:24:41  lr: 0.000017  loss: 9.8666  time: 0.5160  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [5850/8650]  eta: 0:24:15  lr: 0.000017  loss: 8.5201  time: 0.4992  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [5900/8650]  eta: 0:23:49  lr: 0.000017  loss: 6.8197  time: 0.4864  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [5950/8650]  eta: 0:23:23  lr: 0.000017  loss: 11.0724  time: 0.4756  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [6000/8650]  eta: 0:22:56  lr: 0.000017  loss: 8.1376  time: 0.4561  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [6050/8650]  eta: 0:22:30  lr: 0.000017  loss: 9.8285  time: 0.4536  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6100/8650]  eta: 0:22:04  lr: 0.000017  loss: 9.5402  time: 0.4467  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6150/8650]  eta: 0:21:38  lr: 0.000017  loss: 9.6295  time: 0.4354  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6200/8650]  eta: 0:21:11  lr: 0.000017  loss: 13.6655  time: 0.4298  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6250/8650]  eta: 0:20:45  lr: 0.000017  loss: 8.1217  time: 0.4218  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6300/8650]  eta: 0:20:19  lr: 0.000017  loss: 7.9863  time: 0.4284  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6350/8650]  eta: 0:19:53  lr: 0.000017  loss: 7.6919  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6400/8650]  eta: 0:19:27  lr: 0.000017  loss: 6.8911  time: 0.4321  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6450/8650]  eta: 0:19:01  lr: 0.000017  loss: 8.7889  time: 0.4349  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6500/8650]  eta: 0:18:35  lr: 0.000017  loss: 12.0165  time: 0.4221  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6550/8650]  eta: 0:18:10  lr: 0.000017  loss: 6.9514  time: 0.4402  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6600/8650]  eta: 0:17:44  lr: 0.000017  loss: 29.2547  time: 0.4394  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6650/8650]  eta: 0:17:18  lr: 0.000017  loss: 7.7061  time: 0.4431  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [7]  [6700/8650]  eta: 0:16:52  lr: 0.000017  loss: 10.6540  time: 0.4390  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6750/8650]  eta: 0:16:26  lr: 0.000017  loss: 10.7660  time: 0.4469  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6800/8650]  eta: 0:16:00  lr: 0.000017  loss: 7.9884  time: 0.4509  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6850/8650]  eta: 0:15:34  lr: 0.000017  loss: 6.7048  time: 0.4553  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [6900/8650]  eta: 0:15:08  lr: 0.000017  loss: 6.1937  time: 0.4667  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [6950/8650]  eta: 0:14:42  lr: 0.000017  loss: 6.4485  time: 0.4743  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [7000/8650]  eta: 0:14:16  lr: 0.000017  loss: 8.9652  time: 0.4897  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7050/8650]  eta: 0:13:50  lr: 0.000017  loss: 8.9966  time: 0.5076  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [7100/8650]  eta: 0:13:24  lr: 0.000017  loss: 11.1333  time: 0.5001  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7150/8650]  eta: 0:12:58  lr: 0.000017  loss: 7.2718  time: 0.5193  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [7]  [7200/8650]  eta: 0:12:32  lr: 0.000017  loss: 7.2616  time: 0.5358  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [7250/8650]  eta: 0:12:06  lr: 0.000017  loss: 9.2809  time: 0.5684  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7300/8650]  eta: 0:11:40  lr: 0.000017  loss: 9.9385  time: 0.5748  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [7]  [7350/8650]  eta: 0:11:14  lr: 0.000017  loss: 9.3384  time: 0.5898  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [7400/8650]  eta: 0:10:48  lr: 0.000017  loss: 8.5349  time: 0.5810  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7450/8650]  eta: 0:10:22  lr: 0.000017  loss: 6.0092  time: 0.5981  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7500/8650]  eta: 0:09:56  lr: 0.000017  loss: 6.2190  time: 0.5970  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [7550/8650]  eta: 0:09:30  lr: 0.000017  loss: 9.1826  time: 0.6046  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7600/8650]  eta: 0:09:04  lr: 0.000017  loss: 9.4467  time: 0.6040  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [7]  [7650/8650]  eta: 0:08:38  lr: 0.000017  loss: 11.4337  time: 0.6365  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [7]  [7700/8650]  eta: 0:08:12  lr: 0.000017  loss: 10.3359  time: 0.6564  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7750/8650]  eta: 0:07:47  lr: 0.000017  loss: 8.2989  time: 0.6516  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7800/8650]  eta: 0:07:21  lr: 0.000017  loss: 8.5611  time: 0.6185  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [7850/8650]  eta: 0:06:55  lr: 0.000017  loss: 9.2754  time: 0.6173  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [7900/8650]  eta: 0:06:29  lr: 0.000017  loss: 12.2691  time: 0.5928  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [7950/8650]  eta: 0:06:03  lr: 0.000017  loss: 10.6156  time: 0.5733  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [8000/8650]  eta: 0:05:37  lr: 0.000017  loss: 4.2285  time: 0.5846  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8050/8650]  eta: 0:05:11  lr: 0.000017  loss: 5.5360  time: 0.5643  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8100/8650]  eta: 0:04:45  lr: 0.000017  loss: 7.0580  time: 0.5732  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [7]  [8150/8650]  eta: 0:04:19  lr: 0.000017  loss: 8.3257  time: 0.5528  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [7]  [8200/8650]  eta: 0:03:53  lr: 0.000017  loss: 9.1498  time: 0.5513  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [7]  [8250/8650]  eta: 0:03:27  lr: 0.000017  loss: 15.8532  time: 0.5605  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [7]  [8300/8650]  eta: 0:03:01  lr: 0.000017  loss: 10.0324  time: 0.5310  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [8350/8650]  eta: 0:02:35  lr: 0.000017  loss: 11.5032  time: 0.4999  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8400/8650]  eta: 0:02:09  lr: 0.000017  loss: 12.5117  time: 0.4729  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7]  [8450/8650]  eta: 0:01:43  lr: 0.000017  loss: 7.2170  time: 0.4714  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8500/8650]  eta: 0:01:17  lr: 0.000017  loss: 9.3437  time: 0.4530  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8550/8650]  eta: 0:00:51  lr: 0.000017  loss: 6.9807  time: 0.4476  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [7]  [8600/8650]  eta: 0:00:25  lr: 0.000017  loss: 3.6678  time: 0.4363  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [7]  [8649/8650]  eta: 0:00:00  lr: 0.000017  loss: 11.4466  time: 0.4419  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [7] Total time: 1:14:45 (0.5186 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 8.6721\n",
            "Loss: 8.5887\n",
            "==========================================\n",
            "Train Epoch: [8]  [   0/8650]  eta: 6:18:40  lr: 0.000017  loss: 8.5887  time: 2.6267  data: 1.2901  max mem: 9086\n",
            "Loss: 8.9799\n",
            "==========================================\n",
            "Loss: 11.2287\n",
            "==========================================\n",
            "Loss: 7.6762\n",
            "==========================================\n",
            "Loss: 7.8738\n",
            "==========================================\n",
            "Loss: 5.6159\n",
            "==========================================\n",
            "Loss: 8.7384\n",
            "==========================================\n",
            "Loss: 8.9461\n",
            "==========================================\n",
            "Loss: 8.6301\n",
            "==========================================\n",
            "Loss: 7.6243\n",
            "==========================================\n",
            "Loss: 8.4243\n",
            "==========================================\n",
            "Loss: 8.9897\n",
            "==========================================\n",
            "Loss: 5.8718\n",
            "==========================================\n",
            "Loss: 7.3765\n",
            "==========================================\n",
            "Loss: 8.9717\n",
            "==========================================\n",
            "Loss: 7.6177\n",
            "==========================================\n",
            "Loss: 15.5338\n",
            "==========================================\n",
            "Loss: 9.7865\n",
            "==========================================\n",
            "Loss: 6.7609\n",
            "==========================================\n",
            "Loss: 6.5124\n",
            "==========================================\n",
            "Loss: 7.5373\n",
            "==========================================\n",
            "Loss: 8.0037\n",
            "==========================================\n",
            "Loss: 23.1815\n",
            "==========================================\n",
            "Loss: 6.5641\n",
            "==========================================\n",
            "Loss: 7.6523\n",
            "==========================================\n",
            "Loss: 9.24\n",
            "==========================================\n",
            "Loss: 11.1977\n",
            "==========================================\n",
            "Loss: 6.5934\n",
            "==========================================\n",
            "Loss: 8.002\n",
            "==========================================\n",
            "Loss: 8.6218\n",
            "==========================================\n",
            "Loss: 6.9175\n",
            "==========================================\n",
            "Loss: 6.9062\n",
            "==========================================\n",
            "Loss: 8.6392\n",
            "==========================================\n",
            "Loss: 7.4255\n",
            "==========================================\n",
            "Loss: 8.9188\n",
            "==========================================\n",
            "Loss: 5.6186\n",
            "==========================================\n",
            "Loss: 7.8037\n",
            "==========================================\n",
            "Loss: 5.7252\n",
            "==========================================\n",
            "Loss: 7.6388\n",
            "==========================================\n",
            "Loss: 6.9086\n",
            "==========================================\n",
            "Loss: 8.3265\n",
            "==========================================\n",
            "Loss: 7.7741\n",
            "==========================================\n",
            "Loss: 13.4817\n",
            "==========================================\n",
            "Loss: 9.5316\n",
            "==========================================\n",
            "Loss: 6.1108\n",
            "==========================================\n",
            "Loss: 11.6081\n",
            "==========================================\n",
            "Loss: 7.7169\n",
            "==========================================\n",
            "Loss: 10.3857\n",
            "==========================================\n",
            "Loss: 6.4361\n",
            "==========================================\n",
            "Loss: 6.0008\n",
            "==========================================\n",
            "Train Epoch: [8]  [  50/8650]  eta: 1:30:06  lr: 0.000017  loss: 7.1514  time: 0.6325  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [8]  [ 100/8650]  eta: 1:23:42  lr: 0.000017  loss: 5.6420  time: 0.5567  data: 0.0012  max mem: 9086\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA/checkpoint_04.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train textVQA + pretrain_animal (Khoi)"
      ],
      "metadata": {
        "id": "i9pDWKQLVYL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHhslGC2SmK",
        "collapsed": true,
        "outputId": "9f5ec3fd-1a2c-474e-de39-10e47bdb00dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "load checkpoint from /content/drive/MyDrive/BLIP/output/pretrain_animals/checkpoint_29.pth\n",
            "_IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_proj_m.weight', 'text_proj_m.bias'])\n",
            "=> loading checkpoint './output/textVQA_pretrain_animals/checkpoint_01.pth'\n",
            "=> loaded checkpoint './output/textVQA_pretrain_animals/checkpoint_01.pth' (epoch 2)\n",
            "Start training\n",
            "Loss: 18.6777\n",
            "==========================================\n",
            "Train Epoch: [2]  [   0/8650]  eta: 8:01:34  lr: 0.000020  loss: 18.6777  time: 3.3404  data: 0.8189  max mem: 6922\n",
            "Loss: 22.4611\n",
            "==========================================\n",
            "Loss: 17.5866\n",
            "==========================================\n",
            "Loss: 21.231\n",
            "==========================================\n",
            "Loss: 21.3843\n",
            "==========================================\n",
            "Loss: 18.8618\n",
            "==========================================\n",
            "Loss: 15.5629\n",
            "==========================================\n",
            "Loss: 23.5244\n",
            "==========================================\n",
            "Loss: 23.9318\n",
            "==========================================\n",
            "Loss: 24.9128\n",
            "==========================================\n",
            "Loss: 21.4092\n",
            "==========================================\n",
            "Loss: 25.6195\n",
            "==========================================\n",
            "Loss: 15.3241\n",
            "==========================================\n",
            "Loss: 28.2695\n",
            "==========================================\n",
            "Loss: 14.47\n",
            "==========================================\n",
            "Loss: 33.8146\n",
            "==========================================\n",
            "Loss: 18.8912\n",
            "==========================================\n",
            "Loss: 12.1435\n",
            "==========================================\n",
            "Loss: 21.7021\n",
            "==========================================\n",
            "Loss: 34.1982\n",
            "==========================================\n",
            "Loss: 27.6942\n",
            "==========================================\n",
            "Loss: 20.1724\n",
            "==========================================\n",
            "Loss: 25.4174\n",
            "==========================================\n",
            "Loss: 21.936\n",
            "==========================================\n",
            "Loss: 16.4927\n",
            "==========================================\n",
            "Loss: 14.1448\n",
            "==========================================\n",
            "Loss: 20.5522\n",
            "==========================================\n",
            "Loss: 21.0139\n",
            "==========================================\n",
            "Loss: 15.5725\n",
            "==========================================\n",
            "Loss: 24.9856\n",
            "==========================================\n",
            "Loss: 43.5794\n",
            "==========================================\n",
            "Loss: 33.0342\n",
            "==========================================\n",
            "Loss: 18.8028\n",
            "==========================================\n",
            "Loss: 20.2039\n",
            "==========================================\n",
            "Loss: 17.1862\n",
            "==========================================\n",
            "Loss: 15.6105\n",
            "==========================================\n",
            "Loss: 35.078\n",
            "==========================================\n",
            "Loss: 30.657\n",
            "==========================================\n",
            "Loss: 19.6413\n",
            "==========================================\n",
            "Loss: 22.2628\n",
            "==========================================\n",
            "Loss: 15.0573\n",
            "==========================================\n",
            "Loss: 27.0616\n",
            "==========================================\n",
            "Loss: 17.0627\n",
            "==========================================\n",
            "Loss: 17.1132\n",
            "==========================================\n",
            "Loss: 10.2461\n",
            "==========================================\n",
            "Loss: 18.6734\n",
            "==========================================\n",
            "Loss: 17.8489\n",
            "==========================================\n",
            "Loss: 26.7007\n",
            "==========================================\n",
            "Loss: 12.4081\n",
            "==========================================\n",
            "Loss: 32.0864\n",
            "==========================================\n",
            "Train Epoch: [2]  [  50/8650]  eta: 1:17:37  lr: 0.000020  loss: 11.3213  time: 0.4140  data: 0.0003  max mem: 6940\n",
            "Train Epoch: [2]  [ 100/8650]  eta: 1:13:30  lr: 0.000020  loss: 26.7528  time: 0.4236  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [ 150/8650]  eta: 1:12:08  lr: 0.000020  loss: 19.2316  time: 0.4453  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 200/8650]  eta: 1:11:05  lr: 0.000020  loss: 17.9074  time: 0.4625  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 250/8650]  eta: 1:10:16  lr: 0.000020  loss: 16.4067  time: 0.4700  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 300/8650]  eta: 1:09:28  lr: 0.000020  loss: 14.8766  time: 0.4674  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 350/8650]  eta: 1:08:55  lr: 0.000020  loss: 23.0860  time: 0.4843  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 400/8650]  eta: 1:08:23  lr: 0.000020  loss: 22.7008  time: 0.4800  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 450/8650]  eta: 1:07:46  lr: 0.000020  loss: 18.1663  time: 0.4739  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 500/8650]  eta: 1:07:12  lr: 0.000020  loss: 21.0927  time: 0.4668  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 550/8650]  eta: 1:06:39  lr: 0.000020  loss: 16.1113  time: 0.4688  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 600/8650]  eta: 1:06:10  lr: 0.000020  loss: 21.0578  time: 0.4567  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 650/8650]  eta: 1:05:42  lr: 0.000020  loss: 12.5453  time: 0.4557  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 700/8650]  eta: 1:05:11  lr: 0.000020  loss: 20.5337  time: 0.4434  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 750/8650]  eta: 1:04:39  lr: 0.000020  loss: 22.1555  time: 0.4441  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 800/8650]  eta: 1:04:11  lr: 0.000020  loss: 25.5829  time: 0.4493  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 850/8650]  eta: 1:03:44  lr: 0.000020  loss: 16.4216  time: 0.4643  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 900/8650]  eta: 1:03:24  lr: 0.000020  loss: 27.3669  time: 0.4984  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 950/8650]  eta: 1:03:01  lr: 0.000020  loss: 19.1353  time: 0.4688  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [1000/8650]  eta: 1:02:37  lr: 0.000020  loss: 38.6375  time: 0.4832  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [1050/8650]  eta: 1:02:12  lr: 0.000020  loss: 39.5535  time: 0.5014  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1100/8650]  eta: 1:01:46  lr: 0.000020  loss: 24.7547  time: 0.4881  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1150/8650]  eta: 1:01:22  lr: 0.000020  loss: 21.8469  time: 0.4944  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1200/8650]  eta: 1:00:58  lr: 0.000020  loss: 22.1881  time: 0.5152  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1250/8650]  eta: 1:00:32  lr: 0.000020  loss: 28.9929  time: 0.5042  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1300/8650]  eta: 1:00:06  lr: 0.000020  loss: 11.9106  time: 0.5008  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [1350/8650]  eta: 0:59:40  lr: 0.000020  loss: 18.8612  time: 0.5110  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1400/8650]  eta: 0:59:14  lr: 0.000020  loss: 34.6210  time: 0.5086  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1450/8650]  eta: 0:58:49  lr: 0.000020  loss: 14.2832  time: 0.5143  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1500/8650]  eta: 0:58:25  lr: 0.000020  loss: 36.6127  time: 0.5238  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1550/8650]  eta: 0:58:01  lr: 0.000020  loss: 29.3976  time: 0.5429  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1600/8650]  eta: 0:57:37  lr: 0.000020  loss: 32.4251  time: 0.5561  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1650/8650]  eta: 0:57:12  lr: 0.000020  loss: 29.5838  time: 0.5524  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1700/8650]  eta: 0:56:47  lr: 0.000020  loss: 23.4966  time: 0.5492  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [1750/8650]  eta: 0:56:23  lr: 0.000020  loss: 18.0073  time: 0.5639  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [1800/8650]  eta: 0:55:58  lr: 0.000020  loss: 12.9533  time: 0.5604  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1850/8650]  eta: 0:55:33  lr: 0.000020  loss: 16.7455  time: 0.5587  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [2]  [1900/8650]  eta: 0:55:07  lr: 0.000020  loss: 27.9282  time: 0.5574  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [1950/8650]  eta: 0:54:42  lr: 0.000020  loss: 26.8558  time: 0.5663  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2000/8650]  eta: 0:54:18  lr: 0.000020  loss: 23.7082  time: 0.5820  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2050/8650]  eta: 0:53:54  lr: 0.000020  loss: 18.5835  time: 0.5880  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [2]  [2100/8650]  eta: 0:53:30  lr: 0.000020  loss: 19.2657  time: 0.5940  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [2]  [2150/8650]  eta: 0:53:05  lr: 0.000020  loss: 14.9809  time: 0.5829  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2200/8650]  eta: 0:52:40  lr: 0.000020  loss: 17.3706  time: 0.5940  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [2250/8650]  eta: 0:52:15  lr: 0.000020  loss: 17.6448  time: 0.5964  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2300/8650]  eta: 0:51:51  lr: 0.000020  loss: 27.0103  time: 0.5935  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2350/8650]  eta: 0:51:26  lr: 0.000020  loss: 23.0971  time: 0.5907  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [2400/8650]  eta: 0:51:02  lr: 0.000020  loss: 20.5081  time: 0.5892  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [2450/8650]  eta: 0:50:37  lr: 0.000020  loss: 20.4577  time: 0.5872  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2500/8650]  eta: 0:50:11  lr: 0.000020  loss: 13.9239  time: 0.5719  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2550/8650]  eta: 0:49:46  lr: 0.000020  loss: 15.3294  time: 0.5766  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2600/8650]  eta: 0:49:21  lr: 0.000020  loss: 23.4801  time: 0.5736  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2650/8650]  eta: 0:48:56  lr: 0.000020  loss: 19.1022  time: 0.5645  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2700/8650]  eta: 0:48:32  lr: 0.000020  loss: 13.9283  time: 0.5853  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2750/8650]  eta: 0:48:07  lr: 0.000020  loss: 14.2848  time: 0.5698  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [2800/8650]  eta: 0:47:42  lr: 0.000020  loss: 22.3524  time: 0.5632  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2850/8650]  eta: 0:47:17  lr: 0.000020  loss: 18.8097  time: 0.5524  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2900/8650]  eta: 0:46:52  lr: 0.000020  loss: 14.6909  time: 0.5469  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2950/8650]  eta: 0:46:27  lr: 0.000020  loss: 38.7368  time: 0.5373  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3000/8650]  eta: 0:46:02  lr: 0.000020  loss: 23.0471  time: 0.5232  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3050/8650]  eta: 0:45:37  lr: 0.000020  loss: 18.9486  time: 0.5268  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3100/8650]  eta: 0:45:13  lr: 0.000020  loss: 11.1348  time: 0.5241  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3150/8650]  eta: 0:44:48  lr: 0.000020  loss: 10.4135  time: 0.5077  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3200/8650]  eta: 0:44:23  lr: 0.000020  loss: 24.7286  time: 0.4886  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3250/8650]  eta: 0:43:59  lr: 0.000020  loss: 24.3019  time: 0.4855  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3300/8650]  eta: 0:43:34  lr: 0.000020  loss: 16.5433  time: 0.4830  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3350/8650]  eta: 0:43:10  lr: 0.000020  loss: 16.2959  time: 0.4875  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3400/8650]  eta: 0:42:45  lr: 0.000020  loss: 22.4982  time: 0.4930  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3450/8650]  eta: 0:42:20  lr: 0.000020  loss: 20.5334  time: 0.4828  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3500/8650]  eta: 0:41:55  lr: 0.000020  loss: 12.4321  time: 0.4606  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3550/8650]  eta: 0:41:30  lr: 0.000020  loss: 15.6064  time: 0.4610  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3600/8650]  eta: 0:41:06  lr: 0.000020  loss: 28.5957  time: 0.4694  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3650/8650]  eta: 0:40:41  lr: 0.000020  loss: 27.8610  time: 0.4664  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3700/8650]  eta: 0:40:16  lr: 0.000020  loss: 45.4184  time: 0.4641  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3750/8650]  eta: 0:39:52  lr: 0.000020  loss: 15.8657  time: 0.4535  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3800/8650]  eta: 0:39:27  lr: 0.000020  loss: 28.7267  time: 0.4404  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3850/8650]  eta: 0:39:03  lr: 0.000020  loss: 11.2155  time: 0.4455  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [3900/8650]  eta: 0:38:39  lr: 0.000020  loss: 22.4864  time: 0.4438  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [3950/8650]  eta: 0:38:14  lr: 0.000020  loss: 19.6391  time: 0.4281  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4000/8650]  eta: 0:37:50  lr: 0.000020  loss: 19.9518  time: 0.4357  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4050/8650]  eta: 0:37:25  lr: 0.000020  loss: 23.6136  time: 0.4392  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4100/8650]  eta: 0:37:01  lr: 0.000020  loss: 28.0065  time: 0.4389  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4150/8650]  eta: 0:36:36  lr: 0.000020  loss: 30.8591  time: 0.4295  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [4200/8650]  eta: 0:36:11  lr: 0.000020  loss: 10.3012  time: 0.4299  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4250/8650]  eta: 0:35:47  lr: 0.000020  loss: 15.7053  time: 0.4277  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4300/8650]  eta: 0:35:22  lr: 0.000020  loss: 22.4956  time: 0.4241  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4350/8650]  eta: 0:34:58  lr: 0.000020  loss: 19.8158  time: 0.4274  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4400/8650]  eta: 0:34:33  lr: 0.000020  loss: 18.8622  time: 0.4285  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4450/8650]  eta: 0:34:08  lr: 0.000020  loss: 27.9112  time: 0.4211  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4500/8650]  eta: 0:33:44  lr: 0.000020  loss: 18.0065  time: 0.4159  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4550/8650]  eta: 0:33:20  lr: 0.000020  loss: 26.4398  time: 0.4229  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4600/8650]  eta: 0:32:55  lr: 0.000020  loss: 18.6580  time: 0.4180  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [4650/8650]  eta: 0:32:31  lr: 0.000020  loss: 26.3294  time: 0.4206  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4700/8650]  eta: 0:32:06  lr: 0.000020  loss: 15.2361  time: 0.4135  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4750/8650]  eta: 0:31:41  lr: 0.000020  loss: 23.4677  time: 0.4159  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4800/8650]  eta: 0:31:17  lr: 0.000020  loss: 18.3755  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4850/8650]  eta: 0:30:53  lr: 0.000020  loss: 19.6894  time: 0.4118  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4900/8650]  eta: 0:30:28  lr: 0.000020  loss: 18.0252  time: 0.4111  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4950/8650]  eta: 0:30:03  lr: 0.000020  loss: 26.3030  time: 0.4132  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5000/8650]  eta: 0:29:39  lr: 0.000020  loss: 19.2550  time: 0.4123  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5050/8650]  eta: 0:29:14  lr: 0.000020  loss: 19.6678  time: 0.4077  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [5100/8650]  eta: 0:28:50  lr: 0.000020  loss: 29.5000  time: 0.4046  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5150/8650]  eta: 0:28:25  lr: 0.000020  loss: 20.2661  time: 0.4040  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5200/8650]  eta: 0:28:01  lr: 0.000020  loss: 30.6165  time: 0.4058  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5250/8650]  eta: 0:27:36  lr: 0.000020  loss: 22.1443  time: 0.4000  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5300/8650]  eta: 0:27:12  lr: 0.000020  loss: 22.1997  time: 0.3987  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5350/8650]  eta: 0:26:47  lr: 0.000020  loss: 20.0713  time: 0.4014  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5400/8650]  eta: 0:26:23  lr: 0.000020  loss: 20.0836  time: 0.4008  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5450/8650]  eta: 0:25:58  lr: 0.000020  loss: 25.4870  time: 0.4002  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5500/8650]  eta: 0:25:34  lr: 0.000020  loss: 23.4917  time: 0.3999  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5550/8650]  eta: 0:25:10  lr: 0.000020  loss: 19.4156  time: 0.3973  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5600/8650]  eta: 0:24:45  lr: 0.000020  loss: 20.0541  time: 0.3978  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5650/8650]  eta: 0:24:21  lr: 0.000020  loss: 16.1023  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5700/8650]  eta: 0:23:56  lr: 0.000020  loss: 23.8969  time: 0.3956  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5750/8650]  eta: 0:23:32  lr: 0.000020  loss: 24.9469  time: 0.3987  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5800/8650]  eta: 0:23:07  lr: 0.000020  loss: 14.8104  time: 0.3985  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5850/8650]  eta: 0:22:43  lr: 0.000020  loss: 30.2436  time: 0.3984  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5900/8650]  eta: 0:22:18  lr: 0.000020  loss: 20.2069  time: 0.4023  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5950/8650]  eta: 0:21:54  lr: 0.000020  loss: 16.5891  time: 0.3969  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6000/8650]  eta: 0:21:29  lr: 0.000020  loss: 15.4188  time: 0.3995  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6050/8650]  eta: 0:21:05  lr: 0.000020  loss: 17.2330  time: 0.4045  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6100/8650]  eta: 0:20:41  lr: 0.000020  loss: 42.9411  time: 0.4095  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [6150/8650]  eta: 0:20:16  lr: 0.000020  loss: 20.3841  time: 0.4086  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6200/8650]  eta: 0:19:52  lr: 0.000020  loss: 36.5661  time: 0.4009  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6250/8650]  eta: 0:19:27  lr: 0.000020  loss: 29.8419  time: 0.4088  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [6300/8650]  eta: 0:19:03  lr: 0.000020  loss: 24.9038  time: 0.4072  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6350/8650]  eta: 0:18:39  lr: 0.000020  loss: 24.2387  time: 0.3994  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6400/8650]  eta: 0:18:14  lr: 0.000020  loss: 30.8603  time: 0.4046  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6450/8650]  eta: 0:17:50  lr: 0.000020  loss: 27.2504  time: 0.4071  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6500/8650]  eta: 0:17:25  lr: 0.000020  loss: 18.4068  time: 0.4060  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6550/8650]  eta: 0:17:01  lr: 0.000020  loss: 23.0896  time: 0.3973  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6600/8650]  eta: 0:16:36  lr: 0.000020  loss: 28.5007  time: 0.4026  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6650/8650]  eta: 0:16:12  lr: 0.000020  loss: 14.2743  time: 0.3990  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6700/8650]  eta: 0:15:48  lr: 0.000020  loss: 27.9416  time: 0.3974  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6750/8650]  eta: 0:15:23  lr: 0.000020  loss: 10.9004  time: 0.3971  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6800/8650]  eta: 0:14:59  lr: 0.000020  loss: 19.7534  time: 0.3951  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6850/8650]  eta: 0:14:35  lr: 0.000020  loss: 19.8190  time: 0.3986  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6900/8650]  eta: 0:14:10  lr: 0.000020  loss: 13.7917  time: 0.3967  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6950/8650]  eta: 0:13:46  lr: 0.000020  loss: 20.5787  time: 0.3943  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7000/8650]  eta: 0:13:21  lr: 0.000020  loss: 24.9594  time: 0.4044  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7050/8650]  eta: 0:12:57  lr: 0.000020  loss: 26.6349  time: 0.3963  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7100/8650]  eta: 0:12:33  lr: 0.000020  loss: 11.3433  time: 0.4141  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7150/8650]  eta: 0:12:09  lr: 0.000020  loss: 18.7803  time: 0.4195  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [7200/8650]  eta: 0:11:44  lr: 0.000020  loss: 28.9034  time: 0.4261  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7250/8650]  eta: 0:11:20  lr: 0.000020  loss: 36.1561  time: 0.4515  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7300/8650]  eta: 0:10:56  lr: 0.000020  loss: 19.7313  time: 0.4493  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7350/8650]  eta: 0:10:31  lr: 0.000020  loss: 28.8929  time: 0.4602  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [7400/8650]  eta: 0:10:07  lr: 0.000020  loss: 19.8307  time: 0.4629  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [7450/8650]  eta: 0:09:43  lr: 0.000020  loss: 20.7325  time: 0.4731  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7500/8650]  eta: 0:09:18  lr: 0.000020  loss: 29.1672  time: 0.4743  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7550/8650]  eta: 0:08:54  lr: 0.000020  loss: 16.7650  time: 0.4909  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7600/8650]  eta: 0:08:30  lr: 0.000020  loss: 20.5881  time: 0.4770  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7650/8650]  eta: 0:08:05  lr: 0.000020  loss: 22.4282  time: 0.4689  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7700/8650]  eta: 0:07:41  lr: 0.000020  loss: 21.4344  time: 0.4869  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7750/8650]  eta: 0:07:17  lr: 0.000020  loss: 16.9838  time: 0.4798  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7800/8650]  eta: 0:06:52  lr: 0.000020  loss: 21.4201  time: 0.4923  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7850/8650]  eta: 0:06:28  lr: 0.000020  loss: 21.2232  time: 0.4950  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7900/8650]  eta: 0:06:04  lr: 0.000020  loss: 27.5584  time: 0.4860  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [7950/8650]  eta: 0:05:40  lr: 0.000020  loss: 31.5639  time: 0.4880  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [8000/8650]  eta: 0:05:15  lr: 0.000020  loss: 26.7422  time: 0.4699  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [8050/8650]  eta: 0:04:51  lr: 0.000020  loss: 20.6335  time: 0.4702  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8100/8650]  eta: 0:04:27  lr: 0.000020  loss: 27.0721  time: 0.4801  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8150/8650]  eta: 0:04:02  lr: 0.000020  loss: 27.7795  time: 0.4678  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [8200/8650]  eta: 0:03:38  lr: 0.000020  loss: 20.8287  time: 0.4843  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [8250/8650]  eta: 0:03:14  lr: 0.000020  loss: 14.3124  time: 0.4841  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8300/8650]  eta: 0:02:49  lr: 0.000020  loss: 22.1478  time: 0.4854  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8350/8650]  eta: 0:02:25  lr: 0.000020  loss: 17.8465  time: 0.4838  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8400/8650]  eta: 0:02:01  lr: 0.000020  loss: 42.7810  time: 0.4871  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [8450/8650]  eta: 0:01:37  lr: 0.000020  loss: 19.0958  time: 0.4774  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8500/8650]  eta: 0:01:12  lr: 0.000020  loss: 18.9799  time: 0.4925  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8550/8650]  eta: 0:00:48  lr: 0.000020  loss: 36.5626  time: 0.4939  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 25.2706  time: 0.4894  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 14.4837  time: 0.5120  data: 0.0023  max mem: 9086\n",
            "Train Epoch: [2] Total time: 1:09:58 (0.4854 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 21.8291\n",
            "Loss: 19.6089\n",
            "==========================================\n",
            "Train Epoch: [3]  [   0/8650]  eta: 4:31:26  lr: 0.000020  loss: 19.6089  time: 1.8828  data: 1.2469  max mem: 9086\n",
            "Loss: 28.7072\n",
            "==========================================\n",
            "Loss: 23.2962\n",
            "==========================================\n",
            "Loss: 29.3774\n",
            "==========================================\n",
            "Loss: 18.6624\n",
            "==========================================\n",
            "Loss: 12.4308\n",
            "==========================================\n",
            "Loss: 24.8697\n",
            "==========================================\n",
            "Loss: 9.6817\n",
            "==========================================\n",
            "Loss: 10.9194\n",
            "==========================================\n",
            "Loss: 20.4439\n",
            "==========================================\n",
            "Loss: 21.2823\n",
            "==========================================\n",
            "Loss: 12.1539\n",
            "==========================================\n",
            "Loss: 18.9392\n",
            "==========================================\n",
            "Loss: 23.5133\n",
            "==========================================\n",
            "Loss: 16.0815\n",
            "==========================================\n",
            "Loss: 14.6606\n",
            "==========================================\n",
            "Loss: 25.5036\n",
            "==========================================\n",
            "Loss: 17.6793\n",
            "==========================================\n",
            "Loss: 16.9793\n",
            "==========================================\n",
            "Loss: 34.1287\n",
            "==========================================\n",
            "Loss: 16.027\n",
            "==========================================\n",
            "Loss: 19.3753\n",
            "==========================================\n",
            "Loss: 19.0314\n",
            "==========================================\n",
            "Loss: 25.1697\n",
            "==========================================\n",
            "Loss: 27.0098\n",
            "==========================================\n",
            "Loss: 17.5787\n",
            "==========================================\n",
            "Loss: 29.6663\n",
            "==========================================\n",
            "Loss: 27.6868\n",
            "==========================================\n",
            "Loss: 17.392\n",
            "==========================================\n",
            "Loss: 15.0832\n",
            "==========================================\n",
            "Loss: 20.0602\n",
            "==========================================\n",
            "Loss: 37.875\n",
            "==========================================\n",
            "Loss: 21.151\n",
            "==========================================\n",
            "Loss: 14.6948\n",
            "==========================================\n",
            "Loss: 13.6557\n",
            "==========================================\n",
            "Loss: 17.8474\n",
            "==========================================\n",
            "Loss: 18.058\n",
            "==========================================\n",
            "Loss: 17.5923\n",
            "==========================================\n",
            "Loss: 28.5652\n",
            "==========================================\n",
            "Loss: 21.808\n",
            "==========================================\n",
            "Loss: 20.7396\n",
            "==========================================\n",
            "Loss: 16.5394\n",
            "==========================================\n",
            "Loss: 22.2349\n",
            "==========================================\n",
            "Loss: 33.2628\n",
            "==========================================\n",
            "Loss: 11.9715\n",
            "==========================================\n",
            "Loss: 26.03\n",
            "==========================================\n",
            "Loss: 22.6692\n",
            "==========================================\n",
            "Loss: 21.9544\n",
            "==========================================\n",
            "Loss: 12.3468\n",
            "==========================================\n",
            "Loss: 20.5349\n",
            "==========================================\n",
            "Train Epoch: [3]  [  50/8650]  eta: 1:28:35  lr: 0.000020  loss: 39.8198  time: 0.6487  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [ 100/8650]  eta: 1:19:29  lr: 0.000020  loss: 26.1111  time: 0.4956  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 150/8650]  eta: 1:15:23  lr: 0.000020  loss: 16.0852  time: 0.4850  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 200/8650]  eta: 1:12:57  lr: 0.000020  loss: 24.8746  time: 0.4780  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 250/8650]  eta: 1:11:14  lr: 0.000020  loss: 24.1433  time: 0.4636  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 300/8650]  eta: 1:10:03  lr: 0.000020  loss: 23.3481  time: 0.4639  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 350/8650]  eta: 1:09:05  lr: 0.000020  loss: 19.3698  time: 0.4584  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 400/8650]  eta: 1:08:21  lr: 0.000020  loss: 9.1068  time: 0.4647  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [ 450/8650]  eta: 1:07:38  lr: 0.000020  loss: 22.6186  time: 0.4530  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 500/8650]  eta: 1:06:58  lr: 0.000020  loss: 18.3626  time: 0.4307  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 550/8650]  eta: 1:06:21  lr: 0.000020  loss: 17.4186  time: 0.4255  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 600/8650]  eta: 1:05:46  lr: 0.000020  loss: 10.2891  time: 0.4181  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 650/8650]  eta: 1:05:14  lr: 0.000020  loss: 23.4872  time: 0.4039  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 700/8650]  eta: 1:04:41  lr: 0.000020  loss: 21.3439  time: 0.3953  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 750/8650]  eta: 1:04:12  lr: 0.000020  loss: 20.4008  time: 0.4033  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 800/8650]  eta: 1:03:43  lr: 0.000020  loss: 22.4207  time: 0.3937  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 850/8650]  eta: 1:03:15  lr: 0.000020  loss: 24.2646  time: 0.3959  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 900/8650]  eta: 1:02:46  lr: 0.000020  loss: 17.6223  time: 0.3944  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 950/8650]  eta: 1:02:18  lr: 0.000020  loss: 19.8883  time: 0.3937  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1000/8650]  eta: 1:01:50  lr: 0.000020  loss: 24.2201  time: 0.3943  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1050/8650]  eta: 1:01:23  lr: 0.000020  loss: 13.4294  time: 0.4000  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1100/8650]  eta: 1:00:57  lr: 0.000020  loss: 16.8049  time: 0.4080  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1150/8650]  eta: 1:00:31  lr: 0.000020  loss: 16.4902  time: 0.3948  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1200/8650]  eta: 1:00:05  lr: 0.000020  loss: 11.6093  time: 0.3945  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1250/8650]  eta: 0:59:40  lr: 0.000020  loss: 24.7252  time: 0.4055  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1300/8650]  eta: 0:59:18  lr: 0.000020  loss: 32.1684  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1350/8650]  eta: 0:58:53  lr: 0.000020  loss: 22.6381  time: 0.3978  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1400/8650]  eta: 0:58:28  lr: 0.000020  loss: 30.5079  time: 0.4019  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1450/8650]  eta: 0:58:03  lr: 0.000020  loss: 22.7810  time: 0.3963  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1500/8650]  eta: 0:57:38  lr: 0.000020  loss: 33.5802  time: 0.3966  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1550/8650]  eta: 0:57:13  lr: 0.000020  loss: 14.1495  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1600/8650]  eta: 0:56:48  lr: 0.000020  loss: 21.4025  time: 0.3992  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1650/8650]  eta: 0:56:23  lr: 0.000020  loss: 20.0984  time: 0.4017  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1700/8650]  eta: 0:55:59  lr: 0.000020  loss: 26.2302  time: 0.4053  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1750/8650]  eta: 0:55:34  lr: 0.000020  loss: 19.3077  time: 0.3979  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1800/8650]  eta: 0:55:09  lr: 0.000020  loss: 17.3304  time: 0.3987  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1850/8650]  eta: 0:54:44  lr: 0.000020  loss: 30.9992  time: 0.4016  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1900/8650]  eta: 0:54:19  lr: 0.000020  loss: 12.8047  time: 0.4060  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1950/8650]  eta: 0:53:53  lr: 0.000020  loss: 15.8736  time: 0.4067  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2000/8650]  eta: 0:53:28  lr: 0.000020  loss: 28.2956  time: 0.4111  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2050/8650]  eta: 0:53:05  lr: 0.000020  loss: 32.5037  time: 0.4183  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2100/8650]  eta: 0:52:42  lr: 0.000020  loss: 18.1520  time: 0.4131  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2150/8650]  eta: 0:52:18  lr: 0.000020  loss: 32.9616  time: 0.4067  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2200/8650]  eta: 0:51:54  lr: 0.000020  loss: 17.9501  time: 0.4098  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2250/8650]  eta: 0:51:29  lr: 0.000020  loss: 17.3497  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2300/8650]  eta: 0:51:06  lr: 0.000020  loss: 23.4581  time: 0.4054  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2350/8650]  eta: 0:50:42  lr: 0.000020  loss: 21.1485  time: 0.4053  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2400/8650]  eta: 0:50:18  lr: 0.000020  loss: 36.1766  time: 0.4024  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2450/8650]  eta: 0:49:54  lr: 0.000020  loss: 26.1741  time: 0.3997  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2500/8650]  eta: 0:49:30  lr: 0.000020  loss: 36.3357  time: 0.4033  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2550/8650]  eta: 0:49:05  lr: 0.000020  loss: 14.5308  time: 0.3984  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2600/8650]  eta: 0:48:40  lr: 0.000020  loss: 21.0431  time: 0.4046  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2650/8650]  eta: 0:48:17  lr: 0.000020  loss: 26.2812  time: 0.4021  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2700/8650]  eta: 0:47:52  lr: 0.000020  loss: 20.7054  time: 0.4002  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2750/8650]  eta: 0:47:29  lr: 0.000020  loss: 23.4867  time: 0.3956  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2800/8650]  eta: 0:47:05  lr: 0.000020  loss: 15.2385  time: 0.4022  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2850/8650]  eta: 0:46:41  lr: 0.000020  loss: 25.2625  time: 0.3978  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2900/8650]  eta: 0:46:17  lr: 0.000020  loss: 18.9691  time: 0.4078  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2950/8650]  eta: 0:45:53  lr: 0.000020  loss: 26.2932  time: 0.4005  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3000/8650]  eta: 0:45:28  lr: 0.000020  loss: 20.5335  time: 0.3981  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3050/8650]  eta: 0:45:04  lr: 0.000020  loss: 17.3782  time: 0.4030  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3100/8650]  eta: 0:44:41  lr: 0.000020  loss: 22.8045  time: 0.4167  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3150/8650]  eta: 0:44:17  lr: 0.000020  loss: 24.8518  time: 0.4208  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3200/8650]  eta: 0:43:53  lr: 0.000020  loss: 26.2357  time: 0.4126  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3250/8650]  eta: 0:43:29  lr: 0.000020  loss: 33.1178  time: 0.4241  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [3300/8650]  eta: 0:43:04  lr: 0.000020  loss: 17.0399  time: 0.4203  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3350/8650]  eta: 0:42:40  lr: 0.000020  loss: 43.9126  time: 0.4234  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3400/8650]  eta: 0:42:16  lr: 0.000020  loss: 17.7728  time: 0.4167  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3450/8650]  eta: 0:41:52  lr: 0.000020  loss: 26.9128  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3500/8650]  eta: 0:41:27  lr: 0.000020  loss: 23.6383  time: 0.4043  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3550/8650]  eta: 0:41:03  lr: 0.000020  loss: 19.2553  time: 0.4054  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3600/8650]  eta: 0:40:38  lr: 0.000020  loss: 27.8970  time: 0.4083  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3650/8650]  eta: 0:40:14  lr: 0.000020  loss: 18.4425  time: 0.4037  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3700/8650]  eta: 0:39:50  lr: 0.000020  loss: 18.8351  time: 0.4113  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3750/8650]  eta: 0:39:25  lr: 0.000020  loss: 15.9258  time: 0.4098  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3800/8650]  eta: 0:39:01  lr: 0.000020  loss: 20.9612  time: 0.4123  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3850/8650]  eta: 0:38:37  lr: 0.000020  loss: 38.2474  time: 0.4188  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3900/8650]  eta: 0:38:12  lr: 0.000020  loss: 27.4093  time: 0.4039  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3950/8650]  eta: 0:37:48  lr: 0.000020  loss: 17.5539  time: 0.4070  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4000/8650]  eta: 0:37:24  lr: 0.000020  loss: 19.9172  time: 0.4135  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4050/8650]  eta: 0:36:59  lr: 0.000020  loss: 14.8258  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4100/8650]  eta: 0:36:35  lr: 0.000020  loss: 19.3086  time: 0.4114  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4150/8650]  eta: 0:36:11  lr: 0.000020  loss: 24.9015  time: 0.4050  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4200/8650]  eta: 0:35:46  lr: 0.000020  loss: 22.9704  time: 0.4038  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4250/8650]  eta: 0:35:22  lr: 0.000020  loss: 14.1597  time: 0.4036  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4300/8650]  eta: 0:34:58  lr: 0.000020  loss: 33.5811  time: 0.4100  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4350/8650]  eta: 0:34:33  lr: 0.000020  loss: 14.1920  time: 0.4103  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4400/8650]  eta: 0:34:09  lr: 0.000020  loss: 25.8344  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4450/8650]  eta: 0:33:45  lr: 0.000020  loss: 23.5756  time: 0.4098  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4500/8650]  eta: 0:33:21  lr: 0.000020  loss: 28.0882  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4550/8650]  eta: 0:32:57  lr: 0.000020  loss: 14.7157  time: 0.4216  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4600/8650]  eta: 0:32:33  lr: 0.000020  loss: 38.2125  time: 0.4157  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4650/8650]  eta: 0:32:09  lr: 0.000020  loss: 15.7320  time: 0.4248  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4700/8650]  eta: 0:31:45  lr: 0.000020  loss: 28.4674  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4750/8650]  eta: 0:31:21  lr: 0.000020  loss: 31.4488  time: 0.4145  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4800/8650]  eta: 0:30:57  lr: 0.000020  loss: 19.2305  time: 0.4265  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4850/8650]  eta: 0:30:32  lr: 0.000020  loss: 20.7254  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4900/8650]  eta: 0:30:08  lr: 0.000020  loss: 31.1350  time: 0.4220  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4950/8650]  eta: 0:29:44  lr: 0.000020  loss: 18.3938  time: 0.4218  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5000/8650]  eta: 0:29:20  lr: 0.000020  loss: 14.9017  time: 0.4298  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5050/8650]  eta: 0:28:56  lr: 0.000020  loss: 19.0212  time: 0.4200  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5100/8650]  eta: 0:28:31  lr: 0.000020  loss: 20.4151  time: 0.4221  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5150/8650]  eta: 0:28:07  lr: 0.000020  loss: 44.8245  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5200/8650]  eta: 0:27:43  lr: 0.000020  loss: 16.8196  time: 0.4147  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5250/8650]  eta: 0:27:19  lr: 0.000020  loss: 24.6476  time: 0.4190  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5300/8650]  eta: 0:26:55  lr: 0.000020  loss: 16.7823  time: 0.4181  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5350/8650]  eta: 0:26:31  lr: 0.000020  loss: 18.9774  time: 0.4173  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5400/8650]  eta: 0:26:07  lr: 0.000020  loss: 15.2866  time: 0.4092  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5450/8650]  eta: 0:25:43  lr: 0.000020  loss: 23.1636  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5500/8650]  eta: 0:25:19  lr: 0.000020  loss: 30.2001  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5550/8650]  eta: 0:24:55  lr: 0.000020  loss: 22.3411  time: 0.4152  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5600/8650]  eta: 0:24:31  lr: 0.000020  loss: 16.4408  time: 0.4241  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5650/8650]  eta: 0:24:06  lr: 0.000020  loss: 17.4072  time: 0.4214  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5700/8650]  eta: 0:23:42  lr: 0.000020  loss: 17.4916  time: 0.4196  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5750/8650]  eta: 0:23:18  lr: 0.000020  loss: 14.1639  time: 0.4261  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5800/8650]  eta: 0:22:54  lr: 0.000020  loss: 21.5745  time: 0.4230  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5850/8650]  eta: 0:22:30  lr: 0.000020  loss: 15.2252  time: 0.4448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [5900/8650]  eta: 0:22:06  lr: 0.000020  loss: 14.3274  time: 0.4324  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5950/8650]  eta: 0:21:42  lr: 0.000020  loss: 22.2436  time: 0.4302  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6000/8650]  eta: 0:21:18  lr: 0.000020  loss: 20.9935  time: 0.4356  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6050/8650]  eta: 0:20:54  lr: 0.000020  loss: 18.2347  time: 0.4406  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6100/8650]  eta: 0:20:30  lr: 0.000020  loss: 24.8378  time: 0.4296  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6150/8650]  eta: 0:20:06  lr: 0.000020  loss: 19.3827  time: 0.4182  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6200/8650]  eta: 0:19:42  lr: 0.000020  loss: 14.7582  time: 0.4136  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6250/8650]  eta: 0:19:17  lr: 0.000020  loss: 17.9009  time: 0.4108  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6300/8650]  eta: 0:18:53  lr: 0.000020  loss: 34.5024  time: 0.4267  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6350/8650]  eta: 0:18:29  lr: 0.000020  loss: 21.2634  time: 0.4317  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6400/8650]  eta: 0:18:05  lr: 0.000020  loss: 26.0872  time: 0.4205  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6450/8650]  eta: 0:17:41  lr: 0.000020  loss: 16.3390  time: 0.4274  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6500/8650]  eta: 0:17:17  lr: 0.000020  loss: 28.3923  time: 0.4409  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6550/8650]  eta: 0:16:53  lr: 0.000020  loss: 16.4090  time: 0.4381  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6600/8650]  eta: 0:16:29  lr: 0.000020  loss: 25.6794  time: 0.4429  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6650/8650]  eta: 0:16:05  lr: 0.000020  loss: 15.1077  time: 0.4429  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6700/8650]  eta: 0:15:41  lr: 0.000020  loss: 21.8213  time: 0.4670  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6750/8650]  eta: 0:15:17  lr: 0.000020  loss: 14.8677  time: 0.4595  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6800/8650]  eta: 0:14:52  lr: 0.000020  loss: 18.3105  time: 0.4694  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6850/8650]  eta: 0:14:28  lr: 0.000020  loss: 17.5540  time: 0.4572  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6900/8650]  eta: 0:14:04  lr: 0.000020  loss: 23.6409  time: 0.4591  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6950/8650]  eta: 0:13:40  lr: 0.000020  loss: 19.3066  time: 0.4478  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7000/8650]  eta: 0:13:16  lr: 0.000020  loss: 13.6403  time: 0.4390  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7050/8650]  eta: 0:12:52  lr: 0.000020  loss: 22.8851  time: 0.4329  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7100/8650]  eta: 0:12:28  lr: 0.000020  loss: 16.8957  time: 0.4315  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7150/8650]  eta: 0:12:03  lr: 0.000020  loss: 25.2059  time: 0.4289  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7200/8650]  eta: 0:11:39  lr: 0.000020  loss: 13.0499  time: 0.4268  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7250/8650]  eta: 0:11:15  lr: 0.000020  loss: 23.1814  time: 0.4199  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7300/8650]  eta: 0:10:51  lr: 0.000020  loss: 21.6755  time: 0.4290  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7350/8650]  eta: 0:10:27  lr: 0.000020  loss: 20.2486  time: 0.4205  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7400/8650]  eta: 0:10:03  lr: 0.000020  loss: 17.4972  time: 0.4134  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7450/8650]  eta: 0:09:39  lr: 0.000020  loss: 20.9969  time: 0.4124  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7500/8650]  eta: 0:09:15  lr: 0.000020  loss: 34.8755  time: 0.4074  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7550/8650]  eta: 0:08:50  lr: 0.000020  loss: 18.1990  time: 0.4097  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7600/8650]  eta: 0:08:26  lr: 0.000020  loss: 22.1834  time: 0.4135  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7650/8650]  eta: 0:08:02  lr: 0.000020  loss: 23.9129  time: 0.4161  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7700/8650]  eta: 0:07:38  lr: 0.000020  loss: 19.9888  time: 0.4068  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7750/8650]  eta: 0:07:14  lr: 0.000020  loss: 28.8108  time: 0.4109  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7800/8650]  eta: 0:06:50  lr: 0.000020  loss: 23.1440  time: 0.4222  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7850/8650]  eta: 0:06:26  lr: 0.000020  loss: 17.2700  time: 0.4214  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7900/8650]  eta: 0:06:02  lr: 0.000020  loss: 15.5459  time: 0.4176  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7950/8650]  eta: 0:05:38  lr: 0.000020  loss: 28.5008  time: 0.4163  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8000/8650]  eta: 0:05:13  lr: 0.000020  loss: 22.1876  time: 0.4068  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8050/8650]  eta: 0:04:49  lr: 0.000020  loss: 24.3288  time: 0.4037  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8100/8650]  eta: 0:04:25  lr: 0.000020  loss: 16.9213  time: 0.4085  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8150/8650]  eta: 0:04:01  lr: 0.000020  loss: 24.7060  time: 0.4008  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8200/8650]  eta: 0:03:37  lr: 0.000020  loss: 15.2431  time: 0.4197  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8250/8650]  eta: 0:03:13  lr: 0.000020  loss: 17.4936  time: 0.4031  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8300/8650]  eta: 0:02:48  lr: 0.000020  loss: 27.1732  time: 0.4025  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8350/8650]  eta: 0:02:24  lr: 0.000020  loss: 25.2996  time: 0.4012  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8400/8650]  eta: 0:02:00  lr: 0.000020  loss: 11.5288  time: 0.4050  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8450/8650]  eta: 0:01:36  lr: 0.000020  loss: 14.5047  time: 0.3991  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [8500/8650]  eta: 0:01:12  lr: 0.000020  loss: 24.5020  time: 0.4041  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8550/8650]  eta: 0:00:48  lr: 0.000020  loss: 15.6698  time: 0.4064  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 13.8819  time: 0.4135  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 12.7587  time: 0.3968  data: 0.0019  max mem: 9086\n",
            "Train Epoch: [3] Total time: 1:09:36 (0.4829 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 21.3628\n",
            "Loss: 18.5198\n",
            "==========================================\n",
            "Train Epoch: [4]  [   0/8650]  eta: 5:10:59  lr: 0.000019  loss: 18.5198  time: 2.1572  data: 1.2738  max mem: 9086\n",
            "Loss: 16.5065\n",
            "==========================================\n",
            "Loss: 18.0017\n",
            "==========================================\n",
            "Loss: 13.3822\n",
            "==========================================\n",
            "Loss: 33.1169\n",
            "==========================================\n",
            "Loss: 22.7271\n",
            "==========================================\n",
            "Loss: 12.3256\n",
            "==========================================\n",
            "Loss: 19.3067\n",
            "==========================================\n",
            "Loss: 31.997\n",
            "==========================================\n",
            "Loss: 26.323\n",
            "==========================================\n",
            "Loss: 21.7696\n",
            "==========================================\n",
            "Loss: 24.1912\n",
            "==========================================\n",
            "Loss: 12.5552\n",
            "==========================================\n",
            "Loss: 16.7061\n",
            "==========================================\n",
            "Loss: 10.9904\n",
            "==========================================\n",
            "Loss: 17.9443\n",
            "==========================================\n",
            "Loss: 26.8903\n",
            "==========================================\n",
            "Loss: 19.9118\n",
            "==========================================\n",
            "Loss: 18.679\n",
            "==========================================\n",
            "Loss: 18.1486\n",
            "==========================================\n",
            "Loss: 20.2055\n",
            "==========================================\n",
            "Loss: 20.4086\n",
            "==========================================\n",
            "Loss: 33.3101\n",
            "==========================================\n",
            "Loss: 12.9444\n",
            "==========================================\n",
            "Loss: 13.8469\n",
            "==========================================\n",
            "Loss: 26.6386\n",
            "==========================================\n",
            "Loss: 14.8553\n",
            "==========================================\n",
            "Loss: 19.7982\n",
            "==========================================\n",
            "Loss: 31.0536\n",
            "==========================================\n",
            "Loss: 21.8713\n",
            "==========================================\n",
            "Loss: 43.7067\n",
            "==========================================\n",
            "Loss: 16.8581\n",
            "==========================================\n",
            "Loss: 13.1076\n",
            "==========================================\n",
            "Loss: 12.6602\n",
            "==========================================\n",
            "Loss: 25.2958\n",
            "==========================================\n",
            "Loss: 14.4593\n",
            "==========================================\n",
            "Loss: 16.1068\n",
            "==========================================\n",
            "Loss: 22.755\n",
            "==========================================\n",
            "Loss: 15.187\n",
            "==========================================\n",
            "Loss: 16.972\n",
            "==========================================\n",
            "Loss: 18.5301\n",
            "==========================================\n",
            "Loss: 20.2051\n",
            "==========================================\n",
            "Loss: 13.3034\n",
            "==========================================\n",
            "Loss: 12.9685\n",
            "==========================================\n",
            "Loss: 13.7839\n",
            "==========================================\n",
            "Loss: 28.4485\n",
            "==========================================\n",
            "Loss: 19.6185\n",
            "==========================================\n",
            "Loss: 16.4477\n",
            "==========================================\n",
            "Loss: 28.2101\n",
            "==========================================\n",
            "Loss: 21.6304\n",
            "==========================================\n",
            "Train Epoch: [4]  [  50/8650]  eta: 1:28:18  lr: 0.000019  loss: 24.8938  time: 0.5781  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [ 100/8650]  eta: 1:20:15  lr: 0.000019  loss: 19.3596  time: 0.5095  data: 0.0005  max mem: 9086\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 63, in train\n",
            "    metric_logger.update(loss=loss.item())\n",
            "                              ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_animals.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_animals \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA_pretrain_animals/checkpoint_01.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Textvqa + PretrainTextCaps"
      ],
      "metadata": {
        "id": "c1Xp2adpkVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_Textcaps.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_textcaps \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA_pretrain_animals/checkpoint_07.pth"
      ],
      "metadata": {
        "id": "9RnYLAjmkcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA + resnet"
      ],
      "metadata": {
        "id": "ZU3eOQrIfwJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/dataset_animals"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ekwts1oGwoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain.py --config configs/pre_animals.yaml --output_dir output/pretrain_animals --checkpoint output/pretrain_animals/checkpoint_22.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5gFi5qxQmH",
        "outputId": "41cbfa9f-fb90-4d33-abf7-62afd1e11954",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating dataset\n",
            "Đang tải dữ liệu từ: /content/dataset_animals/dataset.json\n",
            "number of training samples: 20000\n",
            "Creating model\n",
            "/embeddings/word_embeddings is tied\n",
            "/embeddings/position_embeddings is tied\n",
            "/embeddings/LayerNorm is tied\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "/encoder/layer/2/crossattention/self/query is tied\n",
            "/encoder/layer/2/crossattention/self/key is tied\n",
            "/encoder/layer/2/crossattention/self/value is tied\n",
            "/encoder/layer/2/crossattention/output/dense is tied\n",
            "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/2/intermediate/dense is tied\n",
            "/encoder/layer/2/output/dense is tied\n",
            "/encoder/layer/2/output/LayerNorm is tied\n",
            "/encoder/layer/3/crossattention/self/query is tied\n",
            "/encoder/layer/3/crossattention/self/key is tied\n",
            "/encoder/layer/3/crossattention/self/value is tied\n",
            "/encoder/layer/3/crossattention/output/dense is tied\n",
            "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/3/intermediate/dense is tied\n",
            "/encoder/layer/3/output/dense is tied\n",
            "/encoder/layer/3/output/LayerNorm is tied\n",
            "/encoder/layer/4/crossattention/self/query is tied\n",
            "/encoder/layer/4/crossattention/self/key is tied\n",
            "/encoder/layer/4/crossattention/self/value is tied\n",
            "/encoder/layer/4/crossattention/output/dense is tied\n",
            "/encoder/layer/4/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/4/intermediate/dense is tied\n",
            "/encoder/layer/4/output/dense is tied\n",
            "/encoder/layer/4/output/LayerNorm is tied\n",
            "/encoder/layer/5/crossattention/self/query is tied\n",
            "/encoder/layer/5/crossattention/self/key is tied\n",
            "/encoder/layer/5/crossattention/self/value is tied\n",
            "/encoder/layer/5/crossattention/output/dense is tied\n",
            "/encoder/layer/5/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/5/intermediate/dense is tied\n",
            "/encoder/layer/5/output/dense is tied\n",
            "/encoder/layer/5/output/LayerNorm is tied\n",
            "/encoder/layer/6/crossattention/self/query is tied\n",
            "/encoder/layer/6/crossattention/self/key is tied\n",
            "/encoder/layer/6/crossattention/self/value is tied\n",
            "/encoder/layer/6/crossattention/output/dense is tied\n",
            "/encoder/layer/6/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/6/intermediate/dense is tied\n",
            "/encoder/layer/6/output/dense is tied\n",
            "/encoder/layer/6/output/LayerNorm is tied\n",
            "/encoder/layer/7/crossattention/self/query is tied\n",
            "/encoder/layer/7/crossattention/self/key is tied\n",
            "/encoder/layer/7/crossattention/self/value is tied\n",
            "/encoder/layer/7/crossattention/output/dense is tied\n",
            "/encoder/layer/7/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/7/intermediate/dense is tied\n",
            "/encoder/layer/7/output/dense is tied\n",
            "/encoder/layer/7/output/LayerNorm is tied\n",
            "/encoder/layer/8/crossattention/self/query is tied\n",
            "/encoder/layer/8/crossattention/self/key is tied\n",
            "/encoder/layer/8/crossattention/self/value is tied\n",
            "/encoder/layer/8/crossattention/output/dense is tied\n",
            "/encoder/layer/8/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/8/intermediate/dense is tied\n",
            "/encoder/layer/8/output/dense is tied\n",
            "/encoder/layer/8/output/LayerNorm is tied\n",
            "/encoder/layer/9/crossattention/self/query is tied\n",
            "/encoder/layer/9/crossattention/self/key is tied\n",
            "/encoder/layer/9/crossattention/self/value is tied\n",
            "/encoder/layer/9/crossattention/output/dense is tied\n",
            "/encoder/layer/9/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/9/intermediate/dense is tied\n",
            "/encoder/layer/9/output/dense is tied\n",
            "/encoder/layer/9/output/LayerNorm is tied\n",
            "/encoder/layer/10/crossattention/self/query is tied\n",
            "/encoder/layer/10/crossattention/self/key is tied\n",
            "/encoder/layer/10/crossattention/self/value is tied\n",
            "/encoder/layer/10/crossattention/output/dense is tied\n",
            "/encoder/layer/10/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/10/intermediate/dense is tied\n",
            "/encoder/layer/10/output/dense is tied\n",
            "/encoder/layer/10/output/LayerNorm is tied\n",
            "/encoder/layer/11/crossattention/self/query is tied\n",
            "/encoder/layer/11/crossattention/self/key is tied\n",
            "/encoder/layer/11/crossattention/self/value is tied\n",
            "/encoder/layer/11/crossattention/output/dense is tied\n",
            "/encoder/layer/11/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/11/intermediate/dense is tied\n",
            "/encoder/layer/11/output/dense is tied\n",
            "/encoder/layer/11/output/LayerNorm is tied\n",
            "resume checkpoint from output/pretrain_animals/checkpoint_16.pth\n",
            "Start training\n",
            "Train Epoch: [17]  [   0/2500]  eta: 3:19:38  lr: 0.000050  loss_ita: 6.1392  loss_itm: 0.6385  loss_lm: 4.7927  time: 4.7912  data: 1.5059  max mem: 7121\n",
            "Train Epoch: [17]  [  50/2500]  eta: 0:39:44  lr: 0.000050  loss_ita: 5.9841  loss_itm: 0.6401  loss_lm: 4.4402  time: 0.8945  data: 0.0004  max mem: 7129\n",
            "Train Epoch: [17]  [ 100/2500]  eta: 0:38:02  lr: 0.000050  loss_ita: 5.9510  loss_itm: 0.6436  loss_lm: 4.7203  time: 0.9362  data: 0.0003  max mem: 7129\n",
            "Train Epoch: [17]  [ 150/2500]  eta: 0:36:56  lr: 0.000050  loss_ita: 5.8191  loss_itm: 0.6290  loss_lm: 4.2306  time: 0.9299  data: 0.0006  max mem: 7132\n",
            "Train Epoch: [17]  [ 200/2500]  eta: 0:36:00  lr: 0.000050  loss_ita: 5.8432  loss_itm: 0.6357  loss_lm: 4.4590  time: 0.9304  data: 0.0005  max mem: 7132\n",
            "Train Epoch: [17]  [ 250/2500]  eta: 0:35:06  lr: 0.000050  loss_ita: 6.0463  loss_itm: 0.6259  loss_lm: 4.6369  time: 0.9230  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 300/2500]  eta: 0:34:17  lr: 0.000050  loss_ita: 5.9681  loss_itm: 0.6204  loss_lm: 4.2531  time: 0.9307  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 350/2500]  eta: 0:33:28  lr: 0.000050  loss_ita: 5.8556  loss_itm: 0.6375  loss_lm: 4.7831  time: 0.9301  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 400/2500]  eta: 0:32:40  lr: 0.000050  loss_ita: 6.1228  loss_itm: 0.6355  loss_lm: 4.8984  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 450/2500]  eta: 0:31:51  lr: 0.000050  loss_ita: 6.3495  loss_itm: 0.6393  loss_lm: 4.6135  time: 0.9222  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 500/2500]  eta: 0:31:04  lr: 0.000050  loss_ita: 6.1634  loss_itm: 0.6500  loss_lm: 4.8925  time: 0.9304  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 550/2500]  eta: 0:30:17  lr: 0.000050  loss_ita: 5.9357  loss_itm: 0.6350  loss_lm: 4.1571  time: 0.9271  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 600/2500]  eta: 0:29:30  lr: 0.000050  loss_ita: 5.8463  loss_itm: 0.6394  loss_lm: 4.1982  time: 0.9227  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [ 650/2500]  eta: 0:28:43  lr: 0.000050  loss_ita: 6.0158  loss_itm: 0.6351  loss_lm: 4.7299  time: 0.9328  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 700/2500]  eta: 0:27:57  lr: 0.000050  loss_ita: 6.1428  loss_itm: 0.6314  loss_lm: 4.6872  time: 0.9312  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 750/2500]  eta: 0:27:10  lr: 0.000050  loss_ita: 6.2539  loss_itm: 0.6385  loss_lm: 4.5769  time: 0.9238  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 800/2500]  eta: 0:26:23  lr: 0.000050  loss_ita: 6.0947  loss_itm: 0.6348  loss_lm: 4.4382  time: 0.9340  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 850/2500]  eta: 0:25:36  lr: 0.000050  loss_ita: 5.9637  loss_itm: 0.6348  loss_lm: 4.9964  time: 0.9370  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 900/2500]  eta: 0:24:49  lr: 0.000050  loss_ita: 6.2077  loss_itm: 0.6503  loss_lm: 4.5971  time: 0.9215  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 950/2500]  eta: 0:24:03  lr: 0.000050  loss_ita: 6.2782  loss_itm: 0.6304  loss_lm: 4.1598  time: 0.9229  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1000/2500]  eta: 0:23:16  lr: 0.000050  loss_ita: 6.1663  loss_itm: 0.6368  loss_lm: 4.6378  time: 0.9374  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1050/2500]  eta: 0:22:30  lr: 0.000050  loss_ita: 6.2009  loss_itm: 0.6334  loss_lm: 4.4458  time: 0.9369  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1100/2500]  eta: 0:21:43  lr: 0.000050  loss_ita: 6.1823  loss_itm: 0.6355  loss_lm: 4.9945  time: 0.9254  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1150/2500]  eta: 0:20:56  lr: 0.000050  loss_ita: 6.0765  loss_itm: 0.6280  loss_lm: 4.5967  time: 0.9312  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1200/2500]  eta: 0:20:10  lr: 0.000050  loss_ita: 6.0619  loss_itm: 0.6410  loss_lm: 4.8511  time: 0.9367  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [1250/2500]  eta: 0:19:23  lr: 0.000050  loss_ita: 6.0537  loss_itm: 0.6422  loss_lm: 5.3150  time: 0.9227  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1300/2500]  eta: 0:18:36  lr: 0.000050  loss_ita: 5.9791  loss_itm: 0.6471  loss_lm: 4.5806  time: 0.9206  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1350/2500]  eta: 0:17:50  lr: 0.000050  loss_ita: 5.7339  loss_itm: 0.6316  loss_lm: 4.2935  time: 0.9418  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1400/2500]  eta: 0:17:03  lr: 0.000050  loss_ita: 5.8574  loss_itm: 0.6248  loss_lm: 4.8669  time: 0.9227  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1450/2500]  eta: 0:16:17  lr: 0.000050  loss_ita: 5.8236  loss_itm: 0.6454  loss_lm: 4.4377  time: 0.9271  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1500/2500]  eta: 0:15:30  lr: 0.000050  loss_ita: 5.6852  loss_itm: 0.6495  loss_lm: 4.7744  time: 0.9329  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1550/2500]  eta: 0:14:44  lr: 0.000050  loss_ita: 5.4990  loss_itm: 0.6276  loss_lm: 4.4243  time: 0.9361  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1600/2500]  eta: 0:13:57  lr: 0.000050  loss_ita: 5.6231  loss_itm: 0.6459  loss_lm: 4.4868  time: 0.9241  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1650/2500]  eta: 0:13:10  lr: 0.000050  loss_ita: 5.7932  loss_itm: 0.6374  loss_lm: 4.5288  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1700/2500]  eta: 0:12:24  lr: 0.000050  loss_ita: 5.9085  loss_itm: 0.6344  loss_lm: 4.4765  time: 0.9344  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1750/2500]  eta: 0:11:37  lr: 0.000050  loss_ita: 6.1626  loss_itm: 0.6300  loss_lm: 4.4793  time: 0.9218  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1800/2500]  eta: 0:10:51  lr: 0.000050  loss_ita: 6.1059  loss_itm: 0.6371  loss_lm: 4.8303  time: 0.9242  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1850/2500]  eta: 0:10:04  lr: 0.000050  loss_ita: 5.7550  loss_itm: 0.6354  loss_lm: 4.7988  time: 0.9292  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [1900/2500]  eta: 0:09:18  lr: 0.000050  loss_ita: 5.6715  loss_itm: 0.6439  loss_lm: 4.9443  time: 0.9322  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [1950/2500]  eta: 0:08:31  lr: 0.000050  loss_ita: 5.9078  loss_itm: 0.6322  loss_lm: 4.9832  time: 0.9235  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2000/2500]  eta: 0:07:45  lr: 0.000050  loss_ita: 6.1245  loss_itm: 0.6394  loss_lm: 4.4271  time: 0.9317  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2050/2500]  eta: 0:06:58  lr: 0.000050  loss_ita: 6.3317  loss_itm: 0.6358  loss_lm: 4.5954  time: 0.9336  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2100/2500]  eta: 0:06:12  lr: 0.000050  loss_ita: 6.1765  loss_itm: 0.6298  loss_lm: 4.4585  time: 0.9223  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2150/2500]  eta: 0:05:25  lr: 0.000050  loss_ita: 5.8163  loss_itm: 0.6358  loss_lm: 4.6149  time: 0.9204  data: 0.0002  max mem: 7134\n",
            "Train Epoch: [17]  [2200/2500]  eta: 0:04:39  lr: 0.000050  loss_ita: 5.7070  loss_itm: 0.6437  loss_lm: 4.4813  time: 0.9407  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2250/2500]  eta: 0:03:52  lr: 0.000050  loss_ita: 5.9511  loss_itm: 0.6427  loss_lm: 4.4372  time: 0.9190  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2300/2500]  eta: 0:03:05  lr: 0.000050  loss_ita: 6.0164  loss_itm: 0.6342  loss_lm: 4.4307  time: 0.9265  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2350/2500]  eta: 0:02:19  lr: 0.000050  loss_ita: 5.9607  loss_itm: 0.6364  loss_lm: 4.5868  time: 0.9313  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2400/2500]  eta: 0:01:32  lr: 0.000050  loss_ita: 6.0198  loss_itm: 0.6424  loss_lm: 4.5925  time: 0.9271  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2450/2500]  eta: 0:00:46  lr: 0.000050  loss_ita: 6.2225  loss_itm: 0.6522  loss_lm: 4.5737  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2499/2500]  eta: 0:00:00  lr: 0.000050  loss_ita: 6.3974  loss_itm: 0.6369  loss_lm: 5.1242  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17] Total time: 0:38:44 (0.9298 s / it)\n",
            "Averaged stats: lr: 0.0001  loss_ita: 5.9935  loss_itm: 0.6356  loss_lm: 4.6061\n",
            "Train Epoch: [18]  [   0/2500]  eta: 1:20:04  lr: 0.000045  loss_ita: 6.2714  loss_itm: 0.6457  loss_lm: 4.7524  time: 1.9218  data: 0.8585  max mem: 7970\n",
            "Train Epoch: [18]  [  50/2500]  eta: 0:40:21  lr: 0.000045  loss_ita: 6.3983  loss_itm: 0.6511  loss_lm: 4.1244  time: 0.9882  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 100/2500]  eta: 0:38:50  lr: 0.000045  loss_ita: 6.3850  loss_itm: 0.6482  loss_lm: 4.3894  time: 0.9561  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 150/2500]  eta: 0:37:38  lr: 0.000045  loss_ita: 6.2264  loss_itm: 0.6374  loss_lm: 4.5178  time: 0.9203  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 200/2500]  eta: 0:36:30  lr: 0.000045  loss_ita: 5.9527  loss_itm: 0.6499  loss_lm: 4.5807  time: 0.9236  data: 0.0003  max mem: 7982\n",
            "Train Epoch: [18]  [ 250/2500]  eta: 0:35:32  lr: 0.000045  loss_ita: 5.7489  loss_itm: 0.6327  loss_lm: 4.1563  time: 0.9307  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 300/2500]  eta: 0:34:36  lr: 0.000045  loss_ita: 5.7306  loss_itm: 0.6399  loss_lm: 4.6968  time: 0.9242  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 350/2500]  eta: 0:33:43  lr: 0.000045  loss_ita: 5.9946  loss_itm: 0.6380  loss_lm: 4.8402  time: 0.9228  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 400/2500]  eta: 0:32:53  lr: 0.000045  loss_ita: 6.1702  loss_itm: 0.6330  loss_lm: 4.2296  time: 0.9339  data: 0.0004  max mem: 7983\n",
            "Train Epoch: [18]  [ 450/2500]  eta: 0:32:02  lr: 0.000045  loss_ita: 6.1112  loss_itm: 0.6402  loss_lm: 4.3074  time: 0.9217  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 500/2500]  eta: 0:31:12  lr: 0.000045  loss_ita: 6.2875  loss_itm: 0.6396  loss_lm: 4.1692  time: 0.9172  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 550/2500]  eta: 0:30:24  lr: 0.000045  loss_ita: 6.1270  loss_itm: 0.6236  loss_lm: 4.2401  time: 0.9325  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 600/2500]  eta: 0:29:36  lr: 0.000045  loss_ita: 5.8813  loss_itm: 0.6358  loss_lm: 4.6269  time: 0.9270  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 650/2500]  eta: 0:28:47  lr: 0.000045  loss_ita: 5.8662  loss_itm: 0.6497  loss_lm: 4.5022  time: 0.9237  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 700/2500]  eta: 0:27:59  lr: 0.000045  loss_ita: 6.0300  loss_itm: 0.6378  loss_lm: 4.8883  time: 0.9273  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 750/2500]  eta: 0:27:12  lr: 0.000045  loss_ita: 6.1708  loss_itm: 0.6311  loss_lm: 4.5949  time: 0.9273  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 800/2500]  eta: 0:26:24  lr: 0.000045  loss_ita: 6.1851  loss_itm: 0.6380  loss_lm: 4.5566  time: 0.9163  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 850/2500]  eta: 0:25:36  lr: 0.000045  loss_ita: 6.0905  loss_itm: 0.6362  loss_lm: 4.4975  time: 0.9210  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 900/2500]  eta: 0:24:50  lr: 0.000045  loss_ita: 6.1650  loss_itm: 0.6281  loss_lm: 4.4557  time: 0.9308  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 950/2500]  eta: 0:24:02  lr: 0.000045  loss_ita: 6.4208  loss_itm: 0.6424  loss_lm: 4.9033  time: 0.9256  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1000/2500]  eta: 0:23:16  lr: 0.000045  loss_ita: 6.4365  loss_itm: 0.6313  loss_lm: 4.4134  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1050/2500]  eta: 0:22:29  lr: 0.000045  loss_ita: 6.1584  loss_itm: 0.6530  loss_lm: 4.5601  time: 0.9284  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1100/2500]  eta: 0:21:43  lr: 0.000045  loss_ita: 6.0874  loss_itm: 0.6354  loss_lm: 4.9366  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1150/2500]  eta: 0:20:56  lr: 0.000045  loss_ita: 5.9607  loss_itm: 0.6468  loss_lm: 4.2832  time: 0.9209  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1200/2500]  eta: 0:20:09  lr: 0.000045  loss_ita: 5.9632  loss_itm: 0.6471  loss_lm: 4.6797  time: 0.9294  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1250/2500]  eta: 0:19:22  lr: 0.000045  loss_ita: 6.1932  loss_itm: 0.6278  loss_lm: 4.3471  time: 0.9296  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1300/2500]  eta: 0:18:36  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6202  loss_lm: 4.5908  time: 0.9244  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1350/2500]  eta: 0:17:49  lr: 0.000045  loss_ita: 6.0786  loss_itm: 0.6392  loss_lm: 4.4718  time: 0.9198  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1400/2500]  eta: 0:17:02  lr: 0.000045  loss_ita: 6.0546  loss_itm: 0.6346  loss_lm: 4.4181  time: 0.9355  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1450/2500]  eta: 0:16:16  lr: 0.000045  loss_ita: 6.1391  loss_itm: 0.6276  loss_lm: 4.5114  time: 0.9284  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1500/2500]  eta: 0:15:29  lr: 0.000045  loss_ita: 5.7351  loss_itm: 0.6409  loss_lm: 4.5599  time: 0.9248  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [1550/2500]  eta: 0:14:43  lr: 0.000045  loss_ita: 5.2817  loss_itm: 0.6334  loss_lm: 4.4926  time: 0.9290  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1600/2500]  eta: 0:13:56  lr: 0.000045  loss_ita: 5.3891  loss_itm: 0.6325  loss_lm: 4.5553  time: 0.9378  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1650/2500]  eta: 0:13:10  lr: 0.000045  loss_ita: 5.8572  loss_itm: 0.6407  loss_lm: 4.6760  time: 0.9198  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1700/2500]  eta: 0:12:23  lr: 0.000045  loss_ita: 6.2208  loss_itm: 0.6292  loss_lm: 4.2864  time: 0.9233  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1750/2500]  eta: 0:11:37  lr: 0.000045  loss_ita: 6.4253  loss_itm: 0.6383  loss_lm: 4.3046  time: 0.9352  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1800/2500]  eta: 0:10:50  lr: 0.000045  loss_ita: 6.1586  loss_itm: 0.6361  loss_lm: 4.5963  time: 0.9234  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1850/2500]  eta: 0:10:04  lr: 0.000045  loss_ita: 6.1716  loss_itm: 0.6387  loss_lm: 4.4412  time: 0.9251  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1900/2500]  eta: 0:09:17  lr: 0.000045  loss_ita: 6.0535  loss_itm: 0.6367  loss_lm: 4.3708  time: 0.9263  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1950/2500]  eta: 0:08:31  lr: 0.000045  loss_ita: 5.9256  loss_itm: 0.6286  loss_lm: 4.6511  time: 0.9289  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2000/2500]  eta: 0:07:44  lr: 0.000045  loss_ita: 5.7240  loss_itm: 0.6361  loss_lm: 4.3443  time: 0.9228  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2050/2500]  eta: 0:06:58  lr: 0.000045  loss_ita: 5.7294  loss_itm: 0.6406  loss_lm: 4.5127  time: 0.9255  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2100/2500]  eta: 0:06:11  lr: 0.000045  loss_ita: 5.7598  loss_itm: 0.6372  loss_lm: 5.1710  time: 0.9374  data: 0.0006  max mem: 7989\n",
            "Train Epoch: [18]  [2150/2500]  eta: 0:05:25  lr: 0.000045  loss_ita: 6.0122  loss_itm: 0.6364  loss_lm: 4.6644  time: 0.9268  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2200/2500]  eta: 0:04:38  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6409  loss_lm: 3.9354  time: 0.9197  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2250/2500]  eta: 0:03:52  lr: 0.000045  loss_ita: 6.3002  loss_itm: 0.6383  loss_lm: 4.6477  time: 0.9372  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2300/2500]  eta: 0:03:05  lr: 0.000045  loss_ita: 6.2824  loss_itm: 0.6529  loss_lm: 4.5341  time: 0.9280  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2350/2500]  eta: 0:02:19  lr: 0.000045  loss_ita: 6.1060  loss_itm: 0.6350  loss_lm: 4.9516  time: 0.9269  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2400/2500]  eta: 0:01:32  lr: 0.000045  loss_ita: 6.1870  loss_itm: 0.6261  loss_lm: 5.1146  time: 0.9306  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2450/2500]  eta: 0:00:46  lr: 0.000045  loss_ita: 6.0545  loss_itm: 0.6384  loss_lm: 4.0758  time: 0.9333  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2499/2500]  eta: 0:00:00  lr: 0.000045  loss_ita: 6.0094  loss_itm: 0.6447  loss_lm: 4.6467  time: 0.9193  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18] Total time: 0:38:43 (0.9293 s / it)\n",
            "Averaged stats: lr: 0.0000  loss_ita: 6.0553  loss_itm: 0.6362  loss_lm: 4.5673\n",
            "Train Epoch: [19]  [   0/2500]  eta: 1:33:40  lr: 0.000041  loss_ita: 6.0785  loss_itm: 0.6439  loss_lm: 3.9600  time: 2.2481  data: 1.0550  max mem: 7989\n",
            "Train Epoch: [19]  [  50/2500]  eta: 0:40:28  lr: 0.000041  loss_ita: 6.1363  loss_itm: 0.6342  loss_lm: 4.7595  time: 0.9812  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [19]  [ 100/2500]  eta: 0:38:56  lr: 0.000041  loss_ita: 6.1894  loss_itm: 0.6489  loss_lm: 4.6459  time: 0.9512  data: 0.0005  max mem: 7989\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}