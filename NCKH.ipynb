{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "965dd829-8057-4b8e-f518-b8b75b9b83de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "11a60f1e-f3a3-422d-dce6-7935e223beb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.11-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu128)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from cog) (2.32.4)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.40.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Downloading cog-0.16.11-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.50.0\n",
            "    Uninstalling starlette-0.50.0:\n",
            "      Successfully uninstalled starlette-0.50.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.128.2\n",
            "    Uninstalling fastapi-0.128.2:\n",
            "      Successfully uninstalled fastapi-0.128.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.24.0 requires fastapi<1.0.0,>=0.124.1, but you have fastapi 0.118.3 which is incompatible.\n",
            "google-adk 1.24.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "sse-starlette 3.2.0 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.11 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.16.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292936 sha256=0015a358edcccc5017fd126c48a646c38ee161b996fa06c19a178230252eb02c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "# TextVQA (phải chạy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "c87e3dad-fc6a-40b1-df3c-7bbc272e9145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-12 15:12:14--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.111, 13.35.37.123, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M   176MB/s    in 0.6s    \n",
            "\n",
            "2026-02-12 15:12:15 (176 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-02-12 15:12:15--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.111, 13.35.37.123, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2026-02-12 15:12:15 (174 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-02-12 15:12:15--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.111, 13.35.37.123, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  51.9MB/s    in 0.2s    \n",
            "\n",
            "2026-02-12 15:12:16 (51.9 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "0ab993aa-d909-491b-b491-14d6f4096fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-12 15:12:16--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.111, 13.35.37.123, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G  96.0MB/s    in 60s     \n",
            "\n",
            "2026-02-12 15:13:17 (112 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (Dhuy + HNhien)\n"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "4ecfdc11-70ec-48c3-a0a6-7703a0aa1ac7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "reshape position embedding from 900 to 196\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "<All keys matched successfully>\n",
            "Start training\n",
            "Loss: 17.1619\n",
            "==========================================\n",
            "Train Epoch: [0]  [   0/8650]  eta: 9:05:33  lr: 0.000020  loss: 17.1619  time: 3.7842  data: 0.8004  max mem: 6922\n",
            "Loss: 22.2107\n",
            "==========================================\n",
            "Loss: 17.6196\n",
            "==========================================\n",
            "Loss: 22.4522\n",
            "==========================================\n",
            "Loss: 18.5996\n",
            "==========================================\n",
            "Loss: 15.116\n",
            "==========================================\n",
            "Loss: 13.7835\n",
            "==========================================\n",
            "Loss: 22.9052\n",
            "==========================================\n",
            "Loss: 22.1199\n",
            "==========================================\n",
            "Loss: 22.657\n",
            "==========================================\n",
            "Loss: 16.3429\n",
            "==========================================\n",
            "Loss: 16.1788\n",
            "==========================================\n",
            "Loss: 13.1597\n",
            "==========================================\n",
            "Loss: 20.5788\n",
            "==========================================\n",
            "Loss: 15.4407\n",
            "==========================================\n",
            "Loss: 30.1561\n",
            "==========================================\n",
            "Loss: 16.5253\n",
            "==========================================\n",
            "Loss: 10.3148\n",
            "==========================================\n",
            "Loss: 17.5338\n",
            "==========================================\n",
            "Loss: 30.6049\n",
            "==========================================\n",
            "Loss: 24.2531\n",
            "==========================================\n",
            "Loss: 17.5263\n",
            "==========================================\n",
            "Loss: 18.6688\n",
            "==========================================\n",
            "Loss: 16.5131\n",
            "==========================================\n",
            "Loss: 12.7678\n",
            "==========================================\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2d6743aca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1600, in _shutdown_workers\n",
            "    self._pin_memory_thread.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 55, in train\n",
            "    loss = model.forward(image, question, answer, n, weights, train=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip_vqa.py\", line 63, in forward\n",
            "    answer_output = self.text_decoder(\n",
            "                    ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 886, in forward\n",
            "    outputs = self.bert(\n",
            "              ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 781, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 445, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 361, in forward\n",
            "    cross_attention_outputs = self.crossattention(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 286, in forward\n",
            "    attention_output = self.output(self_outputs[0], hidden_states)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 236, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False #\\\n",
        "    # --resume ./output/textVQA/checkpoint_05.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train textVQA + pretrain_animal (Khoi)"
      ],
      "metadata": {
        "id": "i9pDWKQLVYL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHhslGC2SmK",
        "collapsed": true,
        "outputId": "f8f34ddf-bb61-462c-d1ee-22e0ecffb1aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "load checkpoint from /content/drive/MyDrive/BLIP/output/pretrain_animals/checkpoint_29.pth\n",
            "_IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_proj_m.weight', 'text_proj_m.bias'])\n",
            "=> no checkpoint found at '.output/textVQA_pretrain_animals/checkpoint_00.pth'\n",
            "Start training\n",
            "Loss: 33.9543\n",
            "==========================================\n",
            "Train Epoch: [0]  [   0/8650]  eta: 11:42:56  lr: 0.000020  loss: 33.9543  time: 4.8759  data: 1.4664  max mem: 6922\n",
            "Loss: 39.3537\n",
            "==========================================\n",
            "Loss: 37.0053\n",
            "==========================================\n",
            "Loss: 38.5932\n",
            "==========================================\n",
            "Loss: 37.6007\n",
            "==========================================\n",
            "Loss: 36.6881\n",
            "==========================================\n",
            "Loss: 28.276\n",
            "==========================================\n",
            "Loss: 37.5866\n",
            "==========================================\n",
            "Loss: 36.5802\n",
            "==========================================\n",
            "Loss: 36.3897\n",
            "==========================================\n",
            "Loss: 35.2038\n",
            "==========================================\n",
            "Loss: 38.3956\n",
            "==========================================\n",
            "Loss: 30.1647\n",
            "==========================================\n",
            "Loss: 41.5828\n",
            "==========================================\n",
            "Loss: 24.3294\n",
            "==========================================\n",
            "Loss: 47.1137\n",
            "==========================================\n",
            "Loss: 30.3299\n",
            "==========================================\n",
            "Loss: 19.4869\n",
            "==========================================\n",
            "Loss: 30.6597\n",
            "==========================================\n",
            "Loss: 45.1534\n",
            "==========================================\n",
            "Loss: 39.7802\n",
            "==========================================\n",
            "Loss: 33.4387\n",
            "==========================================\n",
            "Loss: 33.0439\n",
            "==========================================\n",
            "Loss: 29.9736\n",
            "==========================================\n",
            "Loss: 28.0138\n",
            "==========================================\n",
            "Loss: 22.2397\n",
            "==========================================\n",
            "Loss: 29.4353\n",
            "==========================================\n",
            "Loss: 29.5495\n",
            "==========================================\n",
            "Loss: 25.0864\n",
            "==========================================\n",
            "Loss: 30.4603\n",
            "==========================================\n",
            "Loss: 50.5748\n",
            "==========================================\n",
            "Loss: 38.139\n",
            "==========================================\n",
            "Loss: 24.0177\n",
            "==========================================\n",
            "Loss: 25.6675\n",
            "==========================================\n",
            "Loss: 23.1447\n",
            "==========================================\n",
            "Loss: 23.0872\n",
            "==========================================\n",
            "Loss: 44.2297\n",
            "==========================================\n",
            "Loss: 38.1544\n",
            "==========================================\n",
            "Loss: 25.7952\n",
            "==========================================\n",
            "Loss: 26.5363\n",
            "==========================================\n",
            "Loss: 19.488\n",
            "==========================================\n",
            "Loss: 33.381\n",
            "==========================================\n",
            "Loss: 21.2742\n",
            "==========================================\n",
            "Loss: 22.2959\n",
            "==========================================\n",
            "Loss: 14.9801\n",
            "==========================================\n",
            "Loss: 25.5695\n",
            "==========================================\n",
            "Loss: 24.5609\n",
            "==========================================\n",
            "Loss: 32.1162\n",
            "==========================================\n",
            "Loss: 16.2912\n",
            "==========================================\n",
            "Loss: 42.6244\n",
            "==========================================\n",
            "Train Epoch: [0]  [  50/8650]  eta: 1:22:25  lr: 0.000020  loss: 18.8952  time: 0.5968  data: 0.0006  max mem: 6940\n",
            "Train Epoch: [0]  [ 100/8650]  eta: 1:17:17  lr: 0.000020  loss: 32.7722  time: 0.5785  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [ 150/8650]  eta: 1:14:28  lr: 0.000020  loss: 23.6812  time: 0.5488  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [ 200/8650]  eta: 1:12:40  lr: 0.000020  loss: 20.6354  time: 0.5399  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [ 250/8650]  eta: 1:11:39  lr: 0.000020  loss: 18.3001  time: 0.5282  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [ 300/8650]  eta: 1:10:47  lr: 0.000020  loss: 17.9857  time: 0.5201  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [ 350/8650]  eta: 1:09:58  lr: 0.000020  loss: 25.4250  time: 0.5108  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [ 400/8650]  eta: 1:09:18  lr: 0.000020  loss: 24.8363  time: 0.5159  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [ 450/8650]  eta: 1:08:52  lr: 0.000020  loss: 20.2931  time: 0.5328  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [ 500/8650]  eta: 1:08:17  lr: 0.000020  loss: 22.6935  time: 0.4795  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [ 550/8650]  eta: 1:07:42  lr: 0.000020  loss: 18.4217  time: 0.4826  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [ 600/8650]  eta: 1:07:17  lr: 0.000020  loss: 23.2089  time: 0.4765  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [ 650/8650]  eta: 1:06:50  lr: 0.000020  loss: 15.7634  time: 0.4627  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [ 700/8650]  eta: 1:06:23  lr: 0.000020  loss: 22.3629  time: 0.4441  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [ 750/8650]  eta: 1:05:55  lr: 0.000020  loss: 24.1271  time: 0.4285  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [ 800/8650]  eta: 1:05:34  lr: 0.000020  loss: 28.3966  time: 0.4193  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [ 850/8650]  eta: 1:05:07  lr: 0.000020  loss: 18.3481  time: 0.4259  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [ 900/8650]  eta: 1:04:41  lr: 0.000020  loss: 29.7336  time: 0.4143  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [ 950/8650]  eta: 1:04:12  lr: 0.000020  loss: 21.1419  time: 0.4113  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1000/8650]  eta: 1:03:44  lr: 0.000020  loss: 41.2681  time: 0.4078  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1050/8650]  eta: 1:03:16  lr: 0.000020  loss: 42.0341  time: 0.4158  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1100/8650]  eta: 1:02:49  lr: 0.000020  loss: 28.4314  time: 0.4088  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1150/8650]  eta: 1:02:21  lr: 0.000020  loss: 23.7598  time: 0.4087  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1200/8650]  eta: 1:01:53  lr: 0.000020  loss: 23.0993  time: 0.4111  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1250/8650]  eta: 1:01:26  lr: 0.000020  loss: 30.0839  time: 0.4129  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1300/8650]  eta: 1:01:00  lr: 0.000020  loss: 12.6830  time: 0.4211  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1350/8650]  eta: 1:00:33  lr: 0.000020  loss: 19.7080  time: 0.4201  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1400/8650]  eta: 1:00:07  lr: 0.000020  loss: 36.6253  time: 0.4241  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1450/8650]  eta: 0:59:40  lr: 0.000020  loss: 15.4997  time: 0.4197  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1500/8650]  eta: 0:59:15  lr: 0.000020  loss: 37.5313  time: 0.4322  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [1550/8650]  eta: 0:58:50  lr: 0.000020  loss: 31.3084  time: 0.4388  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [1600/8650]  eta: 0:58:23  lr: 0.000020  loss: 35.2519  time: 0.4393  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [1650/8650]  eta: 0:57:57  lr: 0.000020  loss: 30.3550  time: 0.4475  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [1700/8650]  eta: 0:57:32  lr: 0.000020  loss: 24.3758  time: 0.4668  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [1750/8650]  eta: 0:57:06  lr: 0.000020  loss: 17.8714  time: 0.4741  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [1800/8650]  eta: 0:56:41  lr: 0.000020  loss: 13.4041  time: 0.4901  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [1850/8650]  eta: 0:56:16  lr: 0.000020  loss: 17.3623  time: 0.4956  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [1900/8650]  eta: 0:55:50  lr: 0.000020  loss: 28.4900  time: 0.4964  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [1950/8650]  eta: 0:55:26  lr: 0.000020  loss: 30.2589  time: 0.5022  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [2000/8650]  eta: 0:55:01  lr: 0.000020  loss: 24.3500  time: 0.5142  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [2050/8650]  eta: 0:54:37  lr: 0.000020  loss: 19.4308  time: 0.5443  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2100/8650]  eta: 0:54:11  lr: 0.000020  loss: 19.6905  time: 0.5320  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2150/8650]  eta: 0:53:46  lr: 0.000020  loss: 15.9936  time: 0.5467  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [2200/8650]  eta: 0:53:21  lr: 0.000020  loss: 19.0504  time: 0.5416  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2250/8650]  eta: 0:52:57  lr: 0.000020  loss: 17.9101  time: 0.5877  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2300/8650]  eta: 0:52:33  lr: 0.000020  loss: 27.5902  time: 0.5726  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2350/8650]  eta: 0:52:08  lr: 0.000020  loss: 23.6932  time: 0.5804  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2400/8650]  eta: 0:51:44  lr: 0.000020  loss: 21.5437  time: 0.5899  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2450/8650]  eta: 0:51:18  lr: 0.000020  loss: 24.3874  time: 0.5897  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2500/8650]  eta: 0:50:53  lr: 0.000020  loss: 14.3953  time: 0.5960  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2550/8650]  eta: 0:50:28  lr: 0.000020  loss: 17.5258  time: 0.5889  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [2600/8650]  eta: 0:50:03  lr: 0.000020  loss: 24.3342  time: 0.5891  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2650/8650]  eta: 0:49:37  lr: 0.000020  loss: 19.7622  time: 0.5908  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [2700/8650]  eta: 0:49:12  lr: 0.000020  loss: 14.3201  time: 0.5722  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [0]  [2750/8650]  eta: 0:48:47  lr: 0.000020  loss: 14.4301  time: 0.5539  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [2800/8650]  eta: 0:48:22  lr: 0.000020  loss: 23.2380  time: 0.5508  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [2850/8650]  eta: 0:47:56  lr: 0.000020  loss: 20.7149  time: 0.5363  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [0]  [2900/8650]  eta: 0:47:31  lr: 0.000020  loss: 14.7620  time: 0.5467  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [2950/8650]  eta: 0:47:06  lr: 0.000020  loss: 38.9126  time: 0.5445  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [3000/8650]  eta: 0:46:41  lr: 0.000020  loss: 23.2726  time: 0.5273  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [3050/8650]  eta: 0:46:16  lr: 0.000020  loss: 19.4908  time: 0.5180  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [3100/8650]  eta: 0:45:51  lr: 0.000020  loss: 12.2127  time: 0.5001  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [3150/8650]  eta: 0:45:26  lr: 0.000020  loss: 11.0496  time: 0.4790  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [3200/8650]  eta: 0:45:01  lr: 0.000020  loss: 24.7931  time: 0.4739  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [3250/8650]  eta: 0:44:36  lr: 0.000020  loss: 25.1284  time: 0.4706  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [3300/8650]  eta: 0:44:10  lr: 0.000020  loss: 18.1889  time: 0.4552  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [3350/8650]  eta: 0:43:45  lr: 0.000020  loss: 16.4420  time: 0.4453  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [3400/8650]  eta: 0:43:20  lr: 0.000020  loss: 22.3279  time: 0.4432  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [3450/8650]  eta: 0:42:55  lr: 0.000020  loss: 21.0052  time: 0.4388  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [3500/8650]  eta: 0:42:29  lr: 0.000020  loss: 13.4062  time: 0.4347  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [3550/8650]  eta: 0:42:04  lr: 0.000020  loss: 15.8692  time: 0.4190  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [3600/8650]  eta: 0:41:39  lr: 0.000020  loss: 29.1593  time: 0.4163  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3650/8650]  eta: 0:41:14  lr: 0.000020  loss: 28.9007  time: 0.4064  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3700/8650]  eta: 0:40:49  lr: 0.000020  loss: 47.0241  time: 0.4096  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3750/8650]  eta: 0:40:23  lr: 0.000020  loss: 17.1098  time: 0.4064  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3800/8650]  eta: 0:39:58  lr: 0.000020  loss: 29.4080  time: 0.4072  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3850/8650]  eta: 0:39:33  lr: 0.000020  loss: 11.6640  time: 0.4099  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3900/8650]  eta: 0:39:08  lr: 0.000020  loss: 22.4837  time: 0.4151  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [3950/8650]  eta: 0:38:44  lr: 0.000020  loss: 20.3633  time: 0.4199  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4000/8650]  eta: 0:38:18  lr: 0.000020  loss: 20.3588  time: 0.4209  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [4050/8650]  eta: 0:37:54  lr: 0.000020  loss: 24.9923  time: 0.4299  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [4100/8650]  eta: 0:37:29  lr: 0.000020  loss: 28.2526  time: 0.4205  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [4150/8650]  eta: 0:37:04  lr: 0.000020  loss: 31.5428  time: 0.4283  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4200/8650]  eta: 0:36:39  lr: 0.000020  loss: 10.5892  time: 0.4330  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4250/8650]  eta: 0:36:15  lr: 0.000020  loss: 16.0748  time: 0.4480  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4300/8650]  eta: 0:35:51  lr: 0.000020  loss: 24.1887  time: 0.4414  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [4350/8650]  eta: 0:35:26  lr: 0.000020  loss: 20.3091  time: 0.4393  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4400/8650]  eta: 0:35:01  lr: 0.000020  loss: 18.7664  time: 0.4361  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [4450/8650]  eta: 0:34:36  lr: 0.000020  loss: 28.6066  time: 0.4411  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [4500/8650]  eta: 0:34:11  lr: 0.000020  loss: 18.8531  time: 0.4578  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [4550/8650]  eta: 0:33:46  lr: 0.000020  loss: 26.9681  time: 0.4711  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [4600/8650]  eta: 0:33:22  lr: 0.000020  loss: 19.1256  time: 0.4833  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [4650/8650]  eta: 0:32:57  lr: 0.000020  loss: 27.2663  time: 0.4976  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [4700/8650]  eta: 0:32:32  lr: 0.000020  loss: 15.6149  time: 0.5009  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [4750/8650]  eta: 0:32:07  lr: 0.000020  loss: 25.2935  time: 0.4982  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [4800/8650]  eta: 0:31:42  lr: 0.000020  loss: 18.8036  time: 0.5154  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [4850/8650]  eta: 0:31:17  lr: 0.000020  loss: 19.9661  time: 0.5199  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [4900/8650]  eta: 0:30:52  lr: 0.000020  loss: 18.6671  time: 0.5118  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [4950/8650]  eta: 0:30:28  lr: 0.000020  loss: 25.9839  time: 0.5333  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [5000/8650]  eta: 0:30:03  lr: 0.000020  loss: 19.2839  time: 0.5394  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5050/8650]  eta: 0:29:38  lr: 0.000020  loss: 20.4884  time: 0.5428  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [5100/8650]  eta: 0:29:14  lr: 0.000020  loss: 30.4647  time: 0.5504  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [5150/8650]  eta: 0:28:49  lr: 0.000020  loss: 21.9570  time: 0.5514  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [5200/8650]  eta: 0:28:24  lr: 0.000020  loss: 30.8598  time: 0.5487  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [5250/8650]  eta: 0:27:59  lr: 0.000020  loss: 22.9318  time: 0.5533  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5300/8650]  eta: 0:27:34  lr: 0.000020  loss: 22.8176  time: 0.5705  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5350/8650]  eta: 0:27:10  lr: 0.000020  loss: 20.0031  time: 0.5592  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5400/8650]  eta: 0:26:45  lr: 0.000020  loss: 20.9439  time: 0.5602  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [5450/8650]  eta: 0:26:20  lr: 0.000020  loss: 25.4919  time: 0.5656  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [5500/8650]  eta: 0:25:55  lr: 0.000020  loss: 24.7273  time: 0.5716  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5550/8650]  eta: 0:25:30  lr: 0.000020  loss: 20.4638  time: 0.5790  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [5600/8650]  eta: 0:25:06  lr: 0.000020  loss: 20.4980  time: 0.5862  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [5650/8650]  eta: 0:24:41  lr: 0.000020  loss: 17.0351  time: 0.5881  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [0]  [5700/8650]  eta: 0:24:16  lr: 0.000020  loss: 24.5508  time: 0.5781  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5750/8650]  eta: 0:23:51  lr: 0.000020  loss: 24.5426  time: 0.5703  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [5800/8650]  eta: 0:23:26  lr: 0.000020  loss: 14.9665  time: 0.5593  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [5850/8650]  eta: 0:23:02  lr: 0.000020  loss: 31.8630  time: 0.5550  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [5900/8650]  eta: 0:22:37  lr: 0.000020  loss: 21.5021  time: 0.5613  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [5950/8650]  eta: 0:22:12  lr: 0.000020  loss: 17.1189  time: 0.5377  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [6000/8650]  eta: 0:21:47  lr: 0.000020  loss: 16.1784  time: 0.5280  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [6050/8650]  eta: 0:21:23  lr: 0.000020  loss: 17.5983  time: 0.5203  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [6100/8650]  eta: 0:20:58  lr: 0.000020  loss: 43.4152  time: 0.5164  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [6150/8650]  eta: 0:20:33  lr: 0.000020  loss: 20.7187  time: 0.5234  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [6200/8650]  eta: 0:20:08  lr: 0.000020  loss: 37.4483  time: 0.5110  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [6250/8650]  eta: 0:19:43  lr: 0.000020  loss: 30.5241  time: 0.5018  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [6300/8650]  eta: 0:19:19  lr: 0.000020  loss: 25.1250  time: 0.4841  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [6350/8650]  eta: 0:18:54  lr: 0.000020  loss: 25.3834  time: 0.4809  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [6400/8650]  eta: 0:18:29  lr: 0.000020  loss: 31.8798  time: 0.4655  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [6450/8650]  eta: 0:18:04  lr: 0.000020  loss: 27.9204  time: 0.4499  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [6500/8650]  eta: 0:17:40  lr: 0.000020  loss: 18.9249  time: 0.4418  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6550/8650]  eta: 0:17:15  lr: 0.000020  loss: 23.3459  time: 0.4315  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [6600/8650]  eta: 0:16:50  lr: 0.000020  loss: 28.4239  time: 0.4147  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6650/8650]  eta: 0:16:25  lr: 0.000020  loss: 14.3470  time: 0.4176  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6700/8650]  eta: 0:16:01  lr: 0.000020  loss: 28.4704  time: 0.4090  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6750/8650]  eta: 0:15:36  lr: 0.000020  loss: 11.7456  time: 0.4263  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6800/8650]  eta: 0:15:11  lr: 0.000020  loss: 20.3830  time: 0.4062  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6850/8650]  eta: 0:14:47  lr: 0.000020  loss: 20.4854  time: 0.4128  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6900/8650]  eta: 0:14:22  lr: 0.000020  loss: 14.6151  time: 0.4057  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [6950/8650]  eta: 0:13:57  lr: 0.000020  loss: 20.3179  time: 0.4088  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7000/8650]  eta: 0:13:33  lr: 0.000020  loss: 24.3423  time: 0.4114  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7050/8650]  eta: 0:13:08  lr: 0.000020  loss: 26.6823  time: 0.4083  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7100/8650]  eta: 0:12:44  lr: 0.000020  loss: 11.5380  time: 0.4153  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7150/8650]  eta: 0:12:19  lr: 0.000020  loss: 19.2308  time: 0.4170  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7200/8650]  eta: 0:11:54  lr: 0.000020  loss: 29.5696  time: 0.4233  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7250/8650]  eta: 0:11:30  lr: 0.000020  loss: 36.9650  time: 0.4248  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7300/8650]  eta: 0:11:05  lr: 0.000020  loss: 20.0084  time: 0.4231  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7350/8650]  eta: 0:10:40  lr: 0.000020  loss: 29.4932  time: 0.4277  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7400/8650]  eta: 0:10:16  lr: 0.000020  loss: 20.7055  time: 0.4288  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [0]  [7450/8650]  eta: 0:09:51  lr: 0.000020  loss: 21.2489  time: 0.4393  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7500/8650]  eta: 0:09:26  lr: 0.000020  loss: 28.4642  time: 0.4459  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7550/8650]  eta: 0:09:01  lr: 0.000020  loss: 16.8591  time: 0.4591  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [7600/8650]  eta: 0:08:37  lr: 0.000020  loss: 21.2499  time: 0.4683  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [0]  [7650/8650]  eta: 0:08:12  lr: 0.000020  loss: 23.2507  time: 0.4762  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [7700/8650]  eta: 0:07:47  lr: 0.000020  loss: 23.0162  time: 0.4798  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [7750/8650]  eta: 0:07:23  lr: 0.000020  loss: 17.2755  time: 0.4867  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [7800/8650]  eta: 0:06:58  lr: 0.000020  loss: 21.9596  time: 0.4956  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [0]  [7850/8650]  eta: 0:06:34  lr: 0.000020  loss: 21.6191  time: 0.5225  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [7900/8650]  eta: 0:06:09  lr: 0.000020  loss: 29.1342  time: 0.5280  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [7950/8650]  eta: 0:05:44  lr: 0.000020  loss: 32.1597  time: 0.5492  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [8000/8650]  eta: 0:05:20  lr: 0.000020  loss: 27.6356  time: 0.5549  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [0]  [8050/8650]  eta: 0:04:55  lr: 0.000020  loss: 21.0116  time: 0.5651  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [8100/8650]  eta: 0:04:31  lr: 0.000020  loss: 27.1500  time: 0.5666  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [8150/8650]  eta: 0:04:06  lr: 0.000020  loss: 28.2604  time: 0.5698  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [8200/8650]  eta: 0:03:41  lr: 0.000020  loss: 21.3886  time: 0.6103  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [8250/8650]  eta: 0:03:17  lr: 0.000020  loss: 15.0594  time: 0.5821  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [8300/8650]  eta: 0:02:52  lr: 0.000020  loss: 22.0663  time: 0.5886  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [0]  [8350/8650]  eta: 0:02:27  lr: 0.000020  loss: 17.7876  time: 0.5796  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [8400/8650]  eta: 0:02:03  lr: 0.000020  loss: 43.8096  time: 0.5882  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [8450/8650]  eta: 0:01:38  lr: 0.000020  loss: 21.6572  time: 0.5667  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [8500/8650]  eta: 0:01:13  lr: 0.000020  loss: 19.1764  time: 0.5758  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [0]  [8550/8650]  eta: 0:00:49  lr: 0.000020  loss: 38.6469  time: 0.5868  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [0]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 26.0029  time: 0.5462  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [0]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 14.8771  time: 0.5332  data: 0.0022  max mem: 9086\n",
            "Train Epoch: [0] Total time: 1:11:02 (0.4928 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 22.8632\n",
            "Loss: 20.0404\n",
            "==========================================\n",
            "Train Epoch: [1]  [   0/8650]  eta: 5:47:26  lr: 0.000020  loss: 20.0404  time: 2.4100  data: 1.4069  max mem: 9086\n",
            "Loss: 29.2713\n",
            "==========================================\n",
            "Loss: 23.446\n",
            "==========================================\n",
            "Loss: 30.2681\n",
            "==========================================\n",
            "Loss: 19.0951\n",
            "==========================================\n",
            "Loss: 12.7913\n",
            "==========================================\n",
            "Loss: 25.4368\n",
            "==========================================\n",
            "Loss: 10.5457\n",
            "==========================================\n",
            "Loss: 11.6625\n",
            "==========================================\n",
            "Loss: 20.5626\n",
            "==========================================\n",
            "Loss: 21.3212\n",
            "==========================================\n",
            "Loss: 12.4428\n",
            "==========================================\n",
            "Loss: 19.9127\n",
            "==========================================\n",
            "Loss: 23.6106\n",
            "==========================================\n",
            "Loss: 16.2023\n",
            "==========================================\n",
            "Loss: 14.7726\n",
            "==========================================\n",
            "Loss: 25.5074\n",
            "==========================================\n",
            "Loss: 18.3071\n",
            "==========================================\n",
            "Loss: 17.317\n",
            "==========================================\n",
            "Loss: 34.908\n",
            "==========================================\n",
            "Loss: 16.9357\n",
            "==========================================\n",
            "Loss: 19.9328\n",
            "==========================================\n",
            "Loss: 19.8\n",
            "==========================================\n",
            "Loss: 25.5644\n",
            "==========================================\n",
            "Loss: 27.4962\n",
            "==========================================\n",
            "Loss: 18.4064\n",
            "==========================================\n",
            "Loss: 30.3142\n",
            "==========================================\n",
            "Loss: 28.3817\n",
            "==========================================\n",
            "Loss: 19.2973\n",
            "==========================================\n",
            "Loss: 15.5842\n",
            "==========================================\n",
            "Loss: 19.9564\n",
            "==========================================\n",
            "Loss: 38.9665\n",
            "==========================================\n",
            "Loss: 21.1318\n",
            "==========================================\n",
            "Loss: 15.647\n",
            "==========================================\n",
            "Loss: 13.8052\n",
            "==========================================\n",
            "Loss: 18.2019\n",
            "==========================================\n",
            "Loss: 19.1434\n",
            "==========================================\n",
            "Loss: 17.7858\n",
            "==========================================\n",
            "Loss: 29.2397\n",
            "==========================================\n",
            "Loss: 22.2321\n",
            "==========================================\n",
            "Loss: 21.1993\n",
            "==========================================\n",
            "Loss: 16.876\n",
            "==========================================\n",
            "Loss: 22.9152\n",
            "==========================================\n",
            "Loss: 33.38\n",
            "==========================================\n",
            "Loss: 12.3057\n",
            "==========================================\n",
            "Loss: 26.4635\n",
            "==========================================\n",
            "Loss: 23.9562\n",
            "==========================================\n",
            "Loss: 22.5142\n",
            "==========================================\n",
            "Loss: 13.6709\n",
            "==========================================\n",
            "Loss: 22.6974\n",
            "==========================================\n",
            "Train Epoch: [1]  [  50/8650]  eta: 1:30:49  lr: 0.000020  loss: 41.0719  time: 0.6885  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [ 100/8650]  eta: 1:22:05  lr: 0.000020  loss: 26.6137  time: 0.5600  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [ 150/8650]  eta: 1:17:33  lr: 0.000020  loss: 16.4653  time: 0.5645  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [ 200/8650]  eta: 1:15:11  lr: 0.000020  loss: 24.9772  time: 0.5787  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [1]  [ 250/8650]  eta: 1:13:40  lr: 0.000020  loss: 24.1953  time: 0.5809  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [ 300/8650]  eta: 1:12:30  lr: 0.000020  loss: 23.8189  time: 0.5811  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [ 350/8650]  eta: 1:11:23  lr: 0.000020  loss: 19.0723  time: 0.5801  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [ 400/8650]  eta: 1:10:26  lr: 0.000020  loss: 9.4392  time: 0.5798  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [ 450/8650]  eta: 1:09:38  lr: 0.000020  loss: 22.9329  time: 0.5686  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [ 500/8650]  eta: 1:08:52  lr: 0.000020  loss: 19.3639  time: 0.5464  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [ 550/8650]  eta: 1:08:10  lr: 0.000020  loss: 17.6659  time: 0.5503  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [ 600/8650]  eta: 1:07:32  lr: 0.000020  loss: 10.3618  time: 0.5325  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 650/8650]  eta: 1:06:56  lr: 0.000020  loss: 24.6869  time: 0.5191  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [ 700/8650]  eta: 1:06:22  lr: 0.000020  loss: 21.4147  time: 0.5139  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 750/8650]  eta: 1:05:49  lr: 0.000020  loss: 21.0725  time: 0.5109  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [ 800/8650]  eta: 1:05:18  lr: 0.000020  loss: 23.0316  time: 0.5008  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [ 850/8650]  eta: 1:04:53  lr: 0.000020  loss: 24.7121  time: 0.4781  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 900/8650]  eta: 1:04:22  lr: 0.000020  loss: 18.0293  time: 0.4683  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [ 950/8650]  eta: 1:03:53  lr: 0.000020  loss: 20.2609  time: 0.4606  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1000/8650]  eta: 1:03:22  lr: 0.000020  loss: 25.1995  time: 0.4458  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [1050/8650]  eta: 1:02:53  lr: 0.000020  loss: 14.4683  time: 0.4427  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [1100/8650]  eta: 1:02:25  lr: 0.000020  loss: 18.0588  time: 0.4463  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [1150/8650]  eta: 1:01:57  lr: 0.000020  loss: 16.1070  time: 0.4210  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [1200/8650]  eta: 1:01:30  lr: 0.000020  loss: 11.6354  time: 0.4066  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1250/8650]  eta: 1:01:06  lr: 0.000020  loss: 24.4165  time: 0.4278  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1300/8650]  eta: 1:00:43  lr: 0.000020  loss: 32.3858  time: 0.4078  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1350/8650]  eta: 1:00:17  lr: 0.000020  loss: 24.1620  time: 0.4152  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1400/8650]  eta: 0:59:51  lr: 0.000020  loss: 31.4446  time: 0.4053  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1450/8650]  eta: 0:59:25  lr: 0.000020  loss: 23.0459  time: 0.4152  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1500/8650]  eta: 0:59:00  lr: 0.000020  loss: 34.4280  time: 0.4097  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1550/8650]  eta: 0:58:36  lr: 0.000020  loss: 14.0686  time: 0.4199  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1600/8650]  eta: 0:58:11  lr: 0.000020  loss: 22.0530  time: 0.4191  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [1650/8650]  eta: 0:57:49  lr: 0.000020  loss: 21.2476  time: 0.4268  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [1700/8650]  eta: 0:57:24  lr: 0.000020  loss: 26.8395  time: 0.4226  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [1750/8650]  eta: 0:57:02  lr: 0.000020  loss: 21.8298  time: 0.4376  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [1800/8650]  eta: 0:56:36  lr: 0.000020  loss: 17.9410  time: 0.4301  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [1850/8650]  eta: 0:56:09  lr: 0.000020  loss: 31.3961  time: 0.4281  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [1900/8650]  eta: 0:55:44  lr: 0.000020  loss: 13.1777  time: 0.4443  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [1950/8650]  eta: 0:55:20  lr: 0.000020  loss: 16.6995  time: 0.4509  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [2000/8650]  eta: 0:54:55  lr: 0.000020  loss: 28.7178  time: 0.4560  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [2050/8650]  eta: 0:54:33  lr: 0.000020  loss: 32.5201  time: 0.4893  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [2100/8650]  eta: 0:54:10  lr: 0.000020  loss: 18.4898  time: 0.4953  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [2150/8650]  eta: 0:53:45  lr: 0.000020  loss: 34.3601  time: 0.5033  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [2200/8650]  eta: 0:53:22  lr: 0.000020  loss: 18.5576  time: 0.5116  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [2250/8650]  eta: 0:52:58  lr: 0.000020  loss: 17.8382  time: 0.5422  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [2300/8650]  eta: 0:52:33  lr: 0.000020  loss: 24.8964  time: 0.5332  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2350/8650]  eta: 0:52:08  lr: 0.000020  loss: 21.7914  time: 0.5521  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2400/8650]  eta: 0:51:44  lr: 0.000020  loss: 37.3085  time: 0.5586  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [2450/8650]  eta: 0:51:19  lr: 0.000020  loss: 26.3073  time: 0.5739  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [1]  [2500/8650]  eta: 0:50:55  lr: 0.000020  loss: 36.8287  time: 0.5816  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2550/8650]  eta: 0:50:30  lr: 0.000020  loss: 14.4934  time: 0.5780  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2600/8650]  eta: 0:50:09  lr: 0.000020  loss: 21.4411  time: 0.6474  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2650/8650]  eta: 0:49:43  lr: 0.000020  loss: 26.1549  time: 0.5926  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2700/8650]  eta: 0:49:17  lr: 0.000020  loss: 21.0168  time: 0.5819  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [2750/8650]  eta: 0:48:52  lr: 0.000020  loss: 24.0839  time: 0.5869  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [2800/8650]  eta: 0:48:27  lr: 0.000020  loss: 17.2280  time: 0.5765  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2850/8650]  eta: 0:48:02  lr: 0.000020  loss: 25.9620  time: 0.5708  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [2900/8650]  eta: 0:47:37  lr: 0.000020  loss: 18.8616  time: 0.5705  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [2950/8650]  eta: 0:47:12  lr: 0.000020  loss: 27.1369  time: 0.5481  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [3000/8650]  eta: 0:46:47  lr: 0.000020  loss: 20.4745  time: 0.5220  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [3050/8650]  eta: 0:46:22  lr: 0.000020  loss: 18.0065  time: 0.5346  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [3100/8650]  eta: 0:45:57  lr: 0.000020  loss: 22.3434  time: 0.5137  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [3150/8650]  eta: 0:45:32  lr: 0.000020  loss: 25.5200  time: 0.5084  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [3200/8650]  eta: 0:45:07  lr: 0.000020  loss: 27.1902  time: 0.5017  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [3250/8650]  eta: 0:44:42  lr: 0.000020  loss: 33.0265  time: 0.4868  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [3300/8650]  eta: 0:44:16  lr: 0.000020  loss: 18.1929  time: 0.4705  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [3350/8650]  eta: 0:43:51  lr: 0.000020  loss: 45.7213  time: 0.4532  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [3400/8650]  eta: 0:43:25  lr: 0.000020  loss: 20.2008  time: 0.4346  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [3450/8650]  eta: 0:43:00  lr: 0.000020  loss: 27.2828  time: 0.4146  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [3500/8650]  eta: 0:42:35  lr: 0.000020  loss: 23.7263  time: 0.4063  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3550/8650]  eta: 0:42:10  lr: 0.000020  loss: 20.8610  time: 0.4070  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3600/8650]  eta: 0:41:44  lr: 0.000020  loss: 28.6222  time: 0.4104  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3650/8650]  eta: 0:41:19  lr: 0.000020  loss: 18.4512  time: 0.4102  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3700/8650]  eta: 0:40:54  lr: 0.000020  loss: 19.3946  time: 0.4050  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [3750/8650]  eta: 0:40:29  lr: 0.000020  loss: 16.3394  time: 0.4079  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3800/8650]  eta: 0:40:04  lr: 0.000020  loss: 21.4918  time: 0.4133  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3850/8650]  eta: 0:39:39  lr: 0.000020  loss: 39.7055  time: 0.4272  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [3900/8650]  eta: 0:39:15  lr: 0.000020  loss: 28.1074  time: 0.4270  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [3950/8650]  eta: 0:38:50  lr: 0.000020  loss: 17.6258  time: 0.4276  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [4000/8650]  eta: 0:38:25  lr: 0.000020  loss: 20.2269  time: 0.4332  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4050/8650]  eta: 0:38:00  lr: 0.000020  loss: 15.5120  time: 0.4190  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4100/8650]  eta: 0:37:35  lr: 0.000020  loss: 19.3452  time: 0.4205  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [4150/8650]  eta: 0:37:10  lr: 0.000020  loss: 26.0279  time: 0.4185  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [4200/8650]  eta: 0:36:45  lr: 0.000020  loss: 23.2071  time: 0.4288  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4250/8650]  eta: 0:36:21  lr: 0.000020  loss: 15.0680  time: 0.4426  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4300/8650]  eta: 0:35:57  lr: 0.000020  loss: 34.4693  time: 0.4273  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [4350/8650]  eta: 0:35:32  lr: 0.000020  loss: 14.4740  time: 0.4275  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4400/8650]  eta: 0:35:08  lr: 0.000020  loss: 26.8841  time: 0.4463  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4450/8650]  eta: 0:34:43  lr: 0.000020  loss: 23.5780  time: 0.4335  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4500/8650]  eta: 0:34:18  lr: 0.000020  loss: 28.3120  time: 0.4372  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [4550/8650]  eta: 0:33:53  lr: 0.000020  loss: 15.5232  time: 0.4573  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4600/8650]  eta: 0:33:28  lr: 0.000020  loss: 39.3767  time: 0.4536  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [4650/8650]  eta: 0:33:03  lr: 0.000020  loss: 17.5988  time: 0.4726  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [4700/8650]  eta: 0:32:38  lr: 0.000020  loss: 29.0738  time: 0.4877  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [4750/8650]  eta: 0:32:13  lr: 0.000020  loss: 31.9686  time: 0.4906  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [4800/8650]  eta: 0:31:48  lr: 0.000020  loss: 19.7131  time: 0.4851  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [4850/8650]  eta: 0:31:23  lr: 0.000020  loss: 21.7448  time: 0.5059  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [4900/8650]  eta: 0:30:58  lr: 0.000020  loss: 31.8416  time: 0.5137  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [4950/8650]  eta: 0:30:33  lr: 0.000020  loss: 19.0313  time: 0.5309  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [5000/8650]  eta: 0:30:08  lr: 0.000020  loss: 15.6588  time: 0.5403  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [5050/8650]  eta: 0:29:43  lr: 0.000020  loss: 19.3596  time: 0.5355  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [5100/8650]  eta: 0:29:18  lr: 0.000020  loss: 21.2942  time: 0.5459  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [5150/8650]  eta: 0:28:54  lr: 0.000020  loss: 46.1899  time: 0.5659  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [5200/8650]  eta: 0:28:29  lr: 0.000020  loss: 16.3311  time: 0.5673  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [5250/8650]  eta: 0:28:04  lr: 0.000020  loss: 26.3486  time: 0.5905  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5300/8650]  eta: 0:27:39  lr: 0.000020  loss: 17.5969  time: 0.5803  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5350/8650]  eta: 0:27:14  lr: 0.000020  loss: 19.2663  time: 0.5781  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [5400/8650]  eta: 0:26:49  lr: 0.000020  loss: 15.3385  time: 0.5991  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [5450/8650]  eta: 0:26:25  lr: 0.000020  loss: 24.0409  time: 0.5876  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [5500/8650]  eta: 0:26:00  lr: 0.000020  loss: 31.7076  time: 0.5807  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5550/8650]  eta: 0:25:35  lr: 0.000020  loss: 22.8455  time: 0.5731  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [5600/8650]  eta: 0:25:10  lr: 0.000020  loss: 17.1096  time: 0.5746  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [5650/8650]  eta: 0:24:45  lr: 0.000020  loss: 17.9794  time: 0.5692  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5700/8650]  eta: 0:24:20  lr: 0.000020  loss: 17.8940  time: 0.5519  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [5750/8650]  eta: 0:23:55  lr: 0.000020  loss: 14.1961  time: 0.5399  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5800/8650]  eta: 0:23:30  lr: 0.000020  loss: 22.4965  time: 0.5202  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [5850/8650]  eta: 0:23:05  lr: 0.000020  loss: 15.8099  time: 0.5226  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [5900/8650]  eta: 0:22:40  lr: 0.000020  loss: 14.6058  time: 0.5147  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [5950/8650]  eta: 0:22:15  lr: 0.000020  loss: 22.9696  time: 0.5016  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [6000/8650]  eta: 0:21:50  lr: 0.000020  loss: 22.0582  time: 0.4977  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [6050/8650]  eta: 0:21:25  lr: 0.000020  loss: 18.6004  time: 0.4926  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [6100/8650]  eta: 0:21:00  lr: 0.000020  loss: 26.4666  time: 0.4796  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [6150/8650]  eta: 0:20:36  lr: 0.000020  loss: 19.4062  time: 0.4635  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [6200/8650]  eta: 0:20:11  lr: 0.000020  loss: 15.5885  time: 0.4562  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [6250/8650]  eta: 0:19:46  lr: 0.000020  loss: 18.3974  time: 0.4382  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [6300/8650]  eta: 0:19:21  lr: 0.000020  loss: 35.2439  time: 0.4446  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [6350/8650]  eta: 0:18:57  lr: 0.000020  loss: 23.3281  time: 0.4177  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [6400/8650]  eta: 0:18:32  lr: 0.000020  loss: 25.5461  time: 0.4037  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6450/8650]  eta: 0:18:07  lr: 0.000020  loss: 16.6038  time: 0.4054  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6500/8650]  eta: 0:17:42  lr: 0.000020  loss: 28.1559  time: 0.4128  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6550/8650]  eta: 0:17:17  lr: 0.000020  loss: 17.8167  time: 0.4072  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6600/8650]  eta: 0:16:52  lr: 0.000020  loss: 26.7282  time: 0.4113  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6650/8650]  eta: 0:16:28  lr: 0.000020  loss: 15.6762  time: 0.4068  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6700/8650]  eta: 0:16:03  lr: 0.000020  loss: 22.5108  time: 0.4095  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6750/8650]  eta: 0:15:38  lr: 0.000020  loss: 15.2237  time: 0.4100  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6800/8650]  eta: 0:15:13  lr: 0.000020  loss: 18.4500  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6850/8650]  eta: 0:14:49  lr: 0.000020  loss: 18.2332  time: 0.4217  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6900/8650]  eta: 0:14:24  lr: 0.000020  loss: 24.7717  time: 0.4254  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [6950/8650]  eta: 0:13:59  lr: 0.000020  loss: 20.8382  time: 0.4211  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [7000/8650]  eta: 0:13:34  lr: 0.000020  loss: 14.5314  time: 0.4519  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [1]  [7050/8650]  eta: 0:13:10  lr: 0.000020  loss: 24.0545  time: 0.4351  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [7100/8650]  eta: 0:12:45  lr: 0.000020  loss: 18.0681  time: 0.4478  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7150/8650]  eta: 0:12:20  lr: 0.000020  loss: 26.1681  time: 0.4549  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7200/8650]  eta: 0:11:55  lr: 0.000020  loss: 13.3234  time: 0.4578  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7250/8650]  eta: 0:11:31  lr: 0.000020  loss: 23.5167  time: 0.4608  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7300/8650]  eta: 0:11:06  lr: 0.000020  loss: 22.7001  time: 0.4597  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7350/8650]  eta: 0:10:41  lr: 0.000020  loss: 20.7633  time: 0.4612  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7400/8650]  eta: 0:10:17  lr: 0.000020  loss: 17.4517  time: 0.4641  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7450/8650]  eta: 0:09:52  lr: 0.000020  loss: 22.1508  time: 0.4773  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7500/8650]  eta: 0:09:27  lr: 0.000020  loss: 35.9607  time: 0.4876  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7550/8650]  eta: 0:09:02  lr: 0.000020  loss: 18.9273  time: 0.4988  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7600/8650]  eta: 0:08:38  lr: 0.000020  loss: 22.3815  time: 0.4885  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [7650/8650]  eta: 0:08:13  lr: 0.000020  loss: 24.2215  time: 0.4979  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7700/8650]  eta: 0:07:48  lr: 0.000020  loss: 20.0663  time: 0.4910  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [7750/8650]  eta: 0:07:24  lr: 0.000020  loss: 29.1755  time: 0.5094  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7800/8650]  eta: 0:06:59  lr: 0.000020  loss: 24.0411  time: 0.5327  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [7850/8650]  eta: 0:06:34  lr: 0.000020  loss: 18.0621  time: 0.5284  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [7900/8650]  eta: 0:06:10  lr: 0.000020  loss: 15.4311  time: 0.5301  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [7950/8650]  eta: 0:05:45  lr: 0.000020  loss: 29.6255  time: 0.5338  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [8000/8650]  eta: 0:05:20  lr: 0.000020  loss: 23.0053  time: 0.5417  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [8050/8650]  eta: 0:04:55  lr: 0.000020  loss: 24.9241  time: 0.5558  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [8100/8650]  eta: 0:04:31  lr: 0.000020  loss: 17.0347  time: 0.5619  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [8150/8650]  eta: 0:04:06  lr: 0.000020  loss: 25.5057  time: 0.5675  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [8200/8650]  eta: 0:03:42  lr: 0.000020  loss: 16.5889  time: 0.5937  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [8250/8650]  eta: 0:03:17  lr: 0.000020  loss: 17.9481  time: 0.5728  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [8300/8650]  eta: 0:02:52  lr: 0.000020  loss: 27.6009  time: 0.5810  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [8350/8650]  eta: 0:02:27  lr: 0.000020  loss: 25.8270  time: 0.5867  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [8400/8650]  eta: 0:02:03  lr: 0.000020  loss: 11.5503  time: 0.5819  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [8450/8650]  eta: 0:01:38  lr: 0.000020  loss: 15.3030  time: 0.5799  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [8500/8650]  eta: 0:01:13  lr: 0.000020  loss: 25.3112  time: 0.5772  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [8550/8650]  eta: 0:00:49  lr: 0.000020  loss: 15.7790  time: 0.5710  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 14.4937  time: 0.5571  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 13.2451  time: 0.5474  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1] Total time: 1:11:05 (0.4931 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 21.9879\n",
            "Loss: 19.2511\n",
            "==========================================\n",
            "Train Epoch: [2]  [   0/8650]  eta: 6:25:00  lr: 0.000020  loss: 19.2511  time: 2.6705  data: 1.5533  max mem: 9086\n",
            "Loss: 16.5559\n",
            "==========================================\n",
            "Loss: 19.2806\n",
            "==========================================\n",
            "Loss: 13.8969\n",
            "==========================================\n",
            "Loss: 34.0758\n",
            "==========================================\n",
            "Loss: 23.3933\n",
            "==========================================\n",
            "Loss: 13.147\n",
            "==========================================\n",
            "Loss: 19.5056\n",
            "==========================================\n",
            "Loss: 32.7862\n",
            "==========================================\n",
            "Loss: 27.0378\n",
            "==========================================\n",
            "Loss: 22.558\n",
            "==========================================\n",
            "Loss: 23.8386\n",
            "==========================================\n",
            "Loss: 12.6657\n",
            "==========================================\n",
            "Loss: 17.0669\n",
            "==========================================\n",
            "Loss: 11.1621\n",
            "==========================================\n",
            "Loss: 18.5173\n",
            "==========================================\n",
            "Loss: 27.7185\n",
            "==========================================\n",
            "Loss: 20.0063\n",
            "==========================================\n",
            "Loss: 19.4239\n",
            "==========================================\n",
            "Loss: 18.5694\n",
            "==========================================\n",
            "Loss: 20.3908\n",
            "==========================================\n",
            "Loss: 21.9254\n",
            "==========================================\n",
            "Loss: 33.371\n",
            "==========================================\n",
            "Loss: 13.3033\n",
            "==========================================\n",
            "Loss: 14.8523\n",
            "==========================================\n",
            "Loss: 27.0644\n",
            "==========================================\n",
            "Loss: 15.3798\n",
            "==========================================\n",
            "Loss: 20.7649\n",
            "==========================================\n",
            "Loss: 31.3668\n",
            "==========================================\n",
            "Loss: 22.521\n",
            "==========================================\n",
            "Loss: 44.274\n",
            "==========================================\n",
            "Loss: 17.5175\n",
            "==========================================\n",
            "Loss: 14.2327\n",
            "==========================================\n",
            "Loss: 12.6987\n",
            "==========================================\n",
            "Loss: 25.5779\n",
            "==========================================\n",
            "Loss: 14.776\n",
            "==========================================\n",
            "Loss: 16.6599\n",
            "==========================================\n",
            "Loss: 23.3692\n",
            "==========================================\n",
            "Loss: 16.0843\n",
            "==========================================\n",
            "Loss: 17.061\n",
            "==========================================\n",
            "Loss: 18.7637\n",
            "==========================================\n",
            "Loss: 20.6308\n",
            "==========================================\n",
            "Loss: 13.9761\n",
            "==========================================\n",
            "Loss: 14.1089\n",
            "==========================================\n",
            "Loss: 14.1064\n",
            "==========================================\n",
            "Loss: 28.177\n",
            "==========================================\n",
            "Loss: 20.3987\n",
            "==========================================\n",
            "Loss: 17.3264\n",
            "==========================================\n",
            "Loss: 28.5283\n",
            "==========================================\n",
            "Loss: 21.847\n",
            "==========================================\n",
            "Train Epoch: [2]  [  50/8650]  eta: 1:24:23  lr: 0.000020  loss: 25.1259  time: 0.6081  data: 0.0005  max mem: 9086\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 63, in train\n",
            "    metric_logger.update(loss=loss.item())\n",
            "                              ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_animals.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_animals \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "   --resume .output/textVQA_pretrain_animals/checkpoint_00.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Textvqa + PretrainTextCaps"
      ],
      "metadata": {
        "id": "c1Xp2adpkVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_Textcaps.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_textcaps \\\n",
        "    --device cuda \\\n",
        "    --distributed False"
      ],
      "metadata": {
        "id": "9RnYLAjmkcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA + resnet"
      ],
      "metadata": {
        "id": "ZU3eOQrIfwJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/dataset_animals"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ekwts1oGwoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain.py --config configs/pre_animals.yaml --output_dir output/pretrain_animals --checkpoint output/pretrain_animals/checkpoint_22.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5gFi5qxQmH",
        "outputId": "41cbfa9f-fb90-4d33-abf7-62afd1e11954",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating dataset\n",
            "Đang tải dữ liệu từ: /content/dataset_animals/dataset.json\n",
            "number of training samples: 20000\n",
            "Creating model\n",
            "/embeddings/word_embeddings is tied\n",
            "/embeddings/position_embeddings is tied\n",
            "/embeddings/LayerNorm is tied\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "/encoder/layer/2/crossattention/self/query is tied\n",
            "/encoder/layer/2/crossattention/self/key is tied\n",
            "/encoder/layer/2/crossattention/self/value is tied\n",
            "/encoder/layer/2/crossattention/output/dense is tied\n",
            "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/2/intermediate/dense is tied\n",
            "/encoder/layer/2/output/dense is tied\n",
            "/encoder/layer/2/output/LayerNorm is tied\n",
            "/encoder/layer/3/crossattention/self/query is tied\n",
            "/encoder/layer/3/crossattention/self/key is tied\n",
            "/encoder/layer/3/crossattention/self/value is tied\n",
            "/encoder/layer/3/crossattention/output/dense is tied\n",
            "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/3/intermediate/dense is tied\n",
            "/encoder/layer/3/output/dense is tied\n",
            "/encoder/layer/3/output/LayerNorm is tied\n",
            "/encoder/layer/4/crossattention/self/query is tied\n",
            "/encoder/layer/4/crossattention/self/key is tied\n",
            "/encoder/layer/4/crossattention/self/value is tied\n",
            "/encoder/layer/4/crossattention/output/dense is tied\n",
            "/encoder/layer/4/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/4/intermediate/dense is tied\n",
            "/encoder/layer/4/output/dense is tied\n",
            "/encoder/layer/4/output/LayerNorm is tied\n",
            "/encoder/layer/5/crossattention/self/query is tied\n",
            "/encoder/layer/5/crossattention/self/key is tied\n",
            "/encoder/layer/5/crossattention/self/value is tied\n",
            "/encoder/layer/5/crossattention/output/dense is tied\n",
            "/encoder/layer/5/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/5/intermediate/dense is tied\n",
            "/encoder/layer/5/output/dense is tied\n",
            "/encoder/layer/5/output/LayerNorm is tied\n",
            "/encoder/layer/6/crossattention/self/query is tied\n",
            "/encoder/layer/6/crossattention/self/key is tied\n",
            "/encoder/layer/6/crossattention/self/value is tied\n",
            "/encoder/layer/6/crossattention/output/dense is tied\n",
            "/encoder/layer/6/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/6/intermediate/dense is tied\n",
            "/encoder/layer/6/output/dense is tied\n",
            "/encoder/layer/6/output/LayerNorm is tied\n",
            "/encoder/layer/7/crossattention/self/query is tied\n",
            "/encoder/layer/7/crossattention/self/key is tied\n",
            "/encoder/layer/7/crossattention/self/value is tied\n",
            "/encoder/layer/7/crossattention/output/dense is tied\n",
            "/encoder/layer/7/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/7/intermediate/dense is tied\n",
            "/encoder/layer/7/output/dense is tied\n",
            "/encoder/layer/7/output/LayerNorm is tied\n",
            "/encoder/layer/8/crossattention/self/query is tied\n",
            "/encoder/layer/8/crossattention/self/key is tied\n",
            "/encoder/layer/8/crossattention/self/value is tied\n",
            "/encoder/layer/8/crossattention/output/dense is tied\n",
            "/encoder/layer/8/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/8/intermediate/dense is tied\n",
            "/encoder/layer/8/output/dense is tied\n",
            "/encoder/layer/8/output/LayerNorm is tied\n",
            "/encoder/layer/9/crossattention/self/query is tied\n",
            "/encoder/layer/9/crossattention/self/key is tied\n",
            "/encoder/layer/9/crossattention/self/value is tied\n",
            "/encoder/layer/9/crossattention/output/dense is tied\n",
            "/encoder/layer/9/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/9/intermediate/dense is tied\n",
            "/encoder/layer/9/output/dense is tied\n",
            "/encoder/layer/9/output/LayerNorm is tied\n",
            "/encoder/layer/10/crossattention/self/query is tied\n",
            "/encoder/layer/10/crossattention/self/key is tied\n",
            "/encoder/layer/10/crossattention/self/value is tied\n",
            "/encoder/layer/10/crossattention/output/dense is tied\n",
            "/encoder/layer/10/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/10/intermediate/dense is tied\n",
            "/encoder/layer/10/output/dense is tied\n",
            "/encoder/layer/10/output/LayerNorm is tied\n",
            "/encoder/layer/11/crossattention/self/query is tied\n",
            "/encoder/layer/11/crossattention/self/key is tied\n",
            "/encoder/layer/11/crossattention/self/value is tied\n",
            "/encoder/layer/11/crossattention/output/dense is tied\n",
            "/encoder/layer/11/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/11/intermediate/dense is tied\n",
            "/encoder/layer/11/output/dense is tied\n",
            "/encoder/layer/11/output/LayerNorm is tied\n",
            "resume checkpoint from output/pretrain_animals/checkpoint_16.pth\n",
            "Start training\n",
            "Train Epoch: [17]  [   0/2500]  eta: 3:19:38  lr: 0.000050  loss_ita: 6.1392  loss_itm: 0.6385  loss_lm: 4.7927  time: 4.7912  data: 1.5059  max mem: 7121\n",
            "Train Epoch: [17]  [  50/2500]  eta: 0:39:44  lr: 0.000050  loss_ita: 5.9841  loss_itm: 0.6401  loss_lm: 4.4402  time: 0.8945  data: 0.0004  max mem: 7129\n",
            "Train Epoch: [17]  [ 100/2500]  eta: 0:38:02  lr: 0.000050  loss_ita: 5.9510  loss_itm: 0.6436  loss_lm: 4.7203  time: 0.9362  data: 0.0003  max mem: 7129\n",
            "Train Epoch: [17]  [ 150/2500]  eta: 0:36:56  lr: 0.000050  loss_ita: 5.8191  loss_itm: 0.6290  loss_lm: 4.2306  time: 0.9299  data: 0.0006  max mem: 7132\n",
            "Train Epoch: [17]  [ 200/2500]  eta: 0:36:00  lr: 0.000050  loss_ita: 5.8432  loss_itm: 0.6357  loss_lm: 4.4590  time: 0.9304  data: 0.0005  max mem: 7132\n",
            "Train Epoch: [17]  [ 250/2500]  eta: 0:35:06  lr: 0.000050  loss_ita: 6.0463  loss_itm: 0.6259  loss_lm: 4.6369  time: 0.9230  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 300/2500]  eta: 0:34:17  lr: 0.000050  loss_ita: 5.9681  loss_itm: 0.6204  loss_lm: 4.2531  time: 0.9307  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 350/2500]  eta: 0:33:28  lr: 0.000050  loss_ita: 5.8556  loss_itm: 0.6375  loss_lm: 4.7831  time: 0.9301  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 400/2500]  eta: 0:32:40  lr: 0.000050  loss_ita: 6.1228  loss_itm: 0.6355  loss_lm: 4.8984  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 450/2500]  eta: 0:31:51  lr: 0.000050  loss_ita: 6.3495  loss_itm: 0.6393  loss_lm: 4.6135  time: 0.9222  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 500/2500]  eta: 0:31:04  lr: 0.000050  loss_ita: 6.1634  loss_itm: 0.6500  loss_lm: 4.8925  time: 0.9304  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 550/2500]  eta: 0:30:17  lr: 0.000050  loss_ita: 5.9357  loss_itm: 0.6350  loss_lm: 4.1571  time: 0.9271  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 600/2500]  eta: 0:29:30  lr: 0.000050  loss_ita: 5.8463  loss_itm: 0.6394  loss_lm: 4.1982  time: 0.9227  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [ 650/2500]  eta: 0:28:43  lr: 0.000050  loss_ita: 6.0158  loss_itm: 0.6351  loss_lm: 4.7299  time: 0.9328  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 700/2500]  eta: 0:27:57  lr: 0.000050  loss_ita: 6.1428  loss_itm: 0.6314  loss_lm: 4.6872  time: 0.9312  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 750/2500]  eta: 0:27:10  lr: 0.000050  loss_ita: 6.2539  loss_itm: 0.6385  loss_lm: 4.5769  time: 0.9238  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 800/2500]  eta: 0:26:23  lr: 0.000050  loss_ita: 6.0947  loss_itm: 0.6348  loss_lm: 4.4382  time: 0.9340  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 850/2500]  eta: 0:25:36  lr: 0.000050  loss_ita: 5.9637  loss_itm: 0.6348  loss_lm: 4.9964  time: 0.9370  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 900/2500]  eta: 0:24:49  lr: 0.000050  loss_ita: 6.2077  loss_itm: 0.6503  loss_lm: 4.5971  time: 0.9215  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 950/2500]  eta: 0:24:03  lr: 0.000050  loss_ita: 6.2782  loss_itm: 0.6304  loss_lm: 4.1598  time: 0.9229  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1000/2500]  eta: 0:23:16  lr: 0.000050  loss_ita: 6.1663  loss_itm: 0.6368  loss_lm: 4.6378  time: 0.9374  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1050/2500]  eta: 0:22:30  lr: 0.000050  loss_ita: 6.2009  loss_itm: 0.6334  loss_lm: 4.4458  time: 0.9369  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1100/2500]  eta: 0:21:43  lr: 0.000050  loss_ita: 6.1823  loss_itm: 0.6355  loss_lm: 4.9945  time: 0.9254  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1150/2500]  eta: 0:20:56  lr: 0.000050  loss_ita: 6.0765  loss_itm: 0.6280  loss_lm: 4.5967  time: 0.9312  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1200/2500]  eta: 0:20:10  lr: 0.000050  loss_ita: 6.0619  loss_itm: 0.6410  loss_lm: 4.8511  time: 0.9367  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [1250/2500]  eta: 0:19:23  lr: 0.000050  loss_ita: 6.0537  loss_itm: 0.6422  loss_lm: 5.3150  time: 0.9227  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1300/2500]  eta: 0:18:36  lr: 0.000050  loss_ita: 5.9791  loss_itm: 0.6471  loss_lm: 4.5806  time: 0.9206  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1350/2500]  eta: 0:17:50  lr: 0.000050  loss_ita: 5.7339  loss_itm: 0.6316  loss_lm: 4.2935  time: 0.9418  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1400/2500]  eta: 0:17:03  lr: 0.000050  loss_ita: 5.8574  loss_itm: 0.6248  loss_lm: 4.8669  time: 0.9227  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1450/2500]  eta: 0:16:17  lr: 0.000050  loss_ita: 5.8236  loss_itm: 0.6454  loss_lm: 4.4377  time: 0.9271  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1500/2500]  eta: 0:15:30  lr: 0.000050  loss_ita: 5.6852  loss_itm: 0.6495  loss_lm: 4.7744  time: 0.9329  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1550/2500]  eta: 0:14:44  lr: 0.000050  loss_ita: 5.4990  loss_itm: 0.6276  loss_lm: 4.4243  time: 0.9361  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1600/2500]  eta: 0:13:57  lr: 0.000050  loss_ita: 5.6231  loss_itm: 0.6459  loss_lm: 4.4868  time: 0.9241  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1650/2500]  eta: 0:13:10  lr: 0.000050  loss_ita: 5.7932  loss_itm: 0.6374  loss_lm: 4.5288  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1700/2500]  eta: 0:12:24  lr: 0.000050  loss_ita: 5.9085  loss_itm: 0.6344  loss_lm: 4.4765  time: 0.9344  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1750/2500]  eta: 0:11:37  lr: 0.000050  loss_ita: 6.1626  loss_itm: 0.6300  loss_lm: 4.4793  time: 0.9218  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1800/2500]  eta: 0:10:51  lr: 0.000050  loss_ita: 6.1059  loss_itm: 0.6371  loss_lm: 4.8303  time: 0.9242  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1850/2500]  eta: 0:10:04  lr: 0.000050  loss_ita: 5.7550  loss_itm: 0.6354  loss_lm: 4.7988  time: 0.9292  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [1900/2500]  eta: 0:09:18  lr: 0.000050  loss_ita: 5.6715  loss_itm: 0.6439  loss_lm: 4.9443  time: 0.9322  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [1950/2500]  eta: 0:08:31  lr: 0.000050  loss_ita: 5.9078  loss_itm: 0.6322  loss_lm: 4.9832  time: 0.9235  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2000/2500]  eta: 0:07:45  lr: 0.000050  loss_ita: 6.1245  loss_itm: 0.6394  loss_lm: 4.4271  time: 0.9317  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2050/2500]  eta: 0:06:58  lr: 0.000050  loss_ita: 6.3317  loss_itm: 0.6358  loss_lm: 4.5954  time: 0.9336  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2100/2500]  eta: 0:06:12  lr: 0.000050  loss_ita: 6.1765  loss_itm: 0.6298  loss_lm: 4.4585  time: 0.9223  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2150/2500]  eta: 0:05:25  lr: 0.000050  loss_ita: 5.8163  loss_itm: 0.6358  loss_lm: 4.6149  time: 0.9204  data: 0.0002  max mem: 7134\n",
            "Train Epoch: [17]  [2200/2500]  eta: 0:04:39  lr: 0.000050  loss_ita: 5.7070  loss_itm: 0.6437  loss_lm: 4.4813  time: 0.9407  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2250/2500]  eta: 0:03:52  lr: 0.000050  loss_ita: 5.9511  loss_itm: 0.6427  loss_lm: 4.4372  time: 0.9190  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2300/2500]  eta: 0:03:05  lr: 0.000050  loss_ita: 6.0164  loss_itm: 0.6342  loss_lm: 4.4307  time: 0.9265  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2350/2500]  eta: 0:02:19  lr: 0.000050  loss_ita: 5.9607  loss_itm: 0.6364  loss_lm: 4.5868  time: 0.9313  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2400/2500]  eta: 0:01:32  lr: 0.000050  loss_ita: 6.0198  loss_itm: 0.6424  loss_lm: 4.5925  time: 0.9271  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2450/2500]  eta: 0:00:46  lr: 0.000050  loss_ita: 6.2225  loss_itm: 0.6522  loss_lm: 4.5737  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2499/2500]  eta: 0:00:00  lr: 0.000050  loss_ita: 6.3974  loss_itm: 0.6369  loss_lm: 5.1242  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17] Total time: 0:38:44 (0.9298 s / it)\n",
            "Averaged stats: lr: 0.0001  loss_ita: 5.9935  loss_itm: 0.6356  loss_lm: 4.6061\n",
            "Train Epoch: [18]  [   0/2500]  eta: 1:20:04  lr: 0.000045  loss_ita: 6.2714  loss_itm: 0.6457  loss_lm: 4.7524  time: 1.9218  data: 0.8585  max mem: 7970\n",
            "Train Epoch: [18]  [  50/2500]  eta: 0:40:21  lr: 0.000045  loss_ita: 6.3983  loss_itm: 0.6511  loss_lm: 4.1244  time: 0.9882  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 100/2500]  eta: 0:38:50  lr: 0.000045  loss_ita: 6.3850  loss_itm: 0.6482  loss_lm: 4.3894  time: 0.9561  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 150/2500]  eta: 0:37:38  lr: 0.000045  loss_ita: 6.2264  loss_itm: 0.6374  loss_lm: 4.5178  time: 0.9203  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 200/2500]  eta: 0:36:30  lr: 0.000045  loss_ita: 5.9527  loss_itm: 0.6499  loss_lm: 4.5807  time: 0.9236  data: 0.0003  max mem: 7982\n",
            "Train Epoch: [18]  [ 250/2500]  eta: 0:35:32  lr: 0.000045  loss_ita: 5.7489  loss_itm: 0.6327  loss_lm: 4.1563  time: 0.9307  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 300/2500]  eta: 0:34:36  lr: 0.000045  loss_ita: 5.7306  loss_itm: 0.6399  loss_lm: 4.6968  time: 0.9242  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 350/2500]  eta: 0:33:43  lr: 0.000045  loss_ita: 5.9946  loss_itm: 0.6380  loss_lm: 4.8402  time: 0.9228  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 400/2500]  eta: 0:32:53  lr: 0.000045  loss_ita: 6.1702  loss_itm: 0.6330  loss_lm: 4.2296  time: 0.9339  data: 0.0004  max mem: 7983\n",
            "Train Epoch: [18]  [ 450/2500]  eta: 0:32:02  lr: 0.000045  loss_ita: 6.1112  loss_itm: 0.6402  loss_lm: 4.3074  time: 0.9217  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 500/2500]  eta: 0:31:12  lr: 0.000045  loss_ita: 6.2875  loss_itm: 0.6396  loss_lm: 4.1692  time: 0.9172  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 550/2500]  eta: 0:30:24  lr: 0.000045  loss_ita: 6.1270  loss_itm: 0.6236  loss_lm: 4.2401  time: 0.9325  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 600/2500]  eta: 0:29:36  lr: 0.000045  loss_ita: 5.8813  loss_itm: 0.6358  loss_lm: 4.6269  time: 0.9270  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 650/2500]  eta: 0:28:47  lr: 0.000045  loss_ita: 5.8662  loss_itm: 0.6497  loss_lm: 4.5022  time: 0.9237  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 700/2500]  eta: 0:27:59  lr: 0.000045  loss_ita: 6.0300  loss_itm: 0.6378  loss_lm: 4.8883  time: 0.9273  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 750/2500]  eta: 0:27:12  lr: 0.000045  loss_ita: 6.1708  loss_itm: 0.6311  loss_lm: 4.5949  time: 0.9273  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 800/2500]  eta: 0:26:24  lr: 0.000045  loss_ita: 6.1851  loss_itm: 0.6380  loss_lm: 4.5566  time: 0.9163  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 850/2500]  eta: 0:25:36  lr: 0.000045  loss_ita: 6.0905  loss_itm: 0.6362  loss_lm: 4.4975  time: 0.9210  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 900/2500]  eta: 0:24:50  lr: 0.000045  loss_ita: 6.1650  loss_itm: 0.6281  loss_lm: 4.4557  time: 0.9308  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 950/2500]  eta: 0:24:02  lr: 0.000045  loss_ita: 6.4208  loss_itm: 0.6424  loss_lm: 4.9033  time: 0.9256  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1000/2500]  eta: 0:23:16  lr: 0.000045  loss_ita: 6.4365  loss_itm: 0.6313  loss_lm: 4.4134  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1050/2500]  eta: 0:22:29  lr: 0.000045  loss_ita: 6.1584  loss_itm: 0.6530  loss_lm: 4.5601  time: 0.9284  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1100/2500]  eta: 0:21:43  lr: 0.000045  loss_ita: 6.0874  loss_itm: 0.6354  loss_lm: 4.9366  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1150/2500]  eta: 0:20:56  lr: 0.000045  loss_ita: 5.9607  loss_itm: 0.6468  loss_lm: 4.2832  time: 0.9209  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1200/2500]  eta: 0:20:09  lr: 0.000045  loss_ita: 5.9632  loss_itm: 0.6471  loss_lm: 4.6797  time: 0.9294  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1250/2500]  eta: 0:19:22  lr: 0.000045  loss_ita: 6.1932  loss_itm: 0.6278  loss_lm: 4.3471  time: 0.9296  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1300/2500]  eta: 0:18:36  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6202  loss_lm: 4.5908  time: 0.9244  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1350/2500]  eta: 0:17:49  lr: 0.000045  loss_ita: 6.0786  loss_itm: 0.6392  loss_lm: 4.4718  time: 0.9198  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1400/2500]  eta: 0:17:02  lr: 0.000045  loss_ita: 6.0546  loss_itm: 0.6346  loss_lm: 4.4181  time: 0.9355  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1450/2500]  eta: 0:16:16  lr: 0.000045  loss_ita: 6.1391  loss_itm: 0.6276  loss_lm: 4.5114  time: 0.9284  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1500/2500]  eta: 0:15:29  lr: 0.000045  loss_ita: 5.7351  loss_itm: 0.6409  loss_lm: 4.5599  time: 0.9248  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [1550/2500]  eta: 0:14:43  lr: 0.000045  loss_ita: 5.2817  loss_itm: 0.6334  loss_lm: 4.4926  time: 0.9290  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1600/2500]  eta: 0:13:56  lr: 0.000045  loss_ita: 5.3891  loss_itm: 0.6325  loss_lm: 4.5553  time: 0.9378  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1650/2500]  eta: 0:13:10  lr: 0.000045  loss_ita: 5.8572  loss_itm: 0.6407  loss_lm: 4.6760  time: 0.9198  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1700/2500]  eta: 0:12:23  lr: 0.000045  loss_ita: 6.2208  loss_itm: 0.6292  loss_lm: 4.2864  time: 0.9233  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1750/2500]  eta: 0:11:37  lr: 0.000045  loss_ita: 6.4253  loss_itm: 0.6383  loss_lm: 4.3046  time: 0.9352  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1800/2500]  eta: 0:10:50  lr: 0.000045  loss_ita: 6.1586  loss_itm: 0.6361  loss_lm: 4.5963  time: 0.9234  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1850/2500]  eta: 0:10:04  lr: 0.000045  loss_ita: 6.1716  loss_itm: 0.6387  loss_lm: 4.4412  time: 0.9251  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1900/2500]  eta: 0:09:17  lr: 0.000045  loss_ita: 6.0535  loss_itm: 0.6367  loss_lm: 4.3708  time: 0.9263  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1950/2500]  eta: 0:08:31  lr: 0.000045  loss_ita: 5.9256  loss_itm: 0.6286  loss_lm: 4.6511  time: 0.9289  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2000/2500]  eta: 0:07:44  lr: 0.000045  loss_ita: 5.7240  loss_itm: 0.6361  loss_lm: 4.3443  time: 0.9228  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2050/2500]  eta: 0:06:58  lr: 0.000045  loss_ita: 5.7294  loss_itm: 0.6406  loss_lm: 4.5127  time: 0.9255  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2100/2500]  eta: 0:06:11  lr: 0.000045  loss_ita: 5.7598  loss_itm: 0.6372  loss_lm: 5.1710  time: 0.9374  data: 0.0006  max mem: 7989\n",
            "Train Epoch: [18]  [2150/2500]  eta: 0:05:25  lr: 0.000045  loss_ita: 6.0122  loss_itm: 0.6364  loss_lm: 4.6644  time: 0.9268  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2200/2500]  eta: 0:04:38  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6409  loss_lm: 3.9354  time: 0.9197  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2250/2500]  eta: 0:03:52  lr: 0.000045  loss_ita: 6.3002  loss_itm: 0.6383  loss_lm: 4.6477  time: 0.9372  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2300/2500]  eta: 0:03:05  lr: 0.000045  loss_ita: 6.2824  loss_itm: 0.6529  loss_lm: 4.5341  time: 0.9280  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2350/2500]  eta: 0:02:19  lr: 0.000045  loss_ita: 6.1060  loss_itm: 0.6350  loss_lm: 4.9516  time: 0.9269  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2400/2500]  eta: 0:01:32  lr: 0.000045  loss_ita: 6.1870  loss_itm: 0.6261  loss_lm: 5.1146  time: 0.9306  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2450/2500]  eta: 0:00:46  lr: 0.000045  loss_ita: 6.0545  loss_itm: 0.6384  loss_lm: 4.0758  time: 0.9333  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2499/2500]  eta: 0:00:00  lr: 0.000045  loss_ita: 6.0094  loss_itm: 0.6447  loss_lm: 4.6467  time: 0.9193  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18] Total time: 0:38:43 (0.9293 s / it)\n",
            "Averaged stats: lr: 0.0000  loss_ita: 6.0553  loss_itm: 0.6362  loss_lm: 4.5673\n",
            "Train Epoch: [19]  [   0/2500]  eta: 1:33:40  lr: 0.000041  loss_ita: 6.0785  loss_itm: 0.6439  loss_lm: 3.9600  time: 2.2481  data: 1.0550  max mem: 7989\n",
            "Train Epoch: [19]  [  50/2500]  eta: 0:40:28  lr: 0.000041  loss_ita: 6.1363  loss_itm: 0.6342  loss_lm: 4.7595  time: 0.9812  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [19]  [ 100/2500]  eta: 0:38:56  lr: 0.000041  loss_ita: 6.1894  loss_itm: 0.6489  loss_lm: 4.6459  time: 0.9512  data: 0.0005  max mem: 7989\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}