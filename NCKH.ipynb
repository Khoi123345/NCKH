{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "179c8e88-60fe-4c1d-b5ff-42c41ed229e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "1ceb43ec-9d95-4135-a155-bd6db6069a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.8-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.40.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<0.119.0,>=0.100->cog) (4.12.1)\n",
            "Downloading cog-0.16.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.50.0\n",
            "    Uninstalling starlette-0.50.0:\n",
            "      Successfully uninstalled starlette-0.50.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.123.10\n",
            "    Uninstalling fastapi-0.123.10:\n",
            "      Successfully uninstalled fastapi-0.123.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "sse-starlette 3.1.2 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.8 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.6\n",
            "    Uninstalling transformers-4.57.6:\n",
            "      Successfully uninstalled transformers-4.57.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.0 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292934 sha256=a9cad83919b32f891f3385eb74b54e3e415e123e1de0658174de8beed8eb6e7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "TextVQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "c22cad15-1291-45f9-ec1e-4720e304a2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-24 13:34:06--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M  70.7MB/s    in 1.4s    \n",
            "\n",
            "2026-01-24 13:34:07 (70.7 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-01-24 13:34:07--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  65.4MB/s    in 0.2s    \n",
            "\n",
            "2026-01-24 13:34:08 (65.4 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-01-24 13:34:08--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2026-01-24 13:34:08 (145 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "abd1a926-4d1f-4734-fece-3efa563257e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-24 13:34:10--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G  74.0MB/s    in 83s     \n",
            "\n",
            "2026-01-24 13:35:33 (81.7 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (sửa lại config nếu cần)"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "6808529d-036d-4ae9-d859-712094d8f56d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 268kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 1.40MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.10MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 4.60MB/s]\n",
            "=> loading checkpoint './output/textVQA/checkpoint_02.pth'\n",
            "=> loaded checkpoint './output/textVQA/checkpoint_02.pth' (epoch 3)\n",
            "Start training\n",
            "Loss: 16.2299\n",
            "==========================================\n",
            "Train Epoch: [3]  [   0/8650]  eta: 10:51:34  lr: 0.000016  loss: 16.2299  time: 4.5196  data: 1.1476  max mem: 6922\n",
            "Loss: 18.6218\n",
            "==========================================\n",
            "Loss: 13.9565\n",
            "==========================================\n",
            "Loss: 18.4371\n",
            "==========================================\n",
            "Loss: 20.1925\n",
            "==========================================\n",
            "Loss: 15.5654\n",
            "==========================================\n",
            "Loss: 13.5641\n",
            "==========================================\n",
            "Loss: 20.253\n",
            "==========================================\n",
            "Loss: 20.3319\n",
            "==========================================\n",
            "Loss: 22.218\n",
            "==========================================\n",
            "Loss: 17.1128\n",
            "==========================================\n",
            "Loss: 20.2039\n",
            "==========================================\n",
            "Loss: 11.524\n",
            "==========================================\n",
            "Loss: 24.2836\n",
            "==========================================\n",
            "Loss: 13.2828\n",
            "==========================================\n",
            "Loss: 27.4007\n",
            "==========================================\n",
            "Loss: 15.5758\n",
            "==========================================\n",
            "Loss: 11.639\n",
            "==========================================\n",
            "Loss: 18.2189\n",
            "==========================================\n",
            "Loss: 28.9577\n",
            "==========================================\n",
            "Loss: 23.6381\n",
            "==========================================\n",
            "Loss: 15.0117\n",
            "==========================================\n",
            "Loss: 21.9603\n",
            "==========================================\n",
            "Loss: 19.4906\n",
            "==========================================\n",
            "Loss: 12.4355\n",
            "==========================================\n",
            "Loss: 12.1633\n",
            "==========================================\n",
            "Loss: 16.9698\n",
            "==========================================\n",
            "Loss: 19.3314\n",
            "==========================================\n",
            "Loss: 13.0361\n",
            "==========================================\n",
            "Loss: 19.5225\n",
            "==========================================\n",
            "Loss: 37.3539\n",
            "==========================================\n",
            "Loss: 29.8677\n",
            "==========================================\n",
            "Loss: 16.3869\n",
            "==========================================\n",
            "Loss: 19.4134\n",
            "==========================================\n",
            "Loss: 14.6571\n",
            "==========================================\n",
            "Loss: 12.9376\n",
            "==========================================\n",
            "Loss: 30.4968\n",
            "==========================================\n",
            "Loss: 25.5116\n",
            "==========================================\n",
            "Loss: 17.52\n",
            "==========================================\n",
            "Loss: 20.4768\n",
            "==========================================\n",
            "Loss: 14.0338\n",
            "==========================================\n",
            "Loss: 22.1287\n",
            "==========================================\n",
            "Loss: 14.8389\n",
            "==========================================\n",
            "Loss: 16.2507\n",
            "==========================================\n",
            "Loss: 9.9092\n",
            "==========================================\n",
            "Loss: 14.4196\n",
            "==========================================\n",
            "Loss: 16.1637\n",
            "==========================================\n",
            "Loss: 21.767\n",
            "==========================================\n",
            "Loss: 11.4817\n",
            "==========================================\n",
            "Loss: 26.2107\n",
            "==========================================\n",
            "Train Epoch: [3]  [  50/8650]  eta: 1:17:17  lr: 0.000016  loss: 8.6812  time: 0.4284  data: 0.0003  max mem: 6940\n",
            "Train Epoch: [3]  [ 100/8650]  eta: 1:11:57  lr: 0.000016  loss: 21.2288  time: 0.4287  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 150/8650]  eta: 1:09:18  lr: 0.000016  loss: 16.2347  time: 0.4044  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 200/8650]  eta: 1:08:15  lr: 0.000016  loss: 15.8890  time: 0.4113  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 250/8650]  eta: 1:07:26  lr: 0.000016  loss: 14.9564  time: 0.4077  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 300/8650]  eta: 1:06:48  lr: 0.000016  loss: 13.6023  time: 0.4035  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 350/8650]  eta: 1:06:12  lr: 0.000016  loss: 19.3422  time: 0.3976  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 400/8650]  eta: 1:05:40  lr: 0.000016  loss: 18.8347  time: 0.3966  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 450/8650]  eta: 1:05:08  lr: 0.000016  loss: 16.5936  time: 0.3914  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 500/8650]  eta: 1:04:41  lr: 0.000016  loss: 19.3528  time: 0.3924  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 550/8650]  eta: 1:04:15  lr: 0.000016  loss: 12.4793  time: 0.3914  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 600/8650]  eta: 1:03:49  lr: 0.000016  loss: 19.1254  time: 0.3909  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 650/8650]  eta: 1:03:27  lr: 0.000016  loss: 11.3167  time: 0.3970  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 700/8650]  eta: 1:03:01  lr: 0.000016  loss: 20.3372  time: 0.3914  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 750/8650]  eta: 1:02:35  lr: 0.000016  loss: 17.6553  time: 0.3880  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 800/8650]  eta: 1:02:13  lr: 0.000016  loss: 19.4113  time: 0.3967  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 850/8650]  eta: 1:01:53  lr: 0.000016  loss: 13.9304  time: 0.4225  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 900/8650]  eta: 1:01:28  lr: 0.000016  loss: 22.3051  time: 0.3946  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 950/8650]  eta: 1:01:04  lr: 0.000016  loss: 16.7860  time: 0.3901  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1000/8650]  eta: 1:00:39  lr: 0.000016  loss: 33.7511  time: 0.3906  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1050/8650]  eta: 1:00:15  lr: 0.000016  loss: 35.2451  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1100/8650]  eta: 0:59:50  lr: 0.000016  loss: 21.8075  time: 0.3877  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1150/8650]  eta: 0:59:26  lr: 0.000016  loss: 19.6575  time: 0.3956  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1200/8650]  eta: 0:59:01  lr: 0.000016  loss: 21.3204  time: 0.3951  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1250/8650]  eta: 0:58:34  lr: 0.000016  loss: 24.9751  time: 0.3918  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1300/8650]  eta: 0:58:11  lr: 0.000016  loss: 11.2171  time: 0.4082  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1350/8650]  eta: 0:57:45  lr: 0.000016  loss: 17.4524  time: 0.4125  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1400/8650]  eta: 0:57:22  lr: 0.000016  loss: 25.9491  time: 0.4113  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [1450/8650]  eta: 0:57:01  lr: 0.000016  loss: 13.3162  time: 0.4264  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [1500/8650]  eta: 0:56:39  lr: 0.000016  loss: 32.1247  time: 0.4302  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1550/8650]  eta: 0:56:17  lr: 0.000016  loss: 26.5587  time: 0.4504  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [1600/8650]  eta: 0:55:55  lr: 0.000016  loss: 22.5416  time: 0.4564  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [1650/8650]  eta: 0:55:32  lr: 0.000016  loss: 25.8644  time: 0.4704  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [1700/8650]  eta: 0:55:09  lr: 0.000016  loss: 18.7457  time: 0.4744  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [1750/8650]  eta: 0:54:47  lr: 0.000016  loss: 12.4631  time: 0.4871  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [1800/8650]  eta: 0:54:24  lr: 0.000016  loss: 11.8231  time: 0.4830  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [1850/8650]  eta: 0:53:59  lr: 0.000016  loss: 14.5130  time: 0.4803  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [1900/8650]  eta: 0:53:35  lr: 0.000016  loss: 21.6531  time: 0.5122  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [1950/8650]  eta: 0:53:12  lr: 0.000016  loss: 23.4871  time: 0.5177  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [2000/8650]  eta: 0:52:48  lr: 0.000016  loss: 19.4370  time: 0.5320  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [2050/8650]  eta: 0:52:24  lr: 0.000016  loss: 17.9298  time: 0.5397  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [2100/8650]  eta: 0:52:00  lr: 0.000016  loss: 16.2384  time: 0.5501  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [2150/8650]  eta: 0:51:36  lr: 0.000016  loss: 14.2620  time: 0.5683  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [2200/8650]  eta: 0:51:12  lr: 0.000016  loss: 14.8499  time: 0.5853  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [3]  [2250/8650]  eta: 0:50:47  lr: 0.000016  loss: 15.5788  time: 0.5678  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [2300/8650]  eta: 0:50:23  lr: 0.000016  loss: 20.4660  time: 0.5667  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [2350/8650]  eta: 0:49:58  lr: 0.000016  loss: 20.7681  time: 0.5502  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [2400/8650]  eta: 0:49:34  lr: 0.000016  loss: 16.1429  time: 0.5548  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [2450/8650]  eta: 0:49:10  lr: 0.000016  loss: 17.9507  time: 0.5540  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [2500/8650]  eta: 0:48:45  lr: 0.000016  loss: 12.6417  time: 0.5414  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [2550/8650]  eta: 0:48:21  lr: 0.000016  loss: 15.1586  time: 0.5462  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [3]  [2600/8650]  eta: 0:47:58  lr: 0.000016  loss: 21.5812  time: 0.5623  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [2650/8650]  eta: 0:47:32  lr: 0.000016  loss: 15.6012  time: 0.5234  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [2700/8650]  eta: 0:47:07  lr: 0.000016  loss: 12.4221  time: 0.5193  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [2750/8650]  eta: 0:46:43  lr: 0.000016  loss: 11.1941  time: 0.5238  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [2800/8650]  eta: 0:46:19  lr: 0.000016  loss: 19.3219  time: 0.5054  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [2850/8650]  eta: 0:45:55  lr: 0.000016  loss: 15.3432  time: 0.4884  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [2900/8650]  eta: 0:45:31  lr: 0.000016  loss: 13.4613  time: 0.4893  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [2950/8650]  eta: 0:45:08  lr: 0.000016  loss: 34.9462  time: 0.4866  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [3000/8650]  eta: 0:44:43  lr: 0.000016  loss: 21.4291  time: 0.4761  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [3050/8650]  eta: 0:44:20  lr: 0.000016  loss: 17.0113  time: 0.4842  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [3100/8650]  eta: 0:43:56  lr: 0.000016  loss: 10.5778  time: 0.4773  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [3150/8650]  eta: 0:43:31  lr: 0.000016  loss: 9.4648  time: 0.4567  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3200/8650]  eta: 0:43:06  lr: 0.000016  loss: 19.9143  time: 0.4454  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [3250/8650]  eta: 0:42:42  lr: 0.000016  loss: 21.5970  time: 0.4282  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [3300/8650]  eta: 0:42:17  lr: 0.000016  loss: 12.0731  time: 0.4077  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [3350/8650]  eta: 0:41:53  lr: 0.000016  loss: 14.5056  time: 0.4052  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3400/8650]  eta: 0:41:29  lr: 0.000016  loss: 19.4904  time: 0.3999  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3450/8650]  eta: 0:41:05  lr: 0.000016  loss: 18.6495  time: 0.3996  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3500/8650]  eta: 0:40:42  lr: 0.000016  loss: 11.0806  time: 0.4156  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3550/8650]  eta: 0:40:18  lr: 0.000016  loss: 12.4424  time: 0.3916  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3600/8650]  eta: 0:39:53  lr: 0.000016  loss: 27.0073  time: 0.3965  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3650/8650]  eta: 0:39:29  lr: 0.000016  loss: 26.2676  time: 0.3863  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3700/8650]  eta: 0:39:06  lr: 0.000016  loss: 39.5504  time: 0.3942  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3750/8650]  eta: 0:38:42  lr: 0.000016  loss: 14.9116  time: 0.3882  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3800/8650]  eta: 0:38:17  lr: 0.000016  loss: 24.5967  time: 0.3868  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3850/8650]  eta: 0:37:54  lr: 0.000016  loss: 10.4623  time: 0.3871  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3900/8650]  eta: 0:37:30  lr: 0.000016  loss: 19.5415  time: 0.3967  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3950/8650]  eta: 0:37:06  lr: 0.000016  loss: 16.4925  time: 0.3940  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4000/8650]  eta: 0:36:42  lr: 0.000016  loss: 15.7578  time: 0.4089  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4050/8650]  eta: 0:36:18  lr: 0.000016  loss: 20.3449  time: 0.4200  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4100/8650]  eta: 0:35:54  lr: 0.000016  loss: 21.8129  time: 0.4228  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4150/8650]  eta: 0:35:30  lr: 0.000016  loss: 24.1718  time: 0.4366  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [4200/8650]  eta: 0:35:07  lr: 0.000016  loss: 9.5378  time: 0.4438  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4250/8650]  eta: 0:34:43  lr: 0.000016  loss: 14.4472  time: 0.4428  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4300/8650]  eta: 0:34:19  lr: 0.000016  loss: 19.3493  time: 0.4536  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4350/8650]  eta: 0:33:55  lr: 0.000016  loss: 18.8424  time: 0.4564  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4400/8650]  eta: 0:33:32  lr: 0.000016  loss: 17.4308  time: 0.4526  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [4450/8650]  eta: 0:33:08  lr: 0.000016  loss: 23.0207  time: 0.4759  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [4500/8650]  eta: 0:32:44  lr: 0.000016  loss: 14.4569  time: 0.4763  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [4550/8650]  eta: 0:32:20  lr: 0.000016  loss: 23.8006  time: 0.4843  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [4600/8650]  eta: 0:31:57  lr: 0.000016  loss: 15.2933  time: 0.5004  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [4650/8650]  eta: 0:31:33  lr: 0.000016  loss: 21.5002  time: 0.5014  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [4700/8650]  eta: 0:31:09  lr: 0.000016  loss: 12.4310  time: 0.5141  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [4750/8650]  eta: 0:30:45  lr: 0.000016  loss: 20.5756  time: 0.5293  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [4800/8650]  eta: 0:30:22  lr: 0.000016  loss: 16.7247  time: 0.5328  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [4850/8650]  eta: 0:29:58  lr: 0.000016  loss: 16.6659  time: 0.5223  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [4900/8650]  eta: 0:29:34  lr: 0.000016  loss: 14.7255  time: 0.5378  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [4950/8650]  eta: 0:29:10  lr: 0.000016  loss: 22.2463  time: 0.5627  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [5000/8650]  eta: 0:28:46  lr: 0.000016  loss: 17.0798  time: 0.5643  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [5050/8650]  eta: 0:28:22  lr: 0.000016  loss: 17.1358  time: 0.5752  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [5100/8650]  eta: 0:27:59  lr: 0.000016  loss: 24.6182  time: 0.5600  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [3]  [5150/8650]  eta: 0:27:35  lr: 0.000016  loss: 12.8063  time: 0.5728  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [5200/8650]  eta: 0:27:11  lr: 0.000016  loss: 24.3610  time: 0.5558  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [5250/8650]  eta: 0:26:47  lr: 0.000016  loss: 20.5932  time: 0.5612  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [5300/8650]  eta: 0:26:24  lr: 0.000016  loss: 18.0839  time: 0.5532  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [5350/8650]  eta: 0:26:00  lr: 0.000016  loss: 17.1053  time: 0.5371  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [5400/8650]  eta: 0:25:36  lr: 0.000016  loss: 16.3361  time: 0.5254  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [5450/8650]  eta: 0:25:13  lr: 0.000016  loss: 21.8666  time: 0.5346  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [5500/8650]  eta: 0:24:49  lr: 0.000016  loss: 18.6757  time: 0.5131  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [5550/8650]  eta: 0:24:25  lr: 0.000016  loss: 15.7250  time: 0.4972  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [5600/8650]  eta: 0:24:02  lr: 0.000016  loss: 16.6075  time: 0.4934  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [5650/8650]  eta: 0:23:38  lr: 0.000016  loss: 14.2098  time: 0.4608  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [5700/8650]  eta: 0:23:14  lr: 0.000016  loss: 21.1186  time: 0.4512  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [5750/8650]  eta: 0:22:50  lr: 0.000016  loss: 23.2508  time: 0.4436  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5800/8650]  eta: 0:22:26  lr: 0.000016  loss: 13.5107  time: 0.4305  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5850/8650]  eta: 0:22:03  lr: 0.000016  loss: 24.1077  time: 0.4263  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5900/8650]  eta: 0:21:39  lr: 0.000016  loss: 17.5049  time: 0.4140  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5950/8650]  eta: 0:21:15  lr: 0.000016  loss: 14.8868  time: 0.4141  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6000/8650]  eta: 0:20:52  lr: 0.000016  loss: 11.8704  time: 0.4081  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6050/8650]  eta: 0:20:28  lr: 0.000016  loss: 16.6219  time: 0.4074  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6100/8650]  eta: 0:20:04  lr: 0.000016  loss: 37.2442  time: 0.3977  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6150/8650]  eta: 0:19:41  lr: 0.000016  loss: 17.8121  time: 0.3896  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6200/8650]  eta: 0:19:17  lr: 0.000016  loss: 30.3013  time: 0.3937  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6250/8650]  eta: 0:18:54  lr: 0.000016  loss: 27.0284  time: 0.3915  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6300/8650]  eta: 0:18:30  lr: 0.000016  loss: 21.0515  time: 0.3866  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6350/8650]  eta: 0:18:06  lr: 0.000016  loss: 21.8472  time: 0.3941  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6400/8650]  eta: 0:17:43  lr: 0.000016  loss: 25.8886  time: 0.3878  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6450/8650]  eta: 0:17:19  lr: 0.000016  loss: 24.6622  time: 0.3900  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6500/8650]  eta: 0:16:55  lr: 0.000016  loss: 15.7545  time: 0.3920  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6550/8650]  eta: 0:16:32  lr: 0.000016  loss: 19.6844  time: 0.4009  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6600/8650]  eta: 0:16:08  lr: 0.000016  loss: 23.9748  time: 0.4225  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6650/8650]  eta: 0:15:45  lr: 0.000016  loss: 11.8841  time: 0.4181  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6700/8650]  eta: 0:15:21  lr: 0.000016  loss: 20.2175  time: 0.4308  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6750/8650]  eta: 0:14:58  lr: 0.000016  loss: 9.7258  time: 0.4392  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6800/8650]  eta: 0:14:34  lr: 0.000016  loss: 15.7469  time: 0.4459  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6850/8650]  eta: 0:14:10  lr: 0.000016  loss: 17.4757  time: 0.4499  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [6900/8650]  eta: 0:13:47  lr: 0.000016  loss: 11.8232  time: 0.4657  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [6950/8650]  eta: 0:13:23  lr: 0.000016  loss: 18.0871  time: 0.4689  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [7000/8650]  eta: 0:13:00  lr: 0.000016  loss: 21.5064  time: 0.4765  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [7050/8650]  eta: 0:12:36  lr: 0.000016  loss: 19.8325  time: 0.4836  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [7100/8650]  eta: 0:12:12  lr: 0.000016  loss: 11.1962  time: 0.4757  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [7150/8650]  eta: 0:11:49  lr: 0.000016  loss: 14.3090  time: 0.4887  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [7200/8650]  eta: 0:11:25  lr: 0.000016  loss: 27.3192  time: 0.5007  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [7250/8650]  eta: 0:11:01  lr: 0.000016  loss: 33.3425  time: 0.5131  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7300/8650]  eta: 0:10:38  lr: 0.000016  loss: 17.6640  time: 0.5163  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7350/8650]  eta: 0:10:14  lr: 0.000016  loss: 25.7286  time: 0.5214  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [7400/8650]  eta: 0:09:50  lr: 0.000016  loss: 14.9586  time: 0.5363  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [3]  [7450/8650]  eta: 0:09:27  lr: 0.000016  loss: 18.8944  time: 0.5472  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [7500/8650]  eta: 0:09:03  lr: 0.000016  loss: 20.2535  time: 0.5502  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [7550/8650]  eta: 0:08:40  lr: 0.000016  loss: 14.5321  time: 0.5603  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7600/8650]  eta: 0:08:16  lr: 0.000016  loss: 16.1981  time: 0.5624  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7650/8650]  eta: 0:07:52  lr: 0.000016  loss: 18.3552  time: 0.5656  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7700/8650]  eta: 0:07:29  lr: 0.000016  loss: 16.0624  time: 0.5694  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [7750/8650]  eta: 0:07:05  lr: 0.000016  loss: 13.8151  time: 0.5741  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [7800/8650]  eta: 0:06:41  lr: 0.000016  loss: 17.0386  time: 0.5514  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7850/8650]  eta: 0:06:18  lr: 0.000016  loss: 19.9492  time: 0.5715  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7900/8650]  eta: 0:05:54  lr: 0.000016  loss: 24.8984  time: 0.5537  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [7950/8650]  eta: 0:05:30  lr: 0.000016  loss: 25.1599  time: 0.5534  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [8000/8650]  eta: 0:05:07  lr: 0.000016  loss: 23.5720  time: 0.5415  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [8050/8650]  eta: 0:04:43  lr: 0.000016  loss: 16.9662  time: 0.5456  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [3]  [8100/8650]  eta: 0:04:19  lr: 0.000016  loss: 20.5851  time: 0.5450  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [3]  [8150/8650]  eta: 0:03:56  lr: 0.000016  loss: 26.9238  time: 0.5395  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [3]  [8200/8650]  eta: 0:03:32  lr: 0.000016  loss: 18.6713  time: 0.5358  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [3]  [8250/8650]  eta: 0:03:09  lr: 0.000016  loss: 13.1894  time: 0.5187  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [8300/8650]  eta: 0:02:45  lr: 0.000016  loss: 18.6513  time: 0.4923  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [8350/8650]  eta: 0:02:21  lr: 0.000016  loss: 15.8847  time: 0.4943  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [8400/8650]  eta: 0:01:58  lr: 0.000016  loss: 31.1172  time: 0.4811  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [8450/8650]  eta: 0:01:34  lr: 0.000016  loss: 13.5181  time: 0.4594  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [8500/8650]  eta: 0:01:10  lr: 0.000016  loss: 18.2312  time: 0.4626  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [8550/8650]  eta: 0:00:47  lr: 0.000016  loss: 30.4343  time: 0.4440  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [8600/8650]  eta: 0:00:23  lr: 0.000016  loss: 22.1502  time: 0.4435  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [8649/8650]  eta: 0:00:00  lr: 0.000016  loss: 13.8814  time: 0.3937  data: 0.0020  max mem: 9086\n",
            "Train Epoch: [3] Total time: 1:08:06 (0.4725 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 18.7291\n",
            "Loss: 18.0856\n",
            "==========================================\n",
            "Train Epoch: [4]  [   0/8650]  eta: 3:09:56  lr: 0.000013  loss: 18.0856  time: 1.3175  data: 0.6754  max mem: 9086\n",
            "Loss: 25.3769\n",
            "==========================================\n",
            "Loss: 20.9434\n",
            "==========================================\n",
            "Loss: 24.836\n",
            "==========================================\n",
            "Loss: 14.3441\n",
            "==========================================\n",
            "Loss: 11.5001\n",
            "==========================================\n",
            "Loss: 22.1854\n",
            "==========================================\n",
            "Loss: 9.8885\n",
            "==========================================\n",
            "Loss: 10.831\n",
            "==========================================\n",
            "Loss: 17.957\n",
            "==========================================\n",
            "Loss: 18.5066\n",
            "==========================================\n",
            "Loss: 10.2308\n",
            "==========================================\n",
            "Loss: 17.7934\n",
            "==========================================\n",
            "Loss: 19.5342\n",
            "==========================================\n",
            "Loss: 11.74\n",
            "==========================================\n",
            "Loss: 13.6117\n",
            "==========================================\n",
            "Loss: 22.5875\n",
            "==========================================\n",
            "Loss: 15.7544\n",
            "==========================================\n",
            "Loss: 14.0993\n",
            "==========================================\n",
            "Loss: 26.2396\n",
            "==========================================\n",
            "Loss: 14.6561\n",
            "==========================================\n",
            "Loss: 14.0764\n",
            "==========================================\n",
            "Loss: 15.7292\n",
            "==========================================\n",
            "Loss: 21.9796\n",
            "==========================================\n",
            "Loss: 21.8541\n",
            "==========================================\n",
            "Loss: 15.0458\n",
            "==========================================\n",
            "Loss: 25.7465\n",
            "==========================================\n",
            "Loss: 20.5757\n",
            "==========================================\n",
            "Loss: 16.4161\n",
            "==========================================\n",
            "Loss: 14.0058\n",
            "==========================================\n",
            "Loss: 17.9258\n",
            "==========================================\n",
            "Loss: 33.4151\n",
            "==========================================\n",
            "Loss: 13.6806\n",
            "==========================================\n",
            "Loss: 13.3553\n",
            "==========================================\n",
            "Loss: 11.9635\n",
            "==========================================\n",
            "Loss: 16.8964\n",
            "==========================================\n",
            "Loss: 17.0336\n",
            "==========================================\n",
            "Loss: 14.2033\n",
            "==========================================\n",
            "Loss: 22.7869\n",
            "==========================================\n",
            "Loss: 18.1982\n",
            "==========================================\n",
            "Loss: 16.8813\n",
            "==========================================\n",
            "Loss: 12.9745\n",
            "==========================================\n",
            "Loss: 20.347\n",
            "==========================================\n",
            "Loss: 26.7278\n",
            "==========================================\n",
            "Loss: 12.0018\n",
            "==========================================\n",
            "Loss: 20.5743\n",
            "==========================================\n",
            "Loss: 19.3771\n",
            "==========================================\n",
            "Loss: 18.389\n",
            "==========================================\n",
            "Loss: 10.0506\n",
            "==========================================\n",
            "Loss: 18.7098\n",
            "==========================================\n",
            "Train Epoch: [4]  [  50/8650]  eta: 1:23:44  lr: 0.000013  loss: 33.6449  time: 0.6697  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 100/8650]  eta: 1:15:48  lr: 0.000013  loss: 23.6381  time: 0.4920  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [ 150/8650]  eta: 1:12:34  lr: 0.000013  loss: 13.5782  time: 0.5052  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [ 200/8650]  eta: 1:10:44  lr: 0.000013  loss: 16.7535  time: 0.5116  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 250/8650]  eta: 1:09:33  lr: 0.000013  loss: 18.3992  time: 0.5118  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 300/8650]  eta: 1:08:40  lr: 0.000013  loss: 20.1962  time: 0.5191  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 350/8650]  eta: 1:07:50  lr: 0.000013  loss: 13.2454  time: 0.5325  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 400/8650]  eta: 1:07:06  lr: 0.000013  loss: 8.8148  time: 0.5352  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 450/8650]  eta: 1:06:26  lr: 0.000013  loss: 18.6606  time: 0.5320  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 500/8650]  eta: 1:05:47  lr: 0.000013  loss: 15.2610  time: 0.5487  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [ 550/8650]  eta: 1:05:15  lr: 0.000013  loss: 15.4543  time: 0.5662  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 600/8650]  eta: 1:04:41  lr: 0.000013  loss: 8.9766  time: 0.5712  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 650/8650]  eta: 1:04:09  lr: 0.000013  loss: 19.1806  time: 0.5732  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [ 700/8650]  eta: 1:03:41  lr: 0.000013  loss: 19.0205  time: 0.5775  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 750/8650]  eta: 1:03:13  lr: 0.000013  loss: 16.9956  time: 0.5710  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [ 800/8650]  eta: 1:02:43  lr: 0.000013  loss: 18.4655  time: 0.5535  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [ 850/8650]  eta: 1:02:15  lr: 0.000013  loss: 20.5412  time: 0.5521  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [ 900/8650]  eta: 1:01:46  lr: 0.000013  loss: 15.3914  time: 0.5493  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [ 950/8650]  eta: 1:01:19  lr: 0.000013  loss: 17.5973  time: 0.5424  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [1000/8650]  eta: 1:00:54  lr: 0.000013  loss: 21.5809  time: 0.5394  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [1050/8650]  eta: 1:00:27  lr: 0.000013  loss: 12.0195  time: 0.5327  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [1100/8650]  eta: 1:00:00  lr: 0.000013  loss: 15.2301  time: 0.5248  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [1150/8650]  eta: 0:59:39  lr: 0.000013  loss: 13.2471  time: 0.5404  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [4]  [1200/8650]  eta: 0:59:13  lr: 0.000013  loss: 9.0893  time: 0.5349  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [1250/8650]  eta: 0:58:46  lr: 0.000013  loss: 18.3402  time: 0.5205  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [1300/8650]  eta: 0:58:18  lr: 0.000013  loss: 27.8902  time: 0.4939  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [1350/8650]  eta: 0:57:53  lr: 0.000013  loss: 19.2419  time: 0.4942  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [1400/8650]  eta: 0:57:27  lr: 0.000013  loss: 26.6140  time: 0.4853  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [1450/8650]  eta: 0:57:00  lr: 0.000013  loss: 17.8000  time: 0.4687  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [1500/8650]  eta: 0:56:36  lr: 0.000013  loss: 25.6129  time: 0.4617  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [4]  [1550/8650]  eta: 0:56:10  lr: 0.000013  loss: 12.8352  time: 0.4488  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [1600/8650]  eta: 0:55:46  lr: 0.000013  loss: 18.6715  time: 0.4398  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [1650/8650]  eta: 0:55:22  lr: 0.000013  loss: 15.7580  time: 0.4408  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [1700/8650]  eta: 0:54:59  lr: 0.000013  loss: 21.8057  time: 0.4460  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [1750/8650]  eta: 0:54:31  lr: 0.000013  loss: 18.6131  time: 0.4174  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [1800/8650]  eta: 0:54:07  lr: 0.000013  loss: 16.4169  time: 0.4088  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [1850/8650]  eta: 0:53:42  lr: 0.000013  loss: 24.3021  time: 0.4025  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [1900/8650]  eta: 0:53:18  lr: 0.000013  loss: 10.0096  time: 0.4069  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [1950/8650]  eta: 0:52:54  lr: 0.000013  loss: 14.7969  time: 0.4050  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [2000/8650]  eta: 0:52:30  lr: 0.000013  loss: 21.6156  time: 0.3927  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2050/8650]  eta: 0:52:07  lr: 0.000013  loss: 27.9437  time: 0.3990  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2100/8650]  eta: 0:51:44  lr: 0.000013  loss: 17.2317  time: 0.4022  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2150/8650]  eta: 0:51:20  lr: 0.000013  loss: 26.8255  time: 0.3953  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [2200/8650]  eta: 0:50:57  lr: 0.000013  loss: 16.1301  time: 0.3943  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2250/8650]  eta: 0:50:33  lr: 0.000013  loss: 16.0089  time: 0.3910  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2300/8650]  eta: 0:50:09  lr: 0.000013  loss: 19.5794  time: 0.3939  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2350/8650]  eta: 0:49:45  lr: 0.000013  loss: 18.1247  time: 0.3892  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2400/8650]  eta: 0:49:21  lr: 0.000013  loss: 32.9012  time: 0.3916  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2450/8650]  eta: 0:48:57  lr: 0.000013  loss: 20.7229  time: 0.3947  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2500/8650]  eta: 0:48:33  lr: 0.000013  loss: 29.4506  time: 0.3967  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2550/8650]  eta: 0:48:09  lr: 0.000013  loss: 11.5318  time: 0.3855  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2600/8650]  eta: 0:47:46  lr: 0.000013  loss: 18.1471  time: 0.3942  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2650/8650]  eta: 0:47:22  lr: 0.000013  loss: 21.7024  time: 0.3895  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2700/8650]  eta: 0:46:58  lr: 0.000013  loss: 17.7100  time: 0.3890  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2750/8650]  eta: 0:46:35  lr: 0.000013  loss: 20.1797  time: 0.3868  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [2800/8650]  eta: 0:46:11  lr: 0.000013  loss: 13.9157  time: 0.4032  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [2850/8650]  eta: 0:45:47  lr: 0.000013  loss: 22.7857  time: 0.4111  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [2900/8650]  eta: 0:45:23  lr: 0.000013  loss: 14.9407  time: 0.4191  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [2950/8650]  eta: 0:45:00  lr: 0.000013  loss: 21.7888  time: 0.4178  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [3000/8650]  eta: 0:44:37  lr: 0.000013  loss: 17.7904  time: 0.4184  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [3050/8650]  eta: 0:44:13  lr: 0.000013  loss: 15.5286  time: 0.4303  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [3100/8650]  eta: 0:43:49  lr: 0.000013  loss: 16.5800  time: 0.4410  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [3150/8650]  eta: 0:43:26  lr: 0.000013  loss: 22.8950  time: 0.4640  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [3200/8650]  eta: 0:43:02  lr: 0.000013  loss: 21.8820  time: 0.4692  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [3250/8650]  eta: 0:42:38  lr: 0.000013  loss: 24.9860  time: 0.4954  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [3300/8650]  eta: 0:42:14  lr: 0.000013  loss: 14.5078  time: 0.4987  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [3350/8650]  eta: 0:41:51  lr: 0.000013  loss: 32.7220  time: 0.5288  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [3400/8650]  eta: 0:41:27  lr: 0.000013  loss: 10.9502  time: 0.5304  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [3450/8650]  eta: 0:41:03  lr: 0.000013  loss: 20.8383  time: 0.5278  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [3500/8650]  eta: 0:40:39  lr: 0.000013  loss: 20.3802  time: 0.5296  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [3550/8650]  eta: 0:40:16  lr: 0.000013  loss: 14.5171  time: 0.5458  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [3600/8650]  eta: 0:39:52  lr: 0.000013  loss: 20.8209  time: 0.5572  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [3650/8650]  eta: 0:39:28  lr: 0.000013  loss: 16.8566  time: 0.5679  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [3700/8650]  eta: 0:39:04  lr: 0.000013  loss: 17.6105  time: 0.5648  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [3750/8650]  eta: 0:38:41  lr: 0.000013  loss: 14.1500  time: 0.5753  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [3800/8650]  eta: 0:38:17  lr: 0.000013  loss: 19.2294  time: 0.5698  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [3850/8650]  eta: 0:37:53  lr: 0.000013  loss: 30.9802  time: 0.5670  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [3900/8650]  eta: 0:37:29  lr: 0.000013  loss: 24.8935  time: 0.5713  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [4]  [3950/8650]  eta: 0:37:05  lr: 0.000013  loss: 15.0676  time: 0.5521  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [4000/8650]  eta: 0:36:41  lr: 0.000013  loss: 18.1824  time: 0.5526  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [4050/8650]  eta: 0:36:17  lr: 0.000013  loss: 12.8838  time: 0.5588  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [4100/8650]  eta: 0:35:53  lr: 0.000013  loss: 17.9406  time: 0.5264  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [4150/8650]  eta: 0:35:29  lr: 0.000013  loss: 21.7760  time: 0.5218  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [4200/8650]  eta: 0:35:05  lr: 0.000013  loss: 20.4869  time: 0.5016  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [4250/8650]  eta: 0:34:41  lr: 0.000013  loss: 11.4929  time: 0.4856  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [4300/8650]  eta: 0:34:17  lr: 0.000013  loss: 28.0382  time: 0.4788  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [4350/8650]  eta: 0:33:53  lr: 0.000013  loss: 12.8195  time: 0.4624  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [4400/8650]  eta: 0:33:29  lr: 0.000013  loss: 23.8468  time: 0.4549  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [4450/8650]  eta: 0:33:05  lr: 0.000013  loss: 16.5577  time: 0.4393  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [4500/8650]  eta: 0:32:41  lr: 0.000013  loss: 21.8441  time: 0.4273  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [4550/8650]  eta: 0:32:17  lr: 0.000013  loss: 14.7751  time: 0.4243  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [4600/8650]  eta: 0:31:54  lr: 0.000013  loss: 28.7331  time: 0.4183  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [4650/8650]  eta: 0:31:30  lr: 0.000013  loss: 12.5726  time: 0.4213  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [4700/8650]  eta: 0:31:07  lr: 0.000013  loss: 25.6076  time: 0.4121  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [4750/8650]  eta: 0:30:43  lr: 0.000013  loss: 25.9701  time: 0.4130  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [4800/8650]  eta: 0:30:19  lr: 0.000013  loss: 16.9984  time: 0.4022  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [4850/8650]  eta: 0:29:56  lr: 0.000013  loss: 19.1326  time: 0.3947  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [4900/8650]  eta: 0:29:32  lr: 0.000013  loss: 24.7013  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [4950/8650]  eta: 0:29:09  lr: 0.000013  loss: 16.6396  time: 0.3940  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [5000/8650]  eta: 0:28:45  lr: 0.000013  loss: 12.1439  time: 0.3951  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [5050/8650]  eta: 0:28:22  lr: 0.000013  loss: 13.4350  time: 0.3916  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [5100/8650]  eta: 0:27:58  lr: 0.000013  loss: 18.5818  time: 0.3906  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [5150/8650]  eta: 0:27:35  lr: 0.000013  loss: 41.5568  time: 0.3896  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [5200/8650]  eta: 0:27:11  lr: 0.000013  loss: 13.0131  time: 0.3884  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [5250/8650]  eta: 0:26:47  lr: 0.000013  loss: 19.3562  time: 0.4164  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [5300/8650]  eta: 0:26:24  lr: 0.000013  loss: 15.5802  time: 0.4236  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [5350/8650]  eta: 0:26:00  lr: 0.000013  loss: 17.7862  time: 0.4476  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [5400/8650]  eta: 0:25:36  lr: 0.000013  loss: 14.1896  time: 0.4582  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [5450/8650]  eta: 0:25:13  lr: 0.000013  loss: 18.2829  time: 0.4772  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [5500/8650]  eta: 0:24:49  lr: 0.000013  loss: 24.5723  time: 0.4772  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [5550/8650]  eta: 0:24:25  lr: 0.000013  loss: 19.8145  time: 0.5019  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [5600/8650]  eta: 0:24:02  lr: 0.000013  loss: 15.3954  time: 0.5266  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [5650/8650]  eta: 0:23:38  lr: 0.000013  loss: 14.8130  time: 0.5517  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [4]  [5700/8650]  eta: 0:23:15  lr: 0.000013  loss: 13.4612  time: 0.5675  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [4]  [5750/8650]  eta: 0:22:51  lr: 0.000013  loss: 14.0091  time: 0.5773  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [5800/8650]  eta: 0:22:27  lr: 0.000013  loss: 19.9239  time: 0.5584  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [5850/8650]  eta: 0:22:04  lr: 0.000013  loss: 14.2796  time: 0.5761  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [5900/8650]  eta: 0:21:40  lr: 0.000013  loss: 13.3102  time: 0.5489  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [5950/8650]  eta: 0:21:16  lr: 0.000013  loss: 18.9353  time: 0.5397  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [4]  [6000/8650]  eta: 0:20:52  lr: 0.000013  loss: 19.1542  time: 0.5412  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [6050/8650]  eta: 0:20:29  lr: 0.000013  loss: 15.8392  time: 0.5493  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [6100/8650]  eta: 0:20:05  lr: 0.000013  loss: 18.5667  time: 0.5352  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [6150/8650]  eta: 0:19:41  lr: 0.000013  loss: 16.5444  time: 0.5077  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [6200/8650]  eta: 0:19:17  lr: 0.000013  loss: 14.2412  time: 0.4901  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [4]  [6250/8650]  eta: 0:18:53  lr: 0.000013  loss: 16.3726  time: 0.4646  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [6300/8650]  eta: 0:18:30  lr: 0.000013  loss: 30.1127  time: 0.4615  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [4]  [6350/8650]  eta: 0:18:06  lr: 0.000013  loss: 16.0389  time: 0.4662  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [6400/8650]  eta: 0:17:42  lr: 0.000013  loss: 20.5591  time: 0.4382  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [6450/8650]  eta: 0:17:18  lr: 0.000013  loss: 12.7874  time: 0.4371  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [6500/8650]  eta: 0:16:55  lr: 0.000013  loss: 20.5463  time: 0.4351  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [6550/8650]  eta: 0:16:31  lr: 0.000013  loss: 13.6065  time: 0.4474  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [6600/8650]  eta: 0:16:07  lr: 0.000013  loss: 22.9715  time: 0.4277  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [6650/8650]  eta: 0:15:44  lr: 0.000013  loss: 13.6323  time: 0.4066  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [4]  [6700/8650]  eta: 0:15:20  lr: 0.000013  loss: 19.8756  time: 0.4013  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [6750/8650]  eta: 0:14:56  lr: 0.000013  loss: 12.8496  time: 0.3908  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [6800/8650]  eta: 0:14:32  lr: 0.000013  loss: 15.2136  time: 0.3890  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [6850/8650]  eta: 0:14:08  lr: 0.000013  loss: 15.3801  time: 0.3946  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [6900/8650]  eta: 0:13:45  lr: 0.000013  loss: 18.6548  time: 0.3911  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [6950/8650]  eta: 0:13:21  lr: 0.000013  loss: 17.4625  time: 0.3842  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7000/8650]  eta: 0:12:58  lr: 0.000013  loss: 11.6708  time: 0.3871  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7050/8650]  eta: 0:12:34  lr: 0.000013  loss: 19.8083  time: 0.3862  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7100/8650]  eta: 0:12:10  lr: 0.000013  loss: 12.0871  time: 0.3878  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7150/8650]  eta: 0:11:47  lr: 0.000013  loss: 21.7444  time: 0.3880  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7200/8650]  eta: 0:11:23  lr: 0.000013  loss: 10.7195  time: 0.3851  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7250/8650]  eta: 0:11:00  lr: 0.000013  loss: 23.4894  time: 0.3831  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7300/8650]  eta: 0:10:36  lr: 0.000013  loss: 20.0738  time: 0.3868  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7350/8650]  eta: 0:10:12  lr: 0.000013  loss: 16.1329  time: 0.3856  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [4]  [7400/8650]  eta: 0:09:49  lr: 0.000013  loss: 15.8264  time: 0.3928  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [7450/8650]  eta: 0:09:25  lr: 0.000013  loss: 15.1279  time: 0.3979  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [7500/8650]  eta: 0:09:01  lr: 0.000013  loss: 28.6727  time: 0.4083  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [4]  [7550/8650]  eta: 0:08:38  lr: 0.000013  loss: 14.7582  time: 0.4232  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [7600/8650]  eta: 0:08:14  lr: 0.000013  loss: 16.0526  time: 0.4448  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [7650/8650]  eta: 0:07:51  lr: 0.000013  loss: 20.9374  time: 0.4499  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [7700/8650]  eta: 0:07:27  lr: 0.000013  loss: 17.1911  time: 0.4618  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [7750/8650]  eta: 0:07:03  lr: 0.000013  loss: 24.4663  time: 0.4700  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [7800/8650]  eta: 0:06:40  lr: 0.000013  loss: 18.1080  time: 0.4847  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [7850/8650]  eta: 0:06:16  lr: 0.000013  loss: 15.1610  time: 0.4965  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [7900/8650]  eta: 0:05:53  lr: 0.000013  loss: 13.6104  time: 0.5051  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [7950/8650]  eta: 0:05:29  lr: 0.000013  loss: 22.7408  time: 0.5291  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [8000/8650]  eta: 0:05:06  lr: 0.000013  loss: 19.4825  time: 0.5270  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [8050/8650]  eta: 0:04:42  lr: 0.000013  loss: 21.7689  time: 0.5249  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [8100/8650]  eta: 0:04:18  lr: 0.000013  loss: 13.8403  time: 0.5327  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [8150/8650]  eta: 0:03:55  lr: 0.000013  loss: 19.4660  time: 0.5510  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [8200/8650]  eta: 0:03:31  lr: 0.000013  loss: 13.1337  time: 0.5527  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [8250/8650]  eta: 0:03:08  lr: 0.000013  loss: 15.9321  time: 0.5494  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [8300/8650]  eta: 0:02:44  lr: 0.000013  loss: 21.6660  time: 0.5516  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [8350/8650]  eta: 0:02:21  lr: 0.000013  loss: 21.9299  time: 0.5639  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [4]  [8400/8650]  eta: 0:01:57  lr: 0.000013  loss: 11.2355  time: 0.5548  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [8450/8650]  eta: 0:01:34  lr: 0.000013  loss: 11.8950  time: 0.5740  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [4]  [8500/8650]  eta: 0:01:10  lr: 0.000013  loss: 18.5207  time: 0.5629  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [4]  [8550/8650]  eta: 0:00:47  lr: 0.000013  loss: 14.8888  time: 0.5285  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4]  [8600/8650]  eta: 0:00:23  lr: 0.000013  loss: 9.9676  time: 0.5276  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [4]  [8649/8650]  eta: 0:00:00  lr: 0.000013  loss: 12.5014  time: 0.4782  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [4] Total time: 1:07:49 (0.4705 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 18.1159\n",
            "Loss: 16.4686\n",
            "==========================================\n",
            "Train Epoch: [5]  [   0/8650]  eta: 3:12:15  lr: 0.000010  loss: 16.4686  time: 1.3336  data: 0.6751  max mem: 9086\n",
            "Loss: 13.7819\n",
            "==========================================\n",
            "Loss: 16.1183\n",
            "==========================================\n",
            "Loss: 11.6584\n",
            "==========================================\n",
            "Loss: 25.5863\n",
            "==========================================\n",
            "Loss: 20.8512\n",
            "==========================================\n",
            "Loss: 11.585\n",
            "==========================================\n",
            "Loss: 16.2521\n",
            "==========================================\n",
            "Loss: 23.6169\n",
            "==========================================\n",
            "Loss: 22.5987\n",
            "==========================================\n",
            "Loss: 20.6082\n",
            "==========================================\n",
            "Loss: 19.8718\n",
            "==========================================\n",
            "Loss: 9.0977\n",
            "==========================================\n",
            "Loss: 14.349\n",
            "==========================================\n",
            "Loss: 9.6849\n",
            "==========================================\n",
            "Loss: 16.2419\n",
            "==========================================\n",
            "Loss: 24.5159\n",
            "==========================================\n",
            "Loss: 17.6032\n",
            "==========================================\n",
            "Loss: 14.461\n",
            "==========================================\n",
            "Loss: 12.8986\n",
            "==========================================\n",
            "Loss: 19.3792\n",
            "==========================================\n",
            "Loss: 17.696\n",
            "==========================================\n",
            "Loss: 27.6577\n",
            "==========================================\n",
            "Loss: 11.2488\n",
            "==========================================\n",
            "Loss: 11.3124\n",
            "==========================================\n",
            "Loss: 21.4708\n",
            "==========================================\n",
            "Loss: 11.8165\n",
            "==========================================\n",
            "Loss: 14.3492\n",
            "==========================================\n",
            "Loss: 28.4043\n",
            "==========================================\n",
            "Loss: 17.914\n",
            "==========================================\n",
            "Loss: 36.2386\n",
            "==========================================\n",
            "Loss: 15.4409\n",
            "==========================================\n",
            "Loss: 10.6457\n",
            "==========================================\n",
            "Loss: 11.1535\n",
            "==========================================\n",
            "Loss: 23.2842\n",
            "==========================================\n",
            "Loss: 12.8789\n",
            "==========================================\n",
            "Loss: 14.4215\n",
            "==========================================\n",
            "Loss: 19.4736\n",
            "==========================================\n",
            "Loss: 12.0717\n",
            "==========================================\n",
            "Loss: 13.5705\n",
            "==========================================\n",
            "Loss: 16.797\n",
            "==========================================\n",
            "Loss: 15.6028\n",
            "==========================================\n",
            "Loss: 12.2362\n",
            "==========================================\n",
            "Loss: 11.5644\n",
            "==========================================\n",
            "Loss: 13.3035\n",
            "==========================================\n",
            "Loss: 22.4933\n",
            "==========================================\n",
            "Loss: 16.0347\n",
            "==========================================\n",
            "Loss: 15.4363\n",
            "==========================================\n",
            "Loss: 20.2108\n",
            "==========================================\n",
            "Loss: 20.4395\n",
            "==========================================\n",
            "Train Epoch: [5]  [  50/8650]  eta: 1:18:26  lr: 0.000010  loss: 22.6643  time: 0.4213  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [ 100/8650]  eta: 1:13:53  lr: 0.000010  loss: 15.2497  time: 0.3828  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [ 150/8650]  eta: 1:11:14  lr: 0.000010  loss: 23.9763  time: 0.3872  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [ 200/8650]  eta: 1:09:35  lr: 0.000010  loss: 18.9160  time: 0.3983  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [ 250/8650]  eta: 1:08:34  lr: 0.000010  loss: 15.9469  time: 0.4159  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 300/8650]  eta: 1:07:53  lr: 0.000010  loss: 13.9303  time: 0.4276  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 350/8650]  eta: 1:07:10  lr: 0.000010  loss: 20.1633  time: 0.4608  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [ 400/8650]  eta: 1:06:33  lr: 0.000010  loss: 10.5077  time: 0.4580  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 450/8650]  eta: 1:05:55  lr: 0.000010  loss: 25.5764  time: 0.4496  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [ 500/8650]  eta: 1:05:21  lr: 0.000010  loss: 21.8802  time: 0.4631  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 550/8650]  eta: 1:04:49  lr: 0.000010  loss: 11.6388  time: 0.4595  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 600/8650]  eta: 1:04:18  lr: 0.000010  loss: 28.6009  time: 0.4749  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 650/8650]  eta: 1:03:51  lr: 0.000010  loss: 17.8589  time: 0.4739  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [ 700/8650]  eta: 1:03:27  lr: 0.000010  loss: 21.1077  time: 0.4833  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 750/8650]  eta: 1:03:03  lr: 0.000010  loss: 17.8985  time: 0.4979  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 800/8650]  eta: 1:02:36  lr: 0.000010  loss: 14.0337  time: 0.4953  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [ 850/8650]  eta: 1:02:11  lr: 0.000010  loss: 17.2723  time: 0.5012  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [ 900/8650]  eta: 1:01:45  lr: 0.000010  loss: 14.9004  time: 0.5098  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [ 950/8650]  eta: 1:01:19  lr: 0.000010  loss: 18.8696  time: 0.5046  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [1000/8650]  eta: 1:00:53  lr: 0.000010  loss: 17.8654  time: 0.5119  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1050/8650]  eta: 1:00:28  lr: 0.000010  loss: 26.5956  time: 0.5219  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1100/8650]  eta: 1:00:01  lr: 0.000010  loss: 17.9659  time: 0.5131  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1150/8650]  eta: 0:59:36  lr: 0.000010  loss: 15.0152  time: 0.5156  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1200/8650]  eta: 0:59:12  lr: 0.000010  loss: 13.0448  time: 0.5063  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [1250/8650]  eta: 0:58:48  lr: 0.000010  loss: 30.0019  time: 0.5297  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1300/8650]  eta: 0:58:24  lr: 0.000010  loss: 17.8462  time: 0.5167  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [1350/8650]  eta: 0:57:58  lr: 0.000010  loss: 16.1961  time: 0.5105  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [1400/8650]  eta: 0:57:32  lr: 0.000010  loss: 19.2758  time: 0.5181  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1450/8650]  eta: 0:57:09  lr: 0.000010  loss: 32.9031  time: 0.5376  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [1500/8650]  eta: 0:56:45  lr: 0.000010  loss: 20.0519  time: 0.5595  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [1550/8650]  eta: 0:56:21  lr: 0.000010  loss: 16.9574  time: 0.5570  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [1600/8650]  eta: 0:55:57  lr: 0.000010  loss: 18.2196  time: 0.5654  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [1650/8650]  eta: 0:55:34  lr: 0.000010  loss: 17.6471  time: 0.5688  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [5]  [1700/8650]  eta: 0:55:11  lr: 0.000010  loss: 22.6804  time: 0.5794  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [5]  [1750/8650]  eta: 0:54:48  lr: 0.000010  loss: 18.9611  time: 0.5902  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [1800/8650]  eta: 0:54:25  lr: 0.000010  loss: 26.9981  time: 0.5855  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [1850/8650]  eta: 0:54:01  lr: 0.000010  loss: 15.4759  time: 0.5937  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [1900/8650]  eta: 0:53:37  lr: 0.000010  loss: 14.2039  time: 0.5861  data: 0.0017  max mem: 9086\n",
            "Train Epoch: [5]  [1950/8650]  eta: 0:53:13  lr: 0.000010  loss: 13.6502  time: 0.5730  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [2000/8650]  eta: 0:52:48  lr: 0.000010  loss: 15.8126  time: 0.5728  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [2050/8650]  eta: 0:52:26  lr: 0.000010  loss: 21.7988  time: 0.5919  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2100/8650]  eta: 0:52:00  lr: 0.000010  loss: 17.1857  time: 0.5531  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [2150/8650]  eta: 0:51:35  lr: 0.000010  loss: 31.7463  time: 0.5486  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [2200/8650]  eta: 0:51:11  lr: 0.000010  loss: 19.0120  time: 0.5629  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [2250/8650]  eta: 0:50:46  lr: 0.000010  loss: 16.4660  time: 0.5410  data: 0.0016  max mem: 9086\n",
            "Train Epoch: [5]  [2300/8650]  eta: 0:50:20  lr: 0.000010  loss: 16.8483  time: 0.5167  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [2350/8650]  eta: 0:49:56  lr: 0.000010  loss: 16.8959  time: 0.5451  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2400/8650]  eta: 0:49:31  lr: 0.000010  loss: 24.4734  time: 0.5259  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2450/8650]  eta: 0:49:06  lr: 0.000010  loss: 12.2768  time: 0.5288  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [5]  [2500/8650]  eta: 0:48:42  lr: 0.000010  loss: 18.6099  time: 0.5178  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2550/8650]  eta: 0:48:18  lr: 0.000010  loss: 21.9309  time: 0.5259  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [2600/8650]  eta: 0:47:53  lr: 0.000010  loss: 22.8878  time: 0.5215  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2650/8650]  eta: 0:47:29  lr: 0.000010  loss: 23.8751  time: 0.5177  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [2700/8650]  eta: 0:47:05  lr: 0.000010  loss: 23.6630  time: 0.5182  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2750/8650]  eta: 0:46:40  lr: 0.000010  loss: 12.3800  time: 0.5178  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [2800/8650]  eta: 0:46:16  lr: 0.000010  loss: 18.9863  time: 0.5155  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [2850/8650]  eta: 0:45:52  lr: 0.000010  loss: 14.0717  time: 0.5146  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [2900/8650]  eta: 0:45:27  lr: 0.000010  loss: 17.0245  time: 0.4995  data: 0.0014  max mem: 9086\n",
            "Train Epoch: [5]  [2950/8650]  eta: 0:45:04  lr: 0.000010  loss: 19.6637  time: 0.5122  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [3000/8650]  eta: 0:44:39  lr: 0.000010  loss: 17.7465  time: 0.5050  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [3050/8650]  eta: 0:44:16  lr: 0.000010  loss: 19.0228  time: 0.5087  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3100/8650]  eta: 0:43:51  lr: 0.000010  loss: 21.1262  time: 0.4983  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [3150/8650]  eta: 0:43:27  lr: 0.000010  loss: 17.6987  time: 0.5115  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [3200/8650]  eta: 0:43:03  lr: 0.000010  loss: 16.1342  time: 0.4958  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3250/8650]  eta: 0:42:39  lr: 0.000010  loss: 16.9006  time: 0.4985  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [3300/8650]  eta: 0:42:15  lr: 0.000010  loss: 12.9570  time: 0.4905  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [3350/8650]  eta: 0:41:51  lr: 0.000010  loss: 19.3896  time: 0.4985  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3400/8650]  eta: 0:41:27  lr: 0.000010  loss: 15.2498  time: 0.4943  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3450/8650]  eta: 0:41:03  lr: 0.000010  loss: 23.7679  time: 0.4998  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3500/8650]  eta: 0:40:40  lr: 0.000010  loss: 21.7445  time: 0.5030  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3550/8650]  eta: 0:40:15  lr: 0.000010  loss: 18.1096  time: 0.4872  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [3600/8650]  eta: 0:39:51  lr: 0.000010  loss: 13.7234  time: 0.4872  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [3650/8650]  eta: 0:39:28  lr: 0.000010  loss: 13.4296  time: 0.4924  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3700/8650]  eta: 0:39:04  lr: 0.000010  loss: 22.6920  time: 0.4808  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3750/8650]  eta: 0:38:40  lr: 0.000010  loss: 15.0043  time: 0.4847  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3800/8650]  eta: 0:38:16  lr: 0.000010  loss: 11.8963  time: 0.4775  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [3850/8650]  eta: 0:37:53  lr: 0.000010  loss: 22.3496  time: 0.4819  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3900/8650]  eta: 0:37:29  lr: 0.000010  loss: 18.1623  time: 0.4659  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [3950/8650]  eta: 0:37:05  lr: 0.000010  loss: 15.9868  time: 0.4838  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4000/8650]  eta: 0:36:41  lr: 0.000010  loss: 19.5400  time: 0.4668  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4050/8650]  eta: 0:36:17  lr: 0.000010  loss: 16.0361  time: 0.4552  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [4100/8650]  eta: 0:35:53  lr: 0.000010  loss: 19.7136  time: 0.4561  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4150/8650]  eta: 0:35:29  lr: 0.000010  loss: 23.0797  time: 0.4554  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4200/8650]  eta: 0:35:05  lr: 0.000010  loss: 20.5213  time: 0.4487  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [4250/8650]  eta: 0:34:42  lr: 0.000010  loss: 18.6707  time: 0.4509  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [4300/8650]  eta: 0:34:18  lr: 0.000010  loss: 22.5799  time: 0.4367  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4350/8650]  eta: 0:33:54  lr: 0.000010  loss: 24.8413  time: 0.4364  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4400/8650]  eta: 0:33:30  lr: 0.000010  loss: 22.5026  time: 0.4449  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4450/8650]  eta: 0:33:06  lr: 0.000010  loss: 24.9719  time: 0.4338  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4500/8650]  eta: 0:32:42  lr: 0.000010  loss: 17.4751  time: 0.4281  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4550/8650]  eta: 0:32:19  lr: 0.000010  loss: 16.4375  time: 0.4301  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4600/8650]  eta: 0:31:55  lr: 0.000010  loss: 18.7288  time: 0.4184  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4650/8650]  eta: 0:31:31  lr: 0.000010  loss: 22.3711  time: 0.4152  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4700/8650]  eta: 0:31:08  lr: 0.000010  loss: 15.5466  time: 0.4371  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4750/8650]  eta: 0:30:44  lr: 0.000010  loss: 23.8209  time: 0.4087  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [4800/8650]  eta: 0:30:20  lr: 0.000010  loss: 14.9450  time: 0.4062  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [4850/8650]  eta: 0:29:57  lr: 0.000010  loss: 13.9219  time: 0.3968  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [4900/8650]  eta: 0:29:33  lr: 0.000010  loss: 18.1634  time: 0.3948  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [4950/8650]  eta: 0:29:09  lr: 0.000010  loss: 17.3596  time: 0.3968  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5000/8650]  eta: 0:28:46  lr: 0.000010  loss: 16.0688  time: 0.4013  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [5050/8650]  eta: 0:28:22  lr: 0.000010  loss: 16.8294  time: 0.3895  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5100/8650]  eta: 0:27:58  lr: 0.000010  loss: 17.8005  time: 0.3866  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5150/8650]  eta: 0:27:34  lr: 0.000010  loss: 22.0108  time: 0.3876  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5200/8650]  eta: 0:27:10  lr: 0.000010  loss: 21.6080  time: 0.3917  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5250/8650]  eta: 0:26:47  lr: 0.000010  loss: 22.3333  time: 0.3854  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5300/8650]  eta: 0:26:23  lr: 0.000010  loss: 22.6746  time: 0.3868  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5350/8650]  eta: 0:25:59  lr: 0.000010  loss: 17.1038  time: 0.3861  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5400/8650]  eta: 0:25:36  lr: 0.000010  loss: 17.1104  time: 0.3846  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5450/8650]  eta: 0:25:12  lr: 0.000010  loss: 19.1291  time: 0.3920  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5500/8650]  eta: 0:24:48  lr: 0.000010  loss: 16.1415  time: 0.3863  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5550/8650]  eta: 0:24:24  lr: 0.000010  loss: 18.4819  time: 0.3868  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5600/8650]  eta: 0:24:01  lr: 0.000010  loss: 19.6923  time: 0.3865  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5650/8650]  eta: 0:23:37  lr: 0.000010  loss: 18.0465  time: 0.3872  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5700/8650]  eta: 0:23:13  lr: 0.000010  loss: 20.5575  time: 0.3856  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5750/8650]  eta: 0:22:49  lr: 0.000010  loss: 14.5658  time: 0.3867  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5800/8650]  eta: 0:22:26  lr: 0.000010  loss: 17.0786  time: 0.3945  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5850/8650]  eta: 0:22:02  lr: 0.000010  loss: 17.1710  time: 0.3901  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [5900/8650]  eta: 0:21:38  lr: 0.000010  loss: 18.2158  time: 0.3881  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [5950/8650]  eta: 0:21:15  lr: 0.000010  loss: 23.3162  time: 0.3886  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6000/8650]  eta: 0:20:51  lr: 0.000010  loss: 19.5634  time: 0.3982  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6050/8650]  eta: 0:20:27  lr: 0.000010  loss: 25.0081  time: 0.3960  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6100/8650]  eta: 0:20:04  lr: 0.000010  loss: 27.6452  time: 0.4011  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6150/8650]  eta: 0:19:40  lr: 0.000010  loss: 18.6088  time: 0.3953  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6200/8650]  eta: 0:19:17  lr: 0.000010  loss: 28.2263  time: 0.4002  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6250/8650]  eta: 0:18:53  lr: 0.000010  loss: 13.9452  time: 0.4063  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [5]  [6300/8650]  eta: 0:18:29  lr: 0.000010  loss: 19.3375  time: 0.4149  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6350/8650]  eta: 0:18:05  lr: 0.000010  loss: 18.0725  time: 0.4187  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [5]  [6400/8650]  eta: 0:17:42  lr: 0.000010  loss: 17.8812  time: 0.4276  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6450/8650]  eta: 0:17:18  lr: 0.000010  loss: 19.5031  time: 0.4354  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6500/8650]  eta: 0:16:55  lr: 0.000010  loss: 27.0111  time: 0.4256  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6550/8650]  eta: 0:16:31  lr: 0.000010  loss: 16.5584  time: 0.4403  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [5]  [6600/8650]  eta: 0:16:07  lr: 0.000010  loss: 75.6905  time: 0.4516  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [6650/8650]  eta: 0:15:44  lr: 0.000010  loss: 17.4720  time: 0.4603  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [6700/8650]  eta: 0:15:20  lr: 0.000010  loss: 24.8429  time: 0.4605  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [6750/8650]  eta: 0:14:57  lr: 0.000010  loss: 29.5093  time: 0.4785  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6800/8650]  eta: 0:14:33  lr: 0.000010  loss: 14.5580  time: 0.4644  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [6850/8650]  eta: 0:14:09  lr: 0.000010  loss: 12.8795  time: 0.4749  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [6900/8650]  eta: 0:13:46  lr: 0.000010  loss: 10.5785  time: 0.4905  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [6950/8650]  eta: 0:13:22  lr: 0.000010  loss: 14.0059  time: 0.5015  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [7000/8650]  eta: 0:12:58  lr: 0.000010  loss: 17.3223  time: 0.5071  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [5]  [7050/8650]  eta: 0:12:35  lr: 0.000010  loss: 16.0947  time: 0.5052  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [5]  [7100/8650]  eta: 0:12:11  lr: 0.000010  loss: 20.3462  time: 0.5037  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7150/8650]  eta: 0:11:47  lr: 0.000010  loss: 15.3771  time: 0.5259  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [7200/8650]  eta: 0:11:24  lr: 0.000010  loss: 14.2852  time: 0.5370  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7250/8650]  eta: 0:11:00  lr: 0.000010  loss: 20.3958  time: 0.5219  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7300/8650]  eta: 0:10:37  lr: 0.000010  loss: 21.4493  time: 0.5313  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7350/8650]  eta: 0:10:13  lr: 0.000010  loss: 14.3397  time: 0.5220  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [7400/8650]  eta: 0:09:49  lr: 0.000010  loss: 19.4074  time: 0.5273  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7450/8650]  eta: 0:09:26  lr: 0.000010  loss: 10.7115  time: 0.5400  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [7500/8650]  eta: 0:09:02  lr: 0.000010  loss: 13.6305  time: 0.5490  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [7550/8650]  eta: 0:08:39  lr: 0.000010  loss: 20.9520  time: 0.5458  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7600/8650]  eta: 0:08:15  lr: 0.000010  loss: 17.9767  time: 0.5490  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7650/8650]  eta: 0:07:51  lr: 0.000010  loss: 18.9531  time: 0.5522  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7700/8650]  eta: 0:07:28  lr: 0.000010  loss: 16.1686  time: 0.5609  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7750/8650]  eta: 0:07:04  lr: 0.000010  loss: 17.8541  time: 0.5663  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [7800/8650]  eta: 0:06:41  lr: 0.000010  loss: 16.3566  time: 0.5678  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [5]  [7850/8650]  eta: 0:06:17  lr: 0.000010  loss: 17.3560  time: 0.5641  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [7900/8650]  eta: 0:05:53  lr: 0.000010  loss: 24.6958  time: 0.5734  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [7950/8650]  eta: 0:05:30  lr: 0.000010  loss: 25.8170  time: 0.5562  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [8000/8650]  eta: 0:05:06  lr: 0.000010  loss: 12.8106  time: 0.5729  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [5]  [8050/8650]  eta: 0:04:43  lr: 0.000010  loss: 13.6065  time: 0.5711  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [8100/8650]  eta: 0:04:19  lr: 0.000010  loss: 13.5079  time: 0.5761  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8150/8650]  eta: 0:03:55  lr: 0.000010  loss: 20.3843  time: 0.5712  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8200/8650]  eta: 0:03:32  lr: 0.000010  loss: 19.3349  time: 0.5392  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [5]  [8250/8650]  eta: 0:03:08  lr: 0.000010  loss: 31.1773  time: 0.5508  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8300/8650]  eta: 0:02:45  lr: 0.000010  loss: 22.0612  time: 0.5456  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [5]  [8350/8650]  eta: 0:02:21  lr: 0.000010  loss: 15.6060  time: 0.5561  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8400/8650]  eta: 0:01:57  lr: 0.000010  loss: 19.8906  time: 0.5377  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5]  [8450/8650]  eta: 0:01:34  lr: 0.000010  loss: 15.1985  time: 0.5174  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [8500/8650]  eta: 0:01:10  lr: 0.000010  loss: 14.9436  time: 0.4964  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [5]  [8550/8650]  eta: 0:00:47  lr: 0.000010  loss: 20.7985  time: 0.5072  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [5]  [8600/8650]  eta: 0:00:23  lr: 0.000010  loss: 7.3807  time: 0.5077  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [5]  [8649/8650]  eta: 0:00:00  lr: 0.000010  loss: 18.1439  time: 0.4515  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [5] Total time: 1:08:01 (0.4718 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 17.9689\n",
            "Loss: 13.4376\n",
            "==========================================\n",
            "Train Epoch: [6]  [   0/8650]  eta: 3:12:52  lr: 0.000007  loss: 13.4376  time: 1.3378  data: 0.5954  max mem: 9086\n",
            "Loss: 20.8744\n",
            "==========================================\n",
            "Loss: 20.0819\n",
            "==========================================\n",
            "Loss: 17.9503\n",
            "==========================================\n",
            "Loss: 21.598\n",
            "==========================================\n",
            "Loss: 13.175\n",
            "==========================================\n",
            "Loss: 15.4222\n",
            "==========================================\n",
            "Loss: 16.8987\n",
            "==========================================\n",
            "Loss: 13.7785\n",
            "==========================================\n",
            "Loss: 12.5769\n",
            "==========================================\n",
            "Loss: 14.4474\n",
            "==========================================\n",
            "Loss: 11.3288\n",
            "==========================================\n",
            "Loss: 14.2334\n",
            "==========================================\n",
            "Loss: 12.7517\n",
            "==========================================\n",
            "Loss: 17.0193\n",
            "==========================================\n",
            "Loss: 12.6983\n",
            "==========================================\n",
            "Loss: 29.6051\n",
            "==========================================\n",
            "Loss: 17.2994\n",
            "==========================================\n",
            "Loss: 15.2467\n",
            "==========================================\n",
            "Loss: 11.9955\n",
            "==========================================\n",
            "Loss: 10.461\n",
            "==========================================\n",
            "Loss: 16.014\n",
            "==========================================\n",
            "Loss: 58.0971\n",
            "==========================================\n",
            "Loss: 15.6003\n",
            "==========================================\n",
            "Loss: 16.8661\n",
            "==========================================\n",
            "Loss: 15.936\n",
            "==========================================\n",
            "Loss: 22.9431\n",
            "==========================================\n",
            "Loss: 14.2093\n",
            "==========================================\n",
            "Loss: 13.4473\n",
            "==========================================\n",
            "Loss: 15.9391\n",
            "==========================================\n",
            "Loss: 12.967\n",
            "==========================================\n",
            "Loss: 12.6379\n",
            "==========================================\n",
            "Loss: 15.2209\n",
            "==========================================\n",
            "Loss: 14.7253\n",
            "==========================================\n",
            "Loss: 16.2511\n",
            "==========================================\n",
            "Loss: 11.0988\n",
            "==========================================\n",
            "Loss: 15.3164\n",
            "==========================================\n",
            "Loss: 14.8684\n",
            "==========================================\n",
            "Loss: 11.7916\n",
            "==========================================\n",
            "Loss: 12.5103\n",
            "==========================================\n",
            "Loss: 12.4806\n",
            "==========================================\n",
            "Loss: 18.1254\n",
            "==========================================\n",
            "Loss: 26.3976\n",
            "==========================================\n",
            "Loss: 19.9242\n",
            "==========================================\n",
            "Loss: 12.6834\n",
            "==========================================\n",
            "Loss: 30.2889\n",
            "==========================================\n",
            "Loss: 20.0994\n",
            "==========================================\n",
            "Loss: 22.2018\n",
            "==========================================\n",
            "Loss: 14.2985\n",
            "==========================================\n",
            "Loss: 15.2121\n",
            "==========================================\n",
            "Train Epoch: [6]  [  50/8650]  eta: 1:21:50  lr: 0.000007  loss: 10.9908  time: 0.5035  data: 0.0005  max mem: 9086\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 63, in train\n",
            "    metric_logger.update(loss=loss.item())\n",
            "                              ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA/checkpoint_05.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwGEdm4q5aL",
        "outputId": "26128e73-dfa8-4c1c-98a2-cddae7a2e1b3",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
            "/usr/local/lib/python3.12/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n",
            "Not using distributed mode\n",
            "Creating vqa datasets...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating SimpleLLaVA model (Float32)...\n",
            "Loading BLIP ViT: base, size: 224\n",
            "Loading LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "2025-11-27 10:28:33.284013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764239313.452546   16233 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764239313.498604   16233 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764239313.850262   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850300   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850304   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850310   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-27 10:28:33.888460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using Initial LR: 1e-05\n",
            "Start training\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/BLIP/transform/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "/content/drive/MyDrive/BLIP/transform/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 180, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 139, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 56, in train\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 82, in _use_grad\n",
            "    ret = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 237, in step\n",
            "    has_complex = self._init_group(\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 181, in _init_group\n",
            "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
            "                          ^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 162628 has 14.74 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 31.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa_llava.py --config ./configs/textvqa.yaml --output_dir ./output/textVQA_llava --device cuda --distributed False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oLyiLEWV2SQi",
        "outputId": "2ebf0440-c1be-4e54-c20f-0005bda56c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/BLIP/train_vqa_llava.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/BLIP/train_vqa_llava.py\n",
        "import argparse\n",
        "import os\n",
        "from ruamel.yaml import YAML\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "\n",
        "from models.simple_llava import SimpleLLaVA\n",
        "import utils\n",
        "from utils import cosine_lr_schedule\n",
        "from data import create_dataset, create_sampler, create_loader\n",
        "from data.vqa_dataset import vqa_collate_fn\n",
        "from data.textvqa_dataset import textvqa_collate_fn\n",
        "from data.utils import save_result\n",
        "\n",
        "def train(model, data_loader, optimizer, epoch, device, max_debug_batches=20):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "\n",
        "    header = 'Train Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "\n",
        "    for i, (image, question, answer, weights, n) in enumerate(\n",
        "            metric_logger.log_every(data_loader, print_freq, header)):\n",
        "\n",
        "        # Check NaN input\n",
        "        if torch.isnan(image).any():\n",
        "            print(f\"Skipping Batch {i} (Input Image NaN)\")\n",
        "            continue\n",
        "\n",
        "        image = image.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Chạy trực tiếp (Float32)\n",
        "        loss = model(image, question, answer, train=True)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"WARNING: Batch {i} Loss is NaN. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping (Vẫn giữ để an toàn)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
        "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, device, config):\n",
        "    model.eval()\n",
        "    result = []\n",
        "    print(\"Generating answers...\")\n",
        "\n",
        "    for n, (image, question, question_id) in enumerate(data_loader):\n",
        "        image = image.to(device, non_blocking=True)\n",
        "        answers = model(image, question, train=False)\n",
        "\n",
        "        for answer, ques_id in zip(answers, question_id):\n",
        "            ques_id = int(ques_id.item())\n",
        "            result.append({\"question_id\": ques_id, \"answer\": answer})\n",
        "\n",
        "        if n % 10 == 0:\n",
        "            print(f\"Sample: Q: {question[0]} | A: {answers[0]}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def main(args, config):\n",
        "    utils.init_distributed_mode(args)\n",
        "    device = torch.device(args.device)\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    print(\"Creating vqa datasets...\")\n",
        "    datasets = create_dataset('textvqa', config)\n",
        "\n",
        "    if args.distributed:\n",
        "        num_tasks = utils.get_world_size()\n",
        "        global_rank = utils.get_rank()\n",
        "        samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
        "    else:\n",
        "        samplers = [None, None]\n",
        "\n",
        "    train_loader, test_loader = create_loader(datasets, samplers,\n",
        "                                              batch_size=[config['batch_size_train'], config['batch_size_test']],\n",
        "                                              num_workers=[4,4], is_trains=[True, False],\n",
        "                                              collate_fns=[textvqa_collate_fn, None])\n",
        "\n",
        "    print(\"Creating SimpleLLaVA model (Float32)...\")\n",
        "    model = SimpleLLaVA(\n",
        "        llm_model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "        image_size=config['image_size'],\n",
        "        vit_type=config['vit'],\n",
        "        vit_grad_ckpt=config.get('vit_grad_ckpt', False),\n",
        "        vit_ckpt_layer=config.get('vit_ckpt_layer', 0),\n",
        "        pretrained_blip_url=config['pretrained'],\n",
        "        freeze_vision=True\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    # Init LR an toàn\n",
        "    init_lr = 1e-5\n",
        "    print(f\"Using Initial LR: {init_lr}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=init_lr, weight_decay=config['weight_decay'])\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(0, config['max_epoch']):\n",
        "        if not args.evaluate:\n",
        "            if args.distributed:\n",
        "                train_loader.sampler.set_epoch(epoch)\n",
        "            cosine_lr_schedule(optimizer, epoch, config['max_epoch'], init_lr, 1e-6)\n",
        "            train_stats = train(model, train_loader, optimizer, epoch, device)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        if utils.is_main_process():\n",
        "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, 'epoch': epoch}\n",
        "            with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "            save_obj = { 'model': model_without_ddp.state_dict(), 'epoch': epoch }\n",
        "            torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
        "\n",
        "        if args.distributed:\n",
        "            dist.barrier()\n",
        "\n",
        "    print(\"Starting Evaluation...\")\n",
        "    vqa_result = evaluation(model_without_ddp, test_loader, device, config)\n",
        "    result_file = save_result(vqa_result, args.result_dir, 'vqa_result')\n",
        "    total_time = time.time() - start_time\n",
        "    print('Training time {}'.format(str(datetime.timedelta(seconds=int(total_time)))))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', default='./configs/textvqa.yaml')\n",
        "    parser.add_argument('--output_dir', default='output/textVQA')\n",
        "    parser.add_argument('--evaluate', action='store_true')\n",
        "    parser.add_argument('--device', default='cuda')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--world_size', default=1, type=int)\n",
        "    parser.add_argument('--dist_url', default='env://')\n",
        "    parser.add_argument('--distributed', default=False, type=bool)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    yaml_loader = YAML()\n",
        "    yaml_loader.typ = 'safe'\n",
        "    with open(args.config, 'r') as f:\n",
        "      config = yaml_loader.load(f)\n",
        "    args.result_dir = os.path.join(args.output_dir, 'result')\n",
        "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
        "    with open(os.path.join(args.output_dir, 'config.yaml'), 'w') as f:\n",
        "      yaml_loader.dump(config, f)\n",
        "    main(args, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHhslGC2SmK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7qwuzqcvSyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}