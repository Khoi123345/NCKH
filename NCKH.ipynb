{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "5c0a7dc6-d19e-42e9-b21b-5d97b3f6567e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "f6dd874e-ee5c-4d47-f37e-e8c209cdad5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.11-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu128)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from cog) (2.32.4)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.40.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->cog) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Downloading cog-0.16.11-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.50.0\n",
            "    Uninstalling starlette-0.50.0:\n",
            "      Successfully uninstalled starlette-0.50.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.128.2\n",
            "    Uninstalling fastapi-0.128.2:\n",
            "      Successfully uninstalled fastapi-0.128.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.24.0 requires fastapi<1.0.0,>=0.124.1, but you have fastapi 0.118.3 which is incompatible.\n",
            "google-adk 1.24.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "sse-starlette 3.2.0 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.11 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.16.2)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292936 sha256=1dc68114a459166494cc7c539fc49927676014804cd1e213a98e57c0efd4e54b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "# TextVQA (phải chạy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "0045c4ad-5617-42f6-e3c1-4d8ecc84c8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-14 05:20:30--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.96, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M   252MB/s    in 0.4s    \n",
            "\n",
            "2026-02-14 05:20:30 (252 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-02-14 05:20:31--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.96, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-02-14 05:20:31 (131 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-02-14 05:20:31--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.96, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-02-14 05:20:31 (154 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "61225724-d60a-46e4-9952-b9a064cc2070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-14 05:20:31--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.96, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G   242MB/s    in 36s     \n",
            "\n",
            "2026-02-14 05:21:07 (187 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (Dhuy + HNhien)\n"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "4ecfdc11-70ec-48c3-a0a6-7703a0aa1ac7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "reshape position embedding from 900 to 196\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "<All keys matched successfully>\n",
            "Start training\n",
            "Loss: 17.1619\n",
            "==========================================\n",
            "Train Epoch: [0]  [   0/8650]  eta: 9:05:33  lr: 0.000020  loss: 17.1619  time: 3.7842  data: 0.8004  max mem: 6922\n",
            "Loss: 22.2107\n",
            "==========================================\n",
            "Loss: 17.6196\n",
            "==========================================\n",
            "Loss: 22.4522\n",
            "==========================================\n",
            "Loss: 18.5996\n",
            "==========================================\n",
            "Loss: 15.116\n",
            "==========================================\n",
            "Loss: 13.7835\n",
            "==========================================\n",
            "Loss: 22.9052\n",
            "==========================================\n",
            "Loss: 22.1199\n",
            "==========================================\n",
            "Loss: 22.657\n",
            "==========================================\n",
            "Loss: 16.3429\n",
            "==========================================\n",
            "Loss: 16.1788\n",
            "==========================================\n",
            "Loss: 13.1597\n",
            "==========================================\n",
            "Loss: 20.5788\n",
            "==========================================\n",
            "Loss: 15.4407\n",
            "==========================================\n",
            "Loss: 30.1561\n",
            "==========================================\n",
            "Loss: 16.5253\n",
            "==========================================\n",
            "Loss: 10.3148\n",
            "==========================================\n",
            "Loss: 17.5338\n",
            "==========================================\n",
            "Loss: 30.6049\n",
            "==========================================\n",
            "Loss: 24.2531\n",
            "==========================================\n",
            "Loss: 17.5263\n",
            "==========================================\n",
            "Loss: 18.6688\n",
            "==========================================\n",
            "Loss: 16.5131\n",
            "==========================================\n",
            "Loss: 12.7678\n",
            "==========================================\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2d6743aca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1600, in _shutdown_workers\n",
            "    self._pin_memory_thread.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa.py\", line 55, in train\n",
            "    loss = model.forward(image, question, answer, n, weights, train=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/blip_vqa.py\", line 63, in forward\n",
            "    answer_output = self.text_decoder(\n",
            "                    ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 886, in forward\n",
            "    outputs = self.bert(\n",
            "              ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 781, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 445, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 361, in forward\n",
            "    cross_attention_outputs = self.crossattention(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 286, in forward\n",
            "    attention_output = self.output(self_outputs[0], hidden_states)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/models/med.py\", line 236, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False #\\\n",
        "    # --resume ./output/textVQA/checkpoint_05.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train textVQA + pretrain_animal (Khoi)"
      ],
      "metadata": {
        "id": "i9pDWKQLVYL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pWHhslGC2SmK",
        "collapsed": true,
        "outputId": "9f5ec3fd-1a2c-474e-de39-10e47bdb00dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "load checkpoint from /content/drive/MyDrive/BLIP/output/pretrain_animals/checkpoint_29.pth\n",
            "_IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.0.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.0.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.1.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.1.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.2.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.2.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.3.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.3.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.4.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.4.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.5.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.5.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_proj_m.weight', 'text_proj_m.bias'])\n",
            "=> loading checkpoint './output/textVQA_pretrain_animals/checkpoint_01.pth'\n",
            "=> loaded checkpoint './output/textVQA_pretrain_animals/checkpoint_01.pth' (epoch 2)\n",
            "Start training\n",
            "Loss: 18.6777\n",
            "==========================================\n",
            "Train Epoch: [2]  [   0/8650]  eta: 8:01:34  lr: 0.000020  loss: 18.6777  time: 3.3404  data: 0.8189  max mem: 6922\n",
            "Loss: 22.4611\n",
            "==========================================\n",
            "Loss: 17.5866\n",
            "==========================================\n",
            "Loss: 21.231\n",
            "==========================================\n",
            "Loss: 21.3843\n",
            "==========================================\n",
            "Loss: 18.8618\n",
            "==========================================\n",
            "Loss: 15.5629\n",
            "==========================================\n",
            "Loss: 23.5244\n",
            "==========================================\n",
            "Loss: 23.9318\n",
            "==========================================\n",
            "Loss: 24.9128\n",
            "==========================================\n",
            "Loss: 21.4092\n",
            "==========================================\n",
            "Loss: 25.6195\n",
            "==========================================\n",
            "Loss: 15.3241\n",
            "==========================================\n",
            "Loss: 28.2695\n",
            "==========================================\n",
            "Loss: 14.47\n",
            "==========================================\n",
            "Loss: 33.8146\n",
            "==========================================\n",
            "Loss: 18.8912\n",
            "==========================================\n",
            "Loss: 12.1435\n",
            "==========================================\n",
            "Loss: 21.7021\n",
            "==========================================\n",
            "Loss: 34.1982\n",
            "==========================================\n",
            "Loss: 27.6942\n",
            "==========================================\n",
            "Loss: 20.1724\n",
            "==========================================\n",
            "Loss: 25.4174\n",
            "==========================================\n",
            "Loss: 21.936\n",
            "==========================================\n",
            "Loss: 16.4927\n",
            "==========================================\n",
            "Loss: 14.1448\n",
            "==========================================\n",
            "Loss: 20.5522\n",
            "==========================================\n",
            "Loss: 21.0139\n",
            "==========================================\n",
            "Loss: 15.5725\n",
            "==========================================\n",
            "Loss: 24.9856\n",
            "==========================================\n",
            "Loss: 43.5794\n",
            "==========================================\n",
            "Loss: 33.0342\n",
            "==========================================\n",
            "Loss: 18.8028\n",
            "==========================================\n",
            "Loss: 20.2039\n",
            "==========================================\n",
            "Loss: 17.1862\n",
            "==========================================\n",
            "Loss: 15.6105\n",
            "==========================================\n",
            "Loss: 35.078\n",
            "==========================================\n",
            "Loss: 30.657\n",
            "==========================================\n",
            "Loss: 19.6413\n",
            "==========================================\n",
            "Loss: 22.2628\n",
            "==========================================\n",
            "Loss: 15.0573\n",
            "==========================================\n",
            "Loss: 27.0616\n",
            "==========================================\n",
            "Loss: 17.0627\n",
            "==========================================\n",
            "Loss: 17.1132\n",
            "==========================================\n",
            "Loss: 10.2461\n",
            "==========================================\n",
            "Loss: 18.6734\n",
            "==========================================\n",
            "Loss: 17.8489\n",
            "==========================================\n",
            "Loss: 26.7007\n",
            "==========================================\n",
            "Loss: 12.4081\n",
            "==========================================\n",
            "Loss: 32.0864\n",
            "==========================================\n",
            "Train Epoch: [2]  [  50/8650]  eta: 1:17:37  lr: 0.000020  loss: 11.3213  time: 0.4140  data: 0.0003  max mem: 6940\n",
            "Train Epoch: [2]  [ 100/8650]  eta: 1:13:30  lr: 0.000020  loss: 26.7528  time: 0.4236  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [ 150/8650]  eta: 1:12:08  lr: 0.000020  loss: 19.2316  time: 0.4453  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 200/8650]  eta: 1:11:05  lr: 0.000020  loss: 17.9074  time: 0.4625  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 250/8650]  eta: 1:10:16  lr: 0.000020  loss: 16.4067  time: 0.4700  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 300/8650]  eta: 1:09:28  lr: 0.000020  loss: 14.8766  time: 0.4674  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 350/8650]  eta: 1:08:55  lr: 0.000020  loss: 23.0860  time: 0.4843  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 400/8650]  eta: 1:08:23  lr: 0.000020  loss: 22.7008  time: 0.4800  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 450/8650]  eta: 1:07:46  lr: 0.000020  loss: 18.1663  time: 0.4739  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 500/8650]  eta: 1:07:12  lr: 0.000020  loss: 21.0927  time: 0.4668  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 550/8650]  eta: 1:06:39  lr: 0.000020  loss: 16.1113  time: 0.4688  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 600/8650]  eta: 1:06:10  lr: 0.000020  loss: 21.0578  time: 0.4567  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 650/8650]  eta: 1:05:42  lr: 0.000020  loss: 12.5453  time: 0.4557  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 700/8650]  eta: 1:05:11  lr: 0.000020  loss: 20.5337  time: 0.4434  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 750/8650]  eta: 1:04:39  lr: 0.000020  loss: 22.1555  time: 0.4441  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 800/8650]  eta: 1:04:11  lr: 0.000020  loss: 25.5829  time: 0.4493  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [ 850/8650]  eta: 1:03:44  lr: 0.000020  loss: 16.4216  time: 0.4643  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 900/8650]  eta: 1:03:24  lr: 0.000020  loss: 27.3669  time: 0.4984  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [ 950/8650]  eta: 1:03:01  lr: 0.000020  loss: 19.1353  time: 0.4688  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [1000/8650]  eta: 1:02:37  lr: 0.000020  loss: 38.6375  time: 0.4832  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [1050/8650]  eta: 1:02:12  lr: 0.000020  loss: 39.5535  time: 0.5014  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1100/8650]  eta: 1:01:46  lr: 0.000020  loss: 24.7547  time: 0.4881  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1150/8650]  eta: 1:01:22  lr: 0.000020  loss: 21.8469  time: 0.4944  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1200/8650]  eta: 1:00:58  lr: 0.000020  loss: 22.1881  time: 0.5152  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1250/8650]  eta: 1:00:32  lr: 0.000020  loss: 28.9929  time: 0.5042  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1300/8650]  eta: 1:00:06  lr: 0.000020  loss: 11.9106  time: 0.5008  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [1350/8650]  eta: 0:59:40  lr: 0.000020  loss: 18.8612  time: 0.5110  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1400/8650]  eta: 0:59:14  lr: 0.000020  loss: 34.6210  time: 0.5086  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1450/8650]  eta: 0:58:49  lr: 0.000020  loss: 14.2832  time: 0.5143  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [1500/8650]  eta: 0:58:25  lr: 0.000020  loss: 36.6127  time: 0.5238  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1550/8650]  eta: 0:58:01  lr: 0.000020  loss: 29.3976  time: 0.5429  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1600/8650]  eta: 0:57:37  lr: 0.000020  loss: 32.4251  time: 0.5561  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1650/8650]  eta: 0:57:12  lr: 0.000020  loss: 29.5838  time: 0.5524  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [1700/8650]  eta: 0:56:47  lr: 0.000020  loss: 23.4966  time: 0.5492  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [1750/8650]  eta: 0:56:23  lr: 0.000020  loss: 18.0073  time: 0.5639  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [1800/8650]  eta: 0:55:58  lr: 0.000020  loss: 12.9533  time: 0.5604  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [1850/8650]  eta: 0:55:33  lr: 0.000020  loss: 16.7455  time: 0.5587  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [2]  [1900/8650]  eta: 0:55:07  lr: 0.000020  loss: 27.9282  time: 0.5574  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [1950/8650]  eta: 0:54:42  lr: 0.000020  loss: 26.8558  time: 0.5663  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2000/8650]  eta: 0:54:18  lr: 0.000020  loss: 23.7082  time: 0.5820  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2050/8650]  eta: 0:53:54  lr: 0.000020  loss: 18.5835  time: 0.5880  data: 0.0015  max mem: 9086\n",
            "Train Epoch: [2]  [2100/8650]  eta: 0:53:30  lr: 0.000020  loss: 19.2657  time: 0.5940  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [2]  [2150/8650]  eta: 0:53:05  lr: 0.000020  loss: 14.9809  time: 0.5829  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2200/8650]  eta: 0:52:40  lr: 0.000020  loss: 17.3706  time: 0.5940  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [2250/8650]  eta: 0:52:15  lr: 0.000020  loss: 17.6448  time: 0.5964  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2300/8650]  eta: 0:51:51  lr: 0.000020  loss: 27.0103  time: 0.5935  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2350/8650]  eta: 0:51:26  lr: 0.000020  loss: 23.0971  time: 0.5907  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [2400/8650]  eta: 0:51:02  lr: 0.000020  loss: 20.5081  time: 0.5892  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [2450/8650]  eta: 0:50:37  lr: 0.000020  loss: 20.4577  time: 0.5872  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2500/8650]  eta: 0:50:11  lr: 0.000020  loss: 13.9239  time: 0.5719  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2550/8650]  eta: 0:49:46  lr: 0.000020  loss: 15.3294  time: 0.5766  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [2]  [2600/8650]  eta: 0:49:21  lr: 0.000020  loss: 23.4801  time: 0.5736  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2650/8650]  eta: 0:48:56  lr: 0.000020  loss: 19.1022  time: 0.5645  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2700/8650]  eta: 0:48:32  lr: 0.000020  loss: 13.9283  time: 0.5853  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2750/8650]  eta: 0:48:07  lr: 0.000020  loss: 14.2848  time: 0.5698  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [2800/8650]  eta: 0:47:42  lr: 0.000020  loss: 22.3524  time: 0.5632  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2850/8650]  eta: 0:47:17  lr: 0.000020  loss: 18.8097  time: 0.5524  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [2900/8650]  eta: 0:46:52  lr: 0.000020  loss: 14.6909  time: 0.5469  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [2]  [2950/8650]  eta: 0:46:27  lr: 0.000020  loss: 38.7368  time: 0.5373  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3000/8650]  eta: 0:46:02  lr: 0.000020  loss: 23.0471  time: 0.5232  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3050/8650]  eta: 0:45:37  lr: 0.000020  loss: 18.9486  time: 0.5268  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3100/8650]  eta: 0:45:13  lr: 0.000020  loss: 11.1348  time: 0.5241  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3150/8650]  eta: 0:44:48  lr: 0.000020  loss: 10.4135  time: 0.5077  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3200/8650]  eta: 0:44:23  lr: 0.000020  loss: 24.7286  time: 0.4886  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [3250/8650]  eta: 0:43:59  lr: 0.000020  loss: 24.3019  time: 0.4855  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3300/8650]  eta: 0:43:34  lr: 0.000020  loss: 16.5433  time: 0.4830  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3350/8650]  eta: 0:43:10  lr: 0.000020  loss: 16.2959  time: 0.4875  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3400/8650]  eta: 0:42:45  lr: 0.000020  loss: 22.4982  time: 0.4930  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3450/8650]  eta: 0:42:20  lr: 0.000020  loss: 20.5334  time: 0.4828  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3500/8650]  eta: 0:41:55  lr: 0.000020  loss: 12.4321  time: 0.4606  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3550/8650]  eta: 0:41:30  lr: 0.000020  loss: 15.6064  time: 0.4610  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3600/8650]  eta: 0:41:06  lr: 0.000020  loss: 28.5957  time: 0.4694  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3650/8650]  eta: 0:40:41  lr: 0.000020  loss: 27.8610  time: 0.4664  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3700/8650]  eta: 0:40:16  lr: 0.000020  loss: 45.4184  time: 0.4641  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3750/8650]  eta: 0:39:52  lr: 0.000020  loss: 15.8657  time: 0.4535  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [3800/8650]  eta: 0:39:27  lr: 0.000020  loss: 28.7267  time: 0.4404  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [3850/8650]  eta: 0:39:03  lr: 0.000020  loss: 11.2155  time: 0.4455  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [3900/8650]  eta: 0:38:39  lr: 0.000020  loss: 22.4864  time: 0.4438  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [3950/8650]  eta: 0:38:14  lr: 0.000020  loss: 19.6391  time: 0.4281  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4000/8650]  eta: 0:37:50  lr: 0.000020  loss: 19.9518  time: 0.4357  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4050/8650]  eta: 0:37:25  lr: 0.000020  loss: 23.6136  time: 0.4392  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4100/8650]  eta: 0:37:01  lr: 0.000020  loss: 28.0065  time: 0.4389  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4150/8650]  eta: 0:36:36  lr: 0.000020  loss: 30.8591  time: 0.4295  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [4200/8650]  eta: 0:36:11  lr: 0.000020  loss: 10.3012  time: 0.4299  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4250/8650]  eta: 0:35:47  lr: 0.000020  loss: 15.7053  time: 0.4277  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4300/8650]  eta: 0:35:22  lr: 0.000020  loss: 22.4956  time: 0.4241  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4350/8650]  eta: 0:34:58  lr: 0.000020  loss: 19.8158  time: 0.4274  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4400/8650]  eta: 0:34:33  lr: 0.000020  loss: 18.8622  time: 0.4285  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4450/8650]  eta: 0:34:08  lr: 0.000020  loss: 27.9112  time: 0.4211  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4500/8650]  eta: 0:33:44  lr: 0.000020  loss: 18.0065  time: 0.4159  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4550/8650]  eta: 0:33:20  lr: 0.000020  loss: 26.4398  time: 0.4229  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [4600/8650]  eta: 0:32:55  lr: 0.000020  loss: 18.6580  time: 0.4180  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [4650/8650]  eta: 0:32:31  lr: 0.000020  loss: 26.3294  time: 0.4206  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4700/8650]  eta: 0:32:06  lr: 0.000020  loss: 15.2361  time: 0.4135  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4750/8650]  eta: 0:31:41  lr: 0.000020  loss: 23.4677  time: 0.4159  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4800/8650]  eta: 0:31:17  lr: 0.000020  loss: 18.3755  time: 0.4141  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4850/8650]  eta: 0:30:53  lr: 0.000020  loss: 19.6894  time: 0.4118  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [4900/8650]  eta: 0:30:28  lr: 0.000020  loss: 18.0252  time: 0.4111  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [4950/8650]  eta: 0:30:03  lr: 0.000020  loss: 26.3030  time: 0.4132  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5000/8650]  eta: 0:29:39  lr: 0.000020  loss: 19.2550  time: 0.4123  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5050/8650]  eta: 0:29:14  lr: 0.000020  loss: 19.6678  time: 0.4077  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [5100/8650]  eta: 0:28:50  lr: 0.000020  loss: 29.5000  time: 0.4046  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5150/8650]  eta: 0:28:25  lr: 0.000020  loss: 20.2661  time: 0.4040  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5200/8650]  eta: 0:28:01  lr: 0.000020  loss: 30.6165  time: 0.4058  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5250/8650]  eta: 0:27:36  lr: 0.000020  loss: 22.1443  time: 0.4000  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5300/8650]  eta: 0:27:12  lr: 0.000020  loss: 22.1997  time: 0.3987  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5350/8650]  eta: 0:26:47  lr: 0.000020  loss: 20.0713  time: 0.4014  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5400/8650]  eta: 0:26:23  lr: 0.000020  loss: 20.0836  time: 0.4008  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5450/8650]  eta: 0:25:58  lr: 0.000020  loss: 25.4870  time: 0.4002  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5500/8650]  eta: 0:25:34  lr: 0.000020  loss: 23.4917  time: 0.3999  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5550/8650]  eta: 0:25:10  lr: 0.000020  loss: 19.4156  time: 0.3973  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5600/8650]  eta: 0:24:45  lr: 0.000020  loss: 20.0541  time: 0.3978  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5650/8650]  eta: 0:24:21  lr: 0.000020  loss: 16.1023  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5700/8650]  eta: 0:23:56  lr: 0.000020  loss: 23.8969  time: 0.3956  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5750/8650]  eta: 0:23:32  lr: 0.000020  loss: 24.9469  time: 0.3987  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [5800/8650]  eta: 0:23:07  lr: 0.000020  loss: 14.8104  time: 0.3985  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5850/8650]  eta: 0:22:43  lr: 0.000020  loss: 30.2436  time: 0.3984  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5900/8650]  eta: 0:22:18  lr: 0.000020  loss: 20.2069  time: 0.4023  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [5950/8650]  eta: 0:21:54  lr: 0.000020  loss: 16.5891  time: 0.3969  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6000/8650]  eta: 0:21:29  lr: 0.000020  loss: 15.4188  time: 0.3995  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6050/8650]  eta: 0:21:05  lr: 0.000020  loss: 17.2330  time: 0.4045  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6100/8650]  eta: 0:20:41  lr: 0.000020  loss: 42.9411  time: 0.4095  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [6150/8650]  eta: 0:20:16  lr: 0.000020  loss: 20.3841  time: 0.4086  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6200/8650]  eta: 0:19:52  lr: 0.000020  loss: 36.5661  time: 0.4009  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6250/8650]  eta: 0:19:27  lr: 0.000020  loss: 29.8419  time: 0.4088  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [6300/8650]  eta: 0:19:03  lr: 0.000020  loss: 24.9038  time: 0.4072  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6350/8650]  eta: 0:18:39  lr: 0.000020  loss: 24.2387  time: 0.3994  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6400/8650]  eta: 0:18:14  lr: 0.000020  loss: 30.8603  time: 0.4046  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6450/8650]  eta: 0:17:50  lr: 0.000020  loss: 27.2504  time: 0.4071  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6500/8650]  eta: 0:17:25  lr: 0.000020  loss: 18.4068  time: 0.4060  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6550/8650]  eta: 0:17:01  lr: 0.000020  loss: 23.0896  time: 0.3973  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6600/8650]  eta: 0:16:36  lr: 0.000020  loss: 28.5007  time: 0.4026  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6650/8650]  eta: 0:16:12  lr: 0.000020  loss: 14.2743  time: 0.3990  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6700/8650]  eta: 0:15:48  lr: 0.000020  loss: 27.9416  time: 0.3974  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6750/8650]  eta: 0:15:23  lr: 0.000020  loss: 10.9004  time: 0.3971  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6800/8650]  eta: 0:14:59  lr: 0.000020  loss: 19.7534  time: 0.3951  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [6850/8650]  eta: 0:14:35  lr: 0.000020  loss: 19.8190  time: 0.3986  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6900/8650]  eta: 0:14:10  lr: 0.000020  loss: 13.7917  time: 0.3967  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [6950/8650]  eta: 0:13:46  lr: 0.000020  loss: 20.5787  time: 0.3943  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7000/8650]  eta: 0:13:21  lr: 0.000020  loss: 24.9594  time: 0.4044  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7050/8650]  eta: 0:12:57  lr: 0.000020  loss: 26.6349  time: 0.3963  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [2]  [7100/8650]  eta: 0:12:33  lr: 0.000020  loss: 11.3433  time: 0.4141  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7150/8650]  eta: 0:12:09  lr: 0.000020  loss: 18.7803  time: 0.4195  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [2]  [7200/8650]  eta: 0:11:44  lr: 0.000020  loss: 28.9034  time: 0.4261  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7250/8650]  eta: 0:11:20  lr: 0.000020  loss: 36.1561  time: 0.4515  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7300/8650]  eta: 0:10:56  lr: 0.000020  loss: 19.7313  time: 0.4493  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7350/8650]  eta: 0:10:31  lr: 0.000020  loss: 28.8929  time: 0.4602  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [7400/8650]  eta: 0:10:07  lr: 0.000020  loss: 19.8307  time: 0.4629  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [7450/8650]  eta: 0:09:43  lr: 0.000020  loss: 20.7325  time: 0.4731  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7500/8650]  eta: 0:09:18  lr: 0.000020  loss: 29.1672  time: 0.4743  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7550/8650]  eta: 0:08:54  lr: 0.000020  loss: 16.7650  time: 0.4909  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7600/8650]  eta: 0:08:30  lr: 0.000020  loss: 20.5881  time: 0.4770  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7650/8650]  eta: 0:08:05  lr: 0.000020  loss: 22.4282  time: 0.4689  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7700/8650]  eta: 0:07:41  lr: 0.000020  loss: 21.4344  time: 0.4869  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [7750/8650]  eta: 0:07:17  lr: 0.000020  loss: 16.9838  time: 0.4798  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7800/8650]  eta: 0:06:52  lr: 0.000020  loss: 21.4201  time: 0.4923  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7850/8650]  eta: 0:06:28  lr: 0.000020  loss: 21.2232  time: 0.4950  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [7900/8650]  eta: 0:06:04  lr: 0.000020  loss: 27.5584  time: 0.4860  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [7950/8650]  eta: 0:05:40  lr: 0.000020  loss: 31.5639  time: 0.4880  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [2]  [8000/8650]  eta: 0:05:15  lr: 0.000020  loss: 26.7422  time: 0.4699  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [8050/8650]  eta: 0:04:51  lr: 0.000020  loss: 20.6335  time: 0.4702  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8100/8650]  eta: 0:04:27  lr: 0.000020  loss: 27.0721  time: 0.4801  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8150/8650]  eta: 0:04:02  lr: 0.000020  loss: 27.7795  time: 0.4678  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [8200/8650]  eta: 0:03:38  lr: 0.000020  loss: 20.8287  time: 0.4843  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [8250/8650]  eta: 0:03:14  lr: 0.000020  loss: 14.3124  time: 0.4841  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8300/8650]  eta: 0:02:49  lr: 0.000020  loss: 22.1478  time: 0.4854  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8350/8650]  eta: 0:02:25  lr: 0.000020  loss: 17.8465  time: 0.4838  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8400/8650]  eta: 0:02:01  lr: 0.000020  loss: 42.7810  time: 0.4871  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [2]  [8450/8650]  eta: 0:01:37  lr: 0.000020  loss: 19.0958  time: 0.4774  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8500/8650]  eta: 0:01:12  lr: 0.000020  loss: 18.9799  time: 0.4925  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [2]  [8550/8650]  eta: 0:00:48  lr: 0.000020  loss: 36.5626  time: 0.4939  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [2]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 25.2706  time: 0.4894  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [2]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 14.4837  time: 0.5120  data: 0.0023  max mem: 9086\n",
            "Train Epoch: [2] Total time: 1:09:58 (0.4854 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 21.8291\n",
            "Loss: 19.6089\n",
            "==========================================\n",
            "Train Epoch: [3]  [   0/8650]  eta: 4:31:26  lr: 0.000020  loss: 19.6089  time: 1.8828  data: 1.2469  max mem: 9086\n",
            "Loss: 28.7072\n",
            "==========================================\n",
            "Loss: 23.2962\n",
            "==========================================\n",
            "Loss: 29.3774\n",
            "==========================================\n",
            "Loss: 18.6624\n",
            "==========================================\n",
            "Loss: 12.4308\n",
            "==========================================\n",
            "Loss: 24.8697\n",
            "==========================================\n",
            "Loss: 9.6817\n",
            "==========================================\n",
            "Loss: 10.9194\n",
            "==========================================\n",
            "Loss: 20.4439\n",
            "==========================================\n",
            "Loss: 21.2823\n",
            "==========================================\n",
            "Loss: 12.1539\n",
            "==========================================\n",
            "Loss: 18.9392\n",
            "==========================================\n",
            "Loss: 23.5133\n",
            "==========================================\n",
            "Loss: 16.0815\n",
            "==========================================\n",
            "Loss: 14.6606\n",
            "==========================================\n",
            "Loss: 25.5036\n",
            "==========================================\n",
            "Loss: 17.6793\n",
            "==========================================\n",
            "Loss: 16.9793\n",
            "==========================================\n",
            "Loss: 34.1287\n",
            "==========================================\n",
            "Loss: 16.027\n",
            "==========================================\n",
            "Loss: 19.3753\n",
            "==========================================\n",
            "Loss: 19.0314\n",
            "==========================================\n",
            "Loss: 25.1697\n",
            "==========================================\n",
            "Loss: 27.0098\n",
            "==========================================\n",
            "Loss: 17.5787\n",
            "==========================================\n",
            "Loss: 29.6663\n",
            "==========================================\n",
            "Loss: 27.6868\n",
            "==========================================\n",
            "Loss: 17.392\n",
            "==========================================\n",
            "Loss: 15.0832\n",
            "==========================================\n",
            "Loss: 20.0602\n",
            "==========================================\n",
            "Loss: 37.875\n",
            "==========================================\n",
            "Loss: 21.151\n",
            "==========================================\n",
            "Loss: 14.6948\n",
            "==========================================\n",
            "Loss: 13.6557\n",
            "==========================================\n",
            "Loss: 17.8474\n",
            "==========================================\n",
            "Loss: 18.058\n",
            "==========================================\n",
            "Loss: 17.5923\n",
            "==========================================\n",
            "Loss: 28.5652\n",
            "==========================================\n",
            "Loss: 21.808\n",
            "==========================================\n",
            "Loss: 20.7396\n",
            "==========================================\n",
            "Loss: 16.5394\n",
            "==========================================\n",
            "Loss: 22.2349\n",
            "==========================================\n",
            "Loss: 33.2628\n",
            "==========================================\n",
            "Loss: 11.9715\n",
            "==========================================\n",
            "Loss: 26.03\n",
            "==========================================\n",
            "Loss: 22.6692\n",
            "==========================================\n",
            "Loss: 21.9544\n",
            "==========================================\n",
            "Loss: 12.3468\n",
            "==========================================\n",
            "Loss: 20.5349\n",
            "==========================================\n",
            "Train Epoch: [3]  [  50/8650]  eta: 1:28:35  lr: 0.000020  loss: 39.8198  time: 0.6487  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [3]  [ 100/8650]  eta: 1:19:29  lr: 0.000020  loss: 26.1111  time: 0.4956  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 150/8650]  eta: 1:15:23  lr: 0.000020  loss: 16.0852  time: 0.4850  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 200/8650]  eta: 1:12:57  lr: 0.000020  loss: 24.8746  time: 0.4780  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 250/8650]  eta: 1:11:14  lr: 0.000020  loss: 24.1433  time: 0.4636  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 300/8650]  eta: 1:10:03  lr: 0.000020  loss: 23.3481  time: 0.4639  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 350/8650]  eta: 1:09:05  lr: 0.000020  loss: 19.3698  time: 0.4584  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [ 400/8650]  eta: 1:08:21  lr: 0.000020  loss: 9.1068  time: 0.4647  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [3]  [ 450/8650]  eta: 1:07:38  lr: 0.000020  loss: 22.6186  time: 0.4530  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 500/8650]  eta: 1:06:58  lr: 0.000020  loss: 18.3626  time: 0.4307  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [ 550/8650]  eta: 1:06:21  lr: 0.000020  loss: 17.4186  time: 0.4255  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 600/8650]  eta: 1:05:46  lr: 0.000020  loss: 10.2891  time: 0.4181  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 650/8650]  eta: 1:05:14  lr: 0.000020  loss: 23.4872  time: 0.4039  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 700/8650]  eta: 1:04:41  lr: 0.000020  loss: 21.3439  time: 0.3953  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 750/8650]  eta: 1:04:12  lr: 0.000020  loss: 20.4008  time: 0.4033  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 800/8650]  eta: 1:03:43  lr: 0.000020  loss: 22.4207  time: 0.3937  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 850/8650]  eta: 1:03:15  lr: 0.000020  loss: 24.2646  time: 0.3959  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [ 900/8650]  eta: 1:02:46  lr: 0.000020  loss: 17.6223  time: 0.3944  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [ 950/8650]  eta: 1:02:18  lr: 0.000020  loss: 19.8883  time: 0.3937  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1000/8650]  eta: 1:01:50  lr: 0.000020  loss: 24.2201  time: 0.3943  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1050/8650]  eta: 1:01:23  lr: 0.000020  loss: 13.4294  time: 0.4000  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1100/8650]  eta: 1:00:57  lr: 0.000020  loss: 16.8049  time: 0.4080  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1150/8650]  eta: 1:00:31  lr: 0.000020  loss: 16.4902  time: 0.3948  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1200/8650]  eta: 1:00:05  lr: 0.000020  loss: 11.6093  time: 0.3945  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1250/8650]  eta: 0:59:40  lr: 0.000020  loss: 24.7252  time: 0.4055  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1300/8650]  eta: 0:59:18  lr: 0.000020  loss: 32.1684  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1350/8650]  eta: 0:58:53  lr: 0.000020  loss: 22.6381  time: 0.3978  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1400/8650]  eta: 0:58:28  lr: 0.000020  loss: 30.5079  time: 0.4019  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [1450/8650]  eta: 0:58:03  lr: 0.000020  loss: 22.7810  time: 0.3963  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1500/8650]  eta: 0:57:38  lr: 0.000020  loss: 33.5802  time: 0.3966  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1550/8650]  eta: 0:57:13  lr: 0.000020  loss: 14.1495  time: 0.3959  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1600/8650]  eta: 0:56:48  lr: 0.000020  loss: 21.4025  time: 0.3992  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1650/8650]  eta: 0:56:23  lr: 0.000020  loss: 20.0984  time: 0.4017  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1700/8650]  eta: 0:55:59  lr: 0.000020  loss: 26.2302  time: 0.4053  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1750/8650]  eta: 0:55:34  lr: 0.000020  loss: 19.3077  time: 0.3979  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1800/8650]  eta: 0:55:09  lr: 0.000020  loss: 17.3304  time: 0.3987  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1850/8650]  eta: 0:54:44  lr: 0.000020  loss: 30.9992  time: 0.4016  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1900/8650]  eta: 0:54:19  lr: 0.000020  loss: 12.8047  time: 0.4060  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [1950/8650]  eta: 0:53:53  lr: 0.000020  loss: 15.8736  time: 0.4067  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2000/8650]  eta: 0:53:28  lr: 0.000020  loss: 28.2956  time: 0.4111  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2050/8650]  eta: 0:53:05  lr: 0.000020  loss: 32.5037  time: 0.4183  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2100/8650]  eta: 0:52:42  lr: 0.000020  loss: 18.1520  time: 0.4131  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2150/8650]  eta: 0:52:18  lr: 0.000020  loss: 32.9616  time: 0.4067  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2200/8650]  eta: 0:51:54  lr: 0.000020  loss: 17.9501  time: 0.4098  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2250/8650]  eta: 0:51:29  lr: 0.000020  loss: 17.3497  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [2300/8650]  eta: 0:51:06  lr: 0.000020  loss: 23.4581  time: 0.4054  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2350/8650]  eta: 0:50:42  lr: 0.000020  loss: 21.1485  time: 0.4053  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2400/8650]  eta: 0:50:18  lr: 0.000020  loss: 36.1766  time: 0.4024  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2450/8650]  eta: 0:49:54  lr: 0.000020  loss: 26.1741  time: 0.3997  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2500/8650]  eta: 0:49:30  lr: 0.000020  loss: 36.3357  time: 0.4033  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2550/8650]  eta: 0:49:05  lr: 0.000020  loss: 14.5308  time: 0.3984  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2600/8650]  eta: 0:48:40  lr: 0.000020  loss: 21.0431  time: 0.4046  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2650/8650]  eta: 0:48:17  lr: 0.000020  loss: 26.2812  time: 0.4021  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2700/8650]  eta: 0:47:52  lr: 0.000020  loss: 20.7054  time: 0.4002  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2750/8650]  eta: 0:47:29  lr: 0.000020  loss: 23.4867  time: 0.3956  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2800/8650]  eta: 0:47:05  lr: 0.000020  loss: 15.2385  time: 0.4022  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2850/8650]  eta: 0:46:41  lr: 0.000020  loss: 25.2625  time: 0.3978  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2900/8650]  eta: 0:46:17  lr: 0.000020  loss: 18.9691  time: 0.4078  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [2950/8650]  eta: 0:45:53  lr: 0.000020  loss: 26.2932  time: 0.4005  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3000/8650]  eta: 0:45:28  lr: 0.000020  loss: 20.5335  time: 0.3981  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3050/8650]  eta: 0:45:04  lr: 0.000020  loss: 17.3782  time: 0.4030  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3100/8650]  eta: 0:44:41  lr: 0.000020  loss: 22.8045  time: 0.4167  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3150/8650]  eta: 0:44:17  lr: 0.000020  loss: 24.8518  time: 0.4208  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3200/8650]  eta: 0:43:53  lr: 0.000020  loss: 26.2357  time: 0.4126  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3250/8650]  eta: 0:43:29  lr: 0.000020  loss: 33.1178  time: 0.4241  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [3300/8650]  eta: 0:43:04  lr: 0.000020  loss: 17.0399  time: 0.4203  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3350/8650]  eta: 0:42:40  lr: 0.000020  loss: 43.9126  time: 0.4234  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3400/8650]  eta: 0:42:16  lr: 0.000020  loss: 17.7728  time: 0.4167  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3450/8650]  eta: 0:41:52  lr: 0.000020  loss: 26.9128  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3500/8650]  eta: 0:41:27  lr: 0.000020  loss: 23.6383  time: 0.4043  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3550/8650]  eta: 0:41:03  lr: 0.000020  loss: 19.2553  time: 0.4054  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3600/8650]  eta: 0:40:38  lr: 0.000020  loss: 27.8970  time: 0.4083  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3650/8650]  eta: 0:40:14  lr: 0.000020  loss: 18.4425  time: 0.4037  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3700/8650]  eta: 0:39:50  lr: 0.000020  loss: 18.8351  time: 0.4113  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3750/8650]  eta: 0:39:25  lr: 0.000020  loss: 15.9258  time: 0.4098  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3800/8650]  eta: 0:39:01  lr: 0.000020  loss: 20.9612  time: 0.4123  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3850/8650]  eta: 0:38:37  lr: 0.000020  loss: 38.2474  time: 0.4188  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [3900/8650]  eta: 0:38:12  lr: 0.000020  loss: 27.4093  time: 0.4039  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [3950/8650]  eta: 0:37:48  lr: 0.000020  loss: 17.5539  time: 0.4070  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4000/8650]  eta: 0:37:24  lr: 0.000020  loss: 19.9172  time: 0.4135  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [4050/8650]  eta: 0:36:59  lr: 0.000020  loss: 14.8258  time: 0.4108  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4100/8650]  eta: 0:36:35  lr: 0.000020  loss: 19.3086  time: 0.4114  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4150/8650]  eta: 0:36:11  lr: 0.000020  loss: 24.9015  time: 0.4050  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4200/8650]  eta: 0:35:46  lr: 0.000020  loss: 22.9704  time: 0.4038  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4250/8650]  eta: 0:35:22  lr: 0.000020  loss: 14.1597  time: 0.4036  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4300/8650]  eta: 0:34:58  lr: 0.000020  loss: 33.5811  time: 0.4100  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4350/8650]  eta: 0:34:33  lr: 0.000020  loss: 14.1920  time: 0.4103  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4400/8650]  eta: 0:34:09  lr: 0.000020  loss: 25.8344  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4450/8650]  eta: 0:33:45  lr: 0.000020  loss: 23.5756  time: 0.4098  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4500/8650]  eta: 0:33:21  lr: 0.000020  loss: 28.0882  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4550/8650]  eta: 0:32:57  lr: 0.000020  loss: 14.7157  time: 0.4216  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4600/8650]  eta: 0:32:33  lr: 0.000020  loss: 38.2125  time: 0.4157  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4650/8650]  eta: 0:32:09  lr: 0.000020  loss: 15.7320  time: 0.4248  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4700/8650]  eta: 0:31:45  lr: 0.000020  loss: 28.4674  time: 0.4206  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4750/8650]  eta: 0:31:21  lr: 0.000020  loss: 31.4488  time: 0.4145  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [4800/8650]  eta: 0:30:57  lr: 0.000020  loss: 19.2305  time: 0.4265  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4850/8650]  eta: 0:30:32  lr: 0.000020  loss: 20.7254  time: 0.4139  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4900/8650]  eta: 0:30:08  lr: 0.000020  loss: 31.1350  time: 0.4220  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [4950/8650]  eta: 0:29:44  lr: 0.000020  loss: 18.3938  time: 0.4218  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5000/8650]  eta: 0:29:20  lr: 0.000020  loss: 14.9017  time: 0.4298  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5050/8650]  eta: 0:28:56  lr: 0.000020  loss: 19.0212  time: 0.4200  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5100/8650]  eta: 0:28:31  lr: 0.000020  loss: 20.4151  time: 0.4221  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5150/8650]  eta: 0:28:07  lr: 0.000020  loss: 44.8245  time: 0.4162  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5200/8650]  eta: 0:27:43  lr: 0.000020  loss: 16.8196  time: 0.4147  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5250/8650]  eta: 0:27:19  lr: 0.000020  loss: 24.6476  time: 0.4190  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5300/8650]  eta: 0:26:55  lr: 0.000020  loss: 16.7823  time: 0.4181  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5350/8650]  eta: 0:26:31  lr: 0.000020  loss: 18.9774  time: 0.4173  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5400/8650]  eta: 0:26:07  lr: 0.000020  loss: 15.2866  time: 0.4092  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5450/8650]  eta: 0:25:43  lr: 0.000020  loss: 23.1636  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5500/8650]  eta: 0:25:19  lr: 0.000020  loss: 30.2001  time: 0.4177  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5550/8650]  eta: 0:24:55  lr: 0.000020  loss: 22.3411  time: 0.4152  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5600/8650]  eta: 0:24:31  lr: 0.000020  loss: 16.4408  time: 0.4241  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5650/8650]  eta: 0:24:06  lr: 0.000020  loss: 17.4072  time: 0.4214  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5700/8650]  eta: 0:23:42  lr: 0.000020  loss: 17.4916  time: 0.4196  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5750/8650]  eta: 0:23:18  lr: 0.000020  loss: 14.1639  time: 0.4261  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [5800/8650]  eta: 0:22:54  lr: 0.000020  loss: 21.5745  time: 0.4230  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [5850/8650]  eta: 0:22:30  lr: 0.000020  loss: 15.2252  time: 0.4448  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [5900/8650]  eta: 0:22:06  lr: 0.000020  loss: 14.3274  time: 0.4324  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [5950/8650]  eta: 0:21:42  lr: 0.000020  loss: 22.2436  time: 0.4302  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6000/8650]  eta: 0:21:18  lr: 0.000020  loss: 20.9935  time: 0.4356  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6050/8650]  eta: 0:20:54  lr: 0.000020  loss: 18.2347  time: 0.4406  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6100/8650]  eta: 0:20:30  lr: 0.000020  loss: 24.8378  time: 0.4296  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6150/8650]  eta: 0:20:06  lr: 0.000020  loss: 19.3827  time: 0.4182  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6200/8650]  eta: 0:19:42  lr: 0.000020  loss: 14.7582  time: 0.4136  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6250/8650]  eta: 0:19:17  lr: 0.000020  loss: 17.9009  time: 0.4108  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6300/8650]  eta: 0:18:53  lr: 0.000020  loss: 34.5024  time: 0.4267  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6350/8650]  eta: 0:18:29  lr: 0.000020  loss: 21.2634  time: 0.4317  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [6400/8650]  eta: 0:18:05  lr: 0.000020  loss: 26.0872  time: 0.4205  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6450/8650]  eta: 0:17:41  lr: 0.000020  loss: 16.3390  time: 0.4274  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6500/8650]  eta: 0:17:17  lr: 0.000020  loss: 28.3923  time: 0.4409  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6550/8650]  eta: 0:16:53  lr: 0.000020  loss: 16.4090  time: 0.4381  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6600/8650]  eta: 0:16:29  lr: 0.000020  loss: 25.6794  time: 0.4429  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6650/8650]  eta: 0:16:05  lr: 0.000020  loss: 15.1077  time: 0.4429  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6700/8650]  eta: 0:15:41  lr: 0.000020  loss: 21.8213  time: 0.4670  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6750/8650]  eta: 0:15:17  lr: 0.000020  loss: 14.8677  time: 0.4595  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6800/8650]  eta: 0:14:52  lr: 0.000020  loss: 18.3105  time: 0.4694  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [6850/8650]  eta: 0:14:28  lr: 0.000020  loss: 17.5540  time: 0.4572  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [6900/8650]  eta: 0:14:04  lr: 0.000020  loss: 23.6409  time: 0.4591  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [3]  [6950/8650]  eta: 0:13:40  lr: 0.000020  loss: 19.3066  time: 0.4478  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7000/8650]  eta: 0:13:16  lr: 0.000020  loss: 13.6403  time: 0.4390  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7050/8650]  eta: 0:12:52  lr: 0.000020  loss: 22.8851  time: 0.4329  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [7100/8650]  eta: 0:12:28  lr: 0.000020  loss: 16.8957  time: 0.4315  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7150/8650]  eta: 0:12:03  lr: 0.000020  loss: 25.2059  time: 0.4289  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7200/8650]  eta: 0:11:39  lr: 0.000020  loss: 13.0499  time: 0.4268  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7250/8650]  eta: 0:11:15  lr: 0.000020  loss: 23.1814  time: 0.4199  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7300/8650]  eta: 0:10:51  lr: 0.000020  loss: 21.6755  time: 0.4290  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7350/8650]  eta: 0:10:27  lr: 0.000020  loss: 20.2486  time: 0.4205  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7400/8650]  eta: 0:10:03  lr: 0.000020  loss: 17.4972  time: 0.4134  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7450/8650]  eta: 0:09:39  lr: 0.000020  loss: 20.9969  time: 0.4124  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7500/8650]  eta: 0:09:15  lr: 0.000020  loss: 34.8755  time: 0.4074  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7550/8650]  eta: 0:08:50  lr: 0.000020  loss: 18.1990  time: 0.4097  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [7600/8650]  eta: 0:08:26  lr: 0.000020  loss: 22.1834  time: 0.4135  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7650/8650]  eta: 0:08:02  lr: 0.000020  loss: 23.9129  time: 0.4161  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7700/8650]  eta: 0:07:38  lr: 0.000020  loss: 19.9888  time: 0.4068  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7750/8650]  eta: 0:07:14  lr: 0.000020  loss: 28.8108  time: 0.4109  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7800/8650]  eta: 0:06:50  lr: 0.000020  loss: 23.1440  time: 0.4222  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7850/8650]  eta: 0:06:26  lr: 0.000020  loss: 17.2700  time: 0.4214  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7900/8650]  eta: 0:06:02  lr: 0.000020  loss: 15.5459  time: 0.4176  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [7950/8650]  eta: 0:05:38  lr: 0.000020  loss: 28.5008  time: 0.4163  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8000/8650]  eta: 0:05:13  lr: 0.000020  loss: 22.1876  time: 0.4068  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8050/8650]  eta: 0:04:49  lr: 0.000020  loss: 24.3288  time: 0.4037  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8100/8650]  eta: 0:04:25  lr: 0.000020  loss: 16.9213  time: 0.4085  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8150/8650]  eta: 0:04:01  lr: 0.000020  loss: 24.7060  time: 0.4008  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8200/8650]  eta: 0:03:37  lr: 0.000020  loss: 15.2431  time: 0.4197  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8250/8650]  eta: 0:03:13  lr: 0.000020  loss: 17.4936  time: 0.4031  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8300/8650]  eta: 0:02:48  lr: 0.000020  loss: 27.1732  time: 0.4025  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8350/8650]  eta: 0:02:24  lr: 0.000020  loss: 25.2996  time: 0.4012  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8400/8650]  eta: 0:02:00  lr: 0.000020  loss: 11.5288  time: 0.4050  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8450/8650]  eta: 0:01:36  lr: 0.000020  loss: 14.5047  time: 0.3991  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [3]  [8500/8650]  eta: 0:01:12  lr: 0.000020  loss: 24.5020  time: 0.4041  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8550/8650]  eta: 0:00:48  lr: 0.000020  loss: 15.6698  time: 0.4064  data: 0.0002  max mem: 9086\n",
            "Train Epoch: [3]  [8600/8650]  eta: 0:00:24  lr: 0.000020  loss: 13.8819  time: 0.4135  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [3]  [8649/8650]  eta: 0:00:00  lr: 0.000020  loss: 12.7587  time: 0.3968  data: 0.0019  max mem: 9086\n",
            "Train Epoch: [3] Total time: 1:09:36 (0.4829 s / it)\n",
            "Averaged stats: lr: 0.0000  loss: 21.3628\n",
            "Loss: 18.5198\n",
            "==========================================\n",
            "Train Epoch: [4]  [   0/8650]  eta: 5:10:59  lr: 0.000019  loss: 18.5198  time: 2.1572  data: 1.2738  max mem: 9086\n",
            "Loss: 16.5065\n",
            "==========================================\n",
            "Loss: 18.0017\n",
            "==========================================\n",
            "Loss: 13.3822\n",
            "==========================================\n",
            "Loss: 33.1169\n",
            "==========================================\n",
            "Loss: 22.7271\n",
            "==========================================\n",
            "Loss: 12.3256\n",
            "==========================================\n",
            "Loss: 19.3067\n",
            "==========================================\n",
            "Loss: 31.997\n",
            "==========================================\n",
            "Loss: 26.323\n",
            "==========================================\n",
            "Loss: 21.7696\n",
            "==========================================\n",
            "Loss: 24.1912\n",
            "==========================================\n",
            "Loss: 12.5552\n",
            "==========================================\n",
            "Loss: 16.7061\n",
            "==========================================\n",
            "Loss: 10.9904\n",
            "==========================================\n",
            "Loss: 17.9443\n",
            "==========================================\n",
            "Loss: 26.8903\n",
            "==========================================\n",
            "Loss: 19.9118\n",
            "==========================================\n",
            "Loss: 18.679\n",
            "==========================================\n",
            "Loss: 18.1486\n",
            "==========================================\n",
            "Loss: 20.2055\n",
            "==========================================\n",
            "Loss: 20.4086\n",
            "==========================================\n",
            "Loss: 33.3101\n",
            "==========================================\n",
            "Loss: 12.9444\n",
            "==========================================\n",
            "Loss: 13.8469\n",
            "==========================================\n",
            "Loss: 26.6386\n",
            "==========================================\n",
            "Loss: 14.8553\n",
            "==========================================\n",
            "Loss: 19.7982\n",
            "==========================================\n",
            "Loss: 31.0536\n",
            "==========================================\n",
            "Loss: 21.8713\n",
            "==========================================\n",
            "Loss: 43.7067\n",
            "==========================================\n",
            "Loss: 16.8581\n",
            "==========================================\n",
            "Loss: 13.1076\n",
            "==========================================\n",
            "Loss: 12.6602\n",
            "==========================================\n",
            "Loss: 25.2958\n",
            "==========================================\n",
            "Loss: 14.4593\n",
            "==========================================\n",
            "Loss: 16.1068\n",
            "==========================================\n",
            "Loss: 22.755\n",
            "==========================================\n",
            "Loss: 15.187\n",
            "==========================================\n",
            "Loss: 16.972\n",
            "==========================================\n",
            "Loss: 18.5301\n",
            "==========================================\n",
            "Loss: 20.2051\n",
            "==========================================\n",
            "Loss: 13.3034\n",
            "==========================================\n",
            "Loss: 12.9685\n",
            "==========================================\n",
            "Loss: 13.7839\n",
            "==========================================\n",
            "Loss: 28.4485\n",
            "==========================================\n",
            "Loss: 19.6185\n",
            "==========================================\n",
            "Loss: 16.4477\n",
            "==========================================\n",
            "Loss: 28.2101\n",
            "==========================================\n",
            "Loss: 21.6304\n",
            "==========================================\n",
            "Train Epoch: [4]  [  50/8650]  eta: 1:28:18  lr: 0.000019  loss: 24.8938  time: 0.5781  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [4]  [ 100/8650]  eta: 1:20:15  lr: 0.000019  loss: 19.3596  time: 0.5095  data: 0.0005  max mem: 9086\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 273, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 220, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device, max_debug_batches=50) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/.shortcut-targets-by-id/149DoVxxntvzUjp2fuVxODPJ1RQguvoHn/BLIP/train_vqa.py\", line 63, in train\n",
            "    metric_logger.update(loss=loss.item())\n",
            "                              ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_animals.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_animals \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA_pretrain_animals/checkpoint_01.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Textvqa + PretrainTextCaps"
      ],
      "metadata": {
        "id": "c1Xp2adpkVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa_Textcaps.yaml \\\n",
        "    --output_dir ./output/textVQA_pretrain_textcaps \\\n",
        "    --device cuda \\\n",
        "    --distributed False"
      ],
      "metadata": {
        "id": "9RnYLAjmkcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA + resnet"
      ],
      "metadata": {
        "id": "ZU3eOQrIfwJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrain**"
      ],
      "metadata": {
        "id": "9DdJzq5-_a8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d71c10"
      },
      "source": [
        "Nếu bạn đã mount Google Drive của mình, bạn có thể sử dụng lệnh sau để giải nén một tệp tin zip từ Drive vào Colab. Hãy thay thế `'/content/drive/MyDrive/path/to/your_file.zip'` bằng đường dẫn thực tế đến tệp zip của bạn và `'/content/destination_folder'` bằng thư mục bạn muốn giải nén đến."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2581be04"
      },
      "source": [
        "# Ví dụ: Giải nén một tệp tin zip từ Google Drive\n",
        "# Tạo thư mục đích nếu nó chưa tồn tại\n",
        "!mkdir -p /content/dataset_animals\n",
        "\n",
        "# Giải nén tệp tin\n",
        "!unzip -q '/content/drive/MyDrive/Datasets_BLIP/coco_animals_blip_ready.zip' -d '/content/dataset_animals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/dataset_animals"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ekwts1oGwoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain.py --config configs/pre_animals.yaml --output_dir output/pretrain_animals --checkpoint output/pretrain_animals/checkpoint_22.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5gFi5qxQmH",
        "outputId": "41cbfa9f-fb90-4d33-abf7-62afd1e11954",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating dataset\n",
            "Đang tải dữ liệu từ: /content/dataset_animals/dataset.json\n",
            "number of training samples: 20000\n",
            "Creating model\n",
            "/embeddings/word_embeddings is tied\n",
            "/embeddings/position_embeddings is tied\n",
            "/embeddings/LayerNorm is tied\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "/encoder/layer/2/crossattention/self/query is tied\n",
            "/encoder/layer/2/crossattention/self/key is tied\n",
            "/encoder/layer/2/crossattention/self/value is tied\n",
            "/encoder/layer/2/crossattention/output/dense is tied\n",
            "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/2/intermediate/dense is tied\n",
            "/encoder/layer/2/output/dense is tied\n",
            "/encoder/layer/2/output/LayerNorm is tied\n",
            "/encoder/layer/3/crossattention/self/query is tied\n",
            "/encoder/layer/3/crossattention/self/key is tied\n",
            "/encoder/layer/3/crossattention/self/value is tied\n",
            "/encoder/layer/3/crossattention/output/dense is tied\n",
            "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/3/intermediate/dense is tied\n",
            "/encoder/layer/3/output/dense is tied\n",
            "/encoder/layer/3/output/LayerNorm is tied\n",
            "/encoder/layer/4/crossattention/self/query is tied\n",
            "/encoder/layer/4/crossattention/self/key is tied\n",
            "/encoder/layer/4/crossattention/self/value is tied\n",
            "/encoder/layer/4/crossattention/output/dense is tied\n",
            "/encoder/layer/4/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/4/intermediate/dense is tied\n",
            "/encoder/layer/4/output/dense is tied\n",
            "/encoder/layer/4/output/LayerNorm is tied\n",
            "/encoder/layer/5/crossattention/self/query is tied\n",
            "/encoder/layer/5/crossattention/self/key is tied\n",
            "/encoder/layer/5/crossattention/self/value is tied\n",
            "/encoder/layer/5/crossattention/output/dense is tied\n",
            "/encoder/layer/5/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/5/intermediate/dense is tied\n",
            "/encoder/layer/5/output/dense is tied\n",
            "/encoder/layer/5/output/LayerNorm is tied\n",
            "/encoder/layer/6/crossattention/self/query is tied\n",
            "/encoder/layer/6/crossattention/self/key is tied\n",
            "/encoder/layer/6/crossattention/self/value is tied\n",
            "/encoder/layer/6/crossattention/output/dense is tied\n",
            "/encoder/layer/6/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/6/intermediate/dense is tied\n",
            "/encoder/layer/6/output/dense is tied\n",
            "/encoder/layer/6/output/LayerNorm is tied\n",
            "/encoder/layer/7/crossattention/self/query is tied\n",
            "/encoder/layer/7/crossattention/self/key is tied\n",
            "/encoder/layer/7/crossattention/self/value is tied\n",
            "/encoder/layer/7/crossattention/output/dense is tied\n",
            "/encoder/layer/7/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/7/intermediate/dense is tied\n",
            "/encoder/layer/7/output/dense is tied\n",
            "/encoder/layer/7/output/LayerNorm is tied\n",
            "/encoder/layer/8/crossattention/self/query is tied\n",
            "/encoder/layer/8/crossattention/self/key is tied\n",
            "/encoder/layer/8/crossattention/self/value is tied\n",
            "/encoder/layer/8/crossattention/output/dense is tied\n",
            "/encoder/layer/8/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/8/intermediate/dense is tied\n",
            "/encoder/layer/8/output/dense is tied\n",
            "/encoder/layer/8/output/LayerNorm is tied\n",
            "/encoder/layer/9/crossattention/self/query is tied\n",
            "/encoder/layer/9/crossattention/self/key is tied\n",
            "/encoder/layer/9/crossattention/self/value is tied\n",
            "/encoder/layer/9/crossattention/output/dense is tied\n",
            "/encoder/layer/9/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/9/intermediate/dense is tied\n",
            "/encoder/layer/9/output/dense is tied\n",
            "/encoder/layer/9/output/LayerNorm is tied\n",
            "/encoder/layer/10/crossattention/self/query is tied\n",
            "/encoder/layer/10/crossattention/self/key is tied\n",
            "/encoder/layer/10/crossattention/self/value is tied\n",
            "/encoder/layer/10/crossattention/output/dense is tied\n",
            "/encoder/layer/10/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/10/intermediate/dense is tied\n",
            "/encoder/layer/10/output/dense is tied\n",
            "/encoder/layer/10/output/LayerNorm is tied\n",
            "/encoder/layer/11/crossattention/self/query is tied\n",
            "/encoder/layer/11/crossattention/self/key is tied\n",
            "/encoder/layer/11/crossattention/self/value is tied\n",
            "/encoder/layer/11/crossattention/output/dense is tied\n",
            "/encoder/layer/11/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/11/intermediate/dense is tied\n",
            "/encoder/layer/11/output/dense is tied\n",
            "/encoder/layer/11/output/LayerNorm is tied\n",
            "resume checkpoint from output/pretrain_animals/checkpoint_16.pth\n",
            "Start training\n",
            "Train Epoch: [17]  [   0/2500]  eta: 3:19:38  lr: 0.000050  loss_ita: 6.1392  loss_itm: 0.6385  loss_lm: 4.7927  time: 4.7912  data: 1.5059  max mem: 7121\n",
            "Train Epoch: [17]  [  50/2500]  eta: 0:39:44  lr: 0.000050  loss_ita: 5.9841  loss_itm: 0.6401  loss_lm: 4.4402  time: 0.8945  data: 0.0004  max mem: 7129\n",
            "Train Epoch: [17]  [ 100/2500]  eta: 0:38:02  lr: 0.000050  loss_ita: 5.9510  loss_itm: 0.6436  loss_lm: 4.7203  time: 0.9362  data: 0.0003  max mem: 7129\n",
            "Train Epoch: [17]  [ 150/2500]  eta: 0:36:56  lr: 0.000050  loss_ita: 5.8191  loss_itm: 0.6290  loss_lm: 4.2306  time: 0.9299  data: 0.0006  max mem: 7132\n",
            "Train Epoch: [17]  [ 200/2500]  eta: 0:36:00  lr: 0.000050  loss_ita: 5.8432  loss_itm: 0.6357  loss_lm: 4.4590  time: 0.9304  data: 0.0005  max mem: 7132\n",
            "Train Epoch: [17]  [ 250/2500]  eta: 0:35:06  lr: 0.000050  loss_ita: 6.0463  loss_itm: 0.6259  loss_lm: 4.6369  time: 0.9230  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 300/2500]  eta: 0:34:17  lr: 0.000050  loss_ita: 5.9681  loss_itm: 0.6204  loss_lm: 4.2531  time: 0.9307  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 350/2500]  eta: 0:33:28  lr: 0.000050  loss_ita: 5.8556  loss_itm: 0.6375  loss_lm: 4.7831  time: 0.9301  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 400/2500]  eta: 0:32:40  lr: 0.000050  loss_ita: 6.1228  loss_itm: 0.6355  loss_lm: 4.8984  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 450/2500]  eta: 0:31:51  lr: 0.000050  loss_ita: 6.3495  loss_itm: 0.6393  loss_lm: 4.6135  time: 0.9222  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 500/2500]  eta: 0:31:04  lr: 0.000050  loss_ita: 6.1634  loss_itm: 0.6500  loss_lm: 4.8925  time: 0.9304  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 550/2500]  eta: 0:30:17  lr: 0.000050  loss_ita: 5.9357  loss_itm: 0.6350  loss_lm: 4.1571  time: 0.9271  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 600/2500]  eta: 0:29:30  lr: 0.000050  loss_ita: 5.8463  loss_itm: 0.6394  loss_lm: 4.1982  time: 0.9227  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [ 650/2500]  eta: 0:28:43  lr: 0.000050  loss_ita: 6.0158  loss_itm: 0.6351  loss_lm: 4.7299  time: 0.9328  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 700/2500]  eta: 0:27:57  lr: 0.000050  loss_ita: 6.1428  loss_itm: 0.6314  loss_lm: 4.6872  time: 0.9312  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [ 750/2500]  eta: 0:27:10  lr: 0.000050  loss_ita: 6.2539  loss_itm: 0.6385  loss_lm: 4.5769  time: 0.9238  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 800/2500]  eta: 0:26:23  lr: 0.000050  loss_ita: 6.0947  loss_itm: 0.6348  loss_lm: 4.4382  time: 0.9340  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [ 850/2500]  eta: 0:25:36  lr: 0.000050  loss_ita: 5.9637  loss_itm: 0.6348  loss_lm: 4.9964  time: 0.9370  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 900/2500]  eta: 0:24:49  lr: 0.000050  loss_ita: 6.2077  loss_itm: 0.6503  loss_lm: 4.5971  time: 0.9215  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [ 950/2500]  eta: 0:24:03  lr: 0.000050  loss_ita: 6.2782  loss_itm: 0.6304  loss_lm: 4.1598  time: 0.9229  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1000/2500]  eta: 0:23:16  lr: 0.000050  loss_ita: 6.1663  loss_itm: 0.6368  loss_lm: 4.6378  time: 0.9374  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1050/2500]  eta: 0:22:30  lr: 0.000050  loss_ita: 6.2009  loss_itm: 0.6334  loss_lm: 4.4458  time: 0.9369  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1100/2500]  eta: 0:21:43  lr: 0.000050  loss_ita: 6.1823  loss_itm: 0.6355  loss_lm: 4.9945  time: 0.9254  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1150/2500]  eta: 0:20:56  lr: 0.000050  loss_ita: 6.0765  loss_itm: 0.6280  loss_lm: 4.5967  time: 0.9312  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1200/2500]  eta: 0:20:10  lr: 0.000050  loss_ita: 6.0619  loss_itm: 0.6410  loss_lm: 4.8511  time: 0.9367  data: 0.0006  max mem: 7133\n",
            "Train Epoch: [17]  [1250/2500]  eta: 0:19:23  lr: 0.000050  loss_ita: 6.0537  loss_itm: 0.6422  loss_lm: 5.3150  time: 0.9227  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1300/2500]  eta: 0:18:36  lr: 0.000050  loss_ita: 5.9791  loss_itm: 0.6471  loss_lm: 4.5806  time: 0.9206  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1350/2500]  eta: 0:17:50  lr: 0.000050  loss_ita: 5.7339  loss_itm: 0.6316  loss_lm: 4.2935  time: 0.9418  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1400/2500]  eta: 0:17:03  lr: 0.000050  loss_ita: 5.8574  loss_itm: 0.6248  loss_lm: 4.8669  time: 0.9227  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1450/2500]  eta: 0:16:17  lr: 0.000050  loss_ita: 5.8236  loss_itm: 0.6454  loss_lm: 4.4377  time: 0.9271  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1500/2500]  eta: 0:15:30  lr: 0.000050  loss_ita: 5.6852  loss_itm: 0.6495  loss_lm: 4.7744  time: 0.9329  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1550/2500]  eta: 0:14:44  lr: 0.000050  loss_ita: 5.4990  loss_itm: 0.6276  loss_lm: 4.4243  time: 0.9361  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1600/2500]  eta: 0:13:57  lr: 0.000050  loss_ita: 5.6231  loss_itm: 0.6459  loss_lm: 4.4868  time: 0.9241  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1650/2500]  eta: 0:13:10  lr: 0.000050  loss_ita: 5.7932  loss_itm: 0.6374  loss_lm: 4.5288  time: 0.9265  data: 0.0003  max mem: 7133\n",
            "Train Epoch: [17]  [1700/2500]  eta: 0:12:24  lr: 0.000050  loss_ita: 5.9085  loss_itm: 0.6344  loss_lm: 4.4765  time: 0.9344  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1750/2500]  eta: 0:11:37  lr: 0.000050  loss_ita: 6.1626  loss_itm: 0.6300  loss_lm: 4.4793  time: 0.9218  data: 0.0004  max mem: 7133\n",
            "Train Epoch: [17]  [1800/2500]  eta: 0:10:51  lr: 0.000050  loss_ita: 6.1059  loss_itm: 0.6371  loss_lm: 4.8303  time: 0.9242  data: 0.0005  max mem: 7133\n",
            "Train Epoch: [17]  [1850/2500]  eta: 0:10:04  lr: 0.000050  loss_ita: 5.7550  loss_itm: 0.6354  loss_lm: 4.7988  time: 0.9292  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [1900/2500]  eta: 0:09:18  lr: 0.000050  loss_ita: 5.6715  loss_itm: 0.6439  loss_lm: 4.9443  time: 0.9322  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [1950/2500]  eta: 0:08:31  lr: 0.000050  loss_ita: 5.9078  loss_itm: 0.6322  loss_lm: 4.9832  time: 0.9235  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2000/2500]  eta: 0:07:45  lr: 0.000050  loss_ita: 6.1245  loss_itm: 0.6394  loss_lm: 4.4271  time: 0.9317  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2050/2500]  eta: 0:06:58  lr: 0.000050  loss_ita: 6.3317  loss_itm: 0.6358  loss_lm: 4.5954  time: 0.9336  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2100/2500]  eta: 0:06:12  lr: 0.000050  loss_ita: 6.1765  loss_itm: 0.6298  loss_lm: 4.4585  time: 0.9223  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2150/2500]  eta: 0:05:25  lr: 0.000050  loss_ita: 5.8163  loss_itm: 0.6358  loss_lm: 4.6149  time: 0.9204  data: 0.0002  max mem: 7134\n",
            "Train Epoch: [17]  [2200/2500]  eta: 0:04:39  lr: 0.000050  loss_ita: 5.7070  loss_itm: 0.6437  loss_lm: 4.4813  time: 0.9407  data: 0.0005  max mem: 7134\n",
            "Train Epoch: [17]  [2250/2500]  eta: 0:03:52  lr: 0.000050  loss_ita: 5.9511  loss_itm: 0.6427  loss_lm: 4.4372  time: 0.9190  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2300/2500]  eta: 0:03:05  lr: 0.000050  loss_ita: 6.0164  loss_itm: 0.6342  loss_lm: 4.4307  time: 0.9265  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2350/2500]  eta: 0:02:19  lr: 0.000050  loss_ita: 5.9607  loss_itm: 0.6364  loss_lm: 4.5868  time: 0.9313  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2400/2500]  eta: 0:01:32  lr: 0.000050  loss_ita: 6.0198  loss_itm: 0.6424  loss_lm: 4.5925  time: 0.9271  data: 0.0004  max mem: 7134\n",
            "Train Epoch: [17]  [2450/2500]  eta: 0:00:46  lr: 0.000050  loss_ita: 6.2225  loss_itm: 0.6522  loss_lm: 4.5737  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17]  [2499/2500]  eta: 0:00:00  lr: 0.000050  loss_ita: 6.3974  loss_itm: 0.6369  loss_lm: 5.1242  time: 0.9197  data: 0.0003  max mem: 7134\n",
            "Train Epoch: [17] Total time: 0:38:44 (0.9298 s / it)\n",
            "Averaged stats: lr: 0.0001  loss_ita: 5.9935  loss_itm: 0.6356  loss_lm: 4.6061\n",
            "Train Epoch: [18]  [   0/2500]  eta: 1:20:04  lr: 0.000045  loss_ita: 6.2714  loss_itm: 0.6457  loss_lm: 4.7524  time: 1.9218  data: 0.8585  max mem: 7970\n",
            "Train Epoch: [18]  [  50/2500]  eta: 0:40:21  lr: 0.000045  loss_ita: 6.3983  loss_itm: 0.6511  loss_lm: 4.1244  time: 0.9882  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 100/2500]  eta: 0:38:50  lr: 0.000045  loss_ita: 6.3850  loss_itm: 0.6482  loss_lm: 4.3894  time: 0.9561  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 150/2500]  eta: 0:37:38  lr: 0.000045  loss_ita: 6.2264  loss_itm: 0.6374  loss_lm: 4.5178  time: 0.9203  data: 0.0005  max mem: 7982\n",
            "Train Epoch: [18]  [ 200/2500]  eta: 0:36:30  lr: 0.000045  loss_ita: 5.9527  loss_itm: 0.6499  loss_lm: 4.5807  time: 0.9236  data: 0.0003  max mem: 7982\n",
            "Train Epoch: [18]  [ 250/2500]  eta: 0:35:32  lr: 0.000045  loss_ita: 5.7489  loss_itm: 0.6327  loss_lm: 4.1563  time: 0.9307  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 300/2500]  eta: 0:34:36  lr: 0.000045  loss_ita: 5.7306  loss_itm: 0.6399  loss_lm: 4.6968  time: 0.9242  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 350/2500]  eta: 0:33:43  lr: 0.000045  loss_ita: 5.9946  loss_itm: 0.6380  loss_lm: 4.8402  time: 0.9228  data: 0.0003  max mem: 7983\n",
            "Train Epoch: [18]  [ 400/2500]  eta: 0:32:53  lr: 0.000045  loss_ita: 6.1702  loss_itm: 0.6330  loss_lm: 4.2296  time: 0.9339  data: 0.0004  max mem: 7983\n",
            "Train Epoch: [18]  [ 450/2500]  eta: 0:32:02  lr: 0.000045  loss_ita: 6.1112  loss_itm: 0.6402  loss_lm: 4.3074  time: 0.9217  data: 0.0002  max mem: 7983\n",
            "Train Epoch: [18]  [ 500/2500]  eta: 0:31:12  lr: 0.000045  loss_ita: 6.2875  loss_itm: 0.6396  loss_lm: 4.1692  time: 0.9172  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 550/2500]  eta: 0:30:24  lr: 0.000045  loss_ita: 6.1270  loss_itm: 0.6236  loss_lm: 4.2401  time: 0.9325  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 600/2500]  eta: 0:29:36  lr: 0.000045  loss_ita: 5.8813  loss_itm: 0.6358  loss_lm: 4.6269  time: 0.9270  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 650/2500]  eta: 0:28:47  lr: 0.000045  loss_ita: 5.8662  loss_itm: 0.6497  loss_lm: 4.5022  time: 0.9237  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 700/2500]  eta: 0:27:59  lr: 0.000045  loss_ita: 6.0300  loss_itm: 0.6378  loss_lm: 4.8883  time: 0.9273  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 750/2500]  eta: 0:27:12  lr: 0.000045  loss_ita: 6.1708  loss_itm: 0.6311  loss_lm: 4.5949  time: 0.9273  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [ 800/2500]  eta: 0:26:24  lr: 0.000045  loss_ita: 6.1851  loss_itm: 0.6380  loss_lm: 4.5566  time: 0.9163  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 850/2500]  eta: 0:25:36  lr: 0.000045  loss_ita: 6.0905  loss_itm: 0.6362  loss_lm: 4.4975  time: 0.9210  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [ 900/2500]  eta: 0:24:50  lr: 0.000045  loss_ita: 6.1650  loss_itm: 0.6281  loss_lm: 4.4557  time: 0.9308  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [ 950/2500]  eta: 0:24:02  lr: 0.000045  loss_ita: 6.4208  loss_itm: 0.6424  loss_lm: 4.9033  time: 0.9256  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1000/2500]  eta: 0:23:16  lr: 0.000045  loss_ita: 6.4365  loss_itm: 0.6313  loss_lm: 4.4134  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1050/2500]  eta: 0:22:29  lr: 0.000045  loss_ita: 6.1584  loss_itm: 0.6530  loss_lm: 4.5601  time: 0.9284  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1100/2500]  eta: 0:21:43  lr: 0.000045  loss_ita: 6.0874  loss_itm: 0.6354  loss_lm: 4.9366  time: 0.9305  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1150/2500]  eta: 0:20:56  lr: 0.000045  loss_ita: 5.9607  loss_itm: 0.6468  loss_lm: 4.2832  time: 0.9209  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1200/2500]  eta: 0:20:09  lr: 0.000045  loss_ita: 5.9632  loss_itm: 0.6471  loss_lm: 4.6797  time: 0.9294  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1250/2500]  eta: 0:19:22  lr: 0.000045  loss_ita: 6.1932  loss_itm: 0.6278  loss_lm: 4.3471  time: 0.9296  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1300/2500]  eta: 0:18:36  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6202  loss_lm: 4.5908  time: 0.9244  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1350/2500]  eta: 0:17:49  lr: 0.000045  loss_ita: 6.0786  loss_itm: 0.6392  loss_lm: 4.4718  time: 0.9198  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1400/2500]  eta: 0:17:02  lr: 0.000045  loss_ita: 6.0546  loss_itm: 0.6346  loss_lm: 4.4181  time: 0.9355  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1450/2500]  eta: 0:16:16  lr: 0.000045  loss_ita: 6.1391  loss_itm: 0.6276  loss_lm: 4.5114  time: 0.9284  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1500/2500]  eta: 0:15:29  lr: 0.000045  loss_ita: 5.7351  loss_itm: 0.6409  loss_lm: 4.5599  time: 0.9248  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [1550/2500]  eta: 0:14:43  lr: 0.000045  loss_ita: 5.2817  loss_itm: 0.6334  loss_lm: 4.4926  time: 0.9290  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1600/2500]  eta: 0:13:56  lr: 0.000045  loss_ita: 5.3891  loss_itm: 0.6325  loss_lm: 4.5553  time: 0.9378  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1650/2500]  eta: 0:13:10  lr: 0.000045  loss_ita: 5.8572  loss_itm: 0.6407  loss_lm: 4.6760  time: 0.9198  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1700/2500]  eta: 0:12:23  lr: 0.000045  loss_ita: 6.2208  loss_itm: 0.6292  loss_lm: 4.2864  time: 0.9233  data: 0.0005  max mem: 7989\n",
            "Train Epoch: [18]  [1750/2500]  eta: 0:11:37  lr: 0.000045  loss_ita: 6.4253  loss_itm: 0.6383  loss_lm: 4.3046  time: 0.9352  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1800/2500]  eta: 0:10:50  lr: 0.000045  loss_ita: 6.1586  loss_itm: 0.6361  loss_lm: 4.5963  time: 0.9234  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [1850/2500]  eta: 0:10:04  lr: 0.000045  loss_ita: 6.1716  loss_itm: 0.6387  loss_lm: 4.4412  time: 0.9251  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1900/2500]  eta: 0:09:17  lr: 0.000045  loss_ita: 6.0535  loss_itm: 0.6367  loss_lm: 4.3708  time: 0.9263  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [1950/2500]  eta: 0:08:31  lr: 0.000045  loss_ita: 5.9256  loss_itm: 0.6286  loss_lm: 4.6511  time: 0.9289  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2000/2500]  eta: 0:07:44  lr: 0.000045  loss_ita: 5.7240  loss_itm: 0.6361  loss_lm: 4.3443  time: 0.9228  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2050/2500]  eta: 0:06:58  lr: 0.000045  loss_ita: 5.7294  loss_itm: 0.6406  loss_lm: 4.5127  time: 0.9255  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2100/2500]  eta: 0:06:11  lr: 0.000045  loss_ita: 5.7598  loss_itm: 0.6372  loss_lm: 5.1710  time: 0.9374  data: 0.0006  max mem: 7989\n",
            "Train Epoch: [18]  [2150/2500]  eta: 0:05:25  lr: 0.000045  loss_ita: 6.0122  loss_itm: 0.6364  loss_lm: 4.6644  time: 0.9268  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2200/2500]  eta: 0:04:38  lr: 0.000045  loss_ita: 6.1487  loss_itm: 0.6409  loss_lm: 3.9354  time: 0.9197  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2250/2500]  eta: 0:03:52  lr: 0.000045  loss_ita: 6.3002  loss_itm: 0.6383  loss_lm: 4.6477  time: 0.9372  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2300/2500]  eta: 0:03:05  lr: 0.000045  loss_ita: 6.2824  loss_itm: 0.6529  loss_lm: 4.5341  time: 0.9280  data: 0.0004  max mem: 7989\n",
            "Train Epoch: [18]  [2350/2500]  eta: 0:02:19  lr: 0.000045  loss_ita: 6.1060  loss_itm: 0.6350  loss_lm: 4.9516  time: 0.9269  data: 0.0002  max mem: 7989\n",
            "Train Epoch: [18]  [2400/2500]  eta: 0:01:32  lr: 0.000045  loss_ita: 6.1870  loss_itm: 0.6261  loss_lm: 5.1146  time: 0.9306  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2450/2500]  eta: 0:00:46  lr: 0.000045  loss_ita: 6.0545  loss_itm: 0.6384  loss_lm: 4.0758  time: 0.9333  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18]  [2499/2500]  eta: 0:00:00  lr: 0.000045  loss_ita: 6.0094  loss_itm: 0.6447  loss_lm: 4.6467  time: 0.9193  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [18] Total time: 0:38:43 (0.9293 s / it)\n",
            "Averaged stats: lr: 0.0000  loss_ita: 6.0553  loss_itm: 0.6362  loss_lm: 4.5673\n",
            "Train Epoch: [19]  [   0/2500]  eta: 1:33:40  lr: 0.000041  loss_ita: 6.0785  loss_itm: 0.6439  loss_lm: 3.9600  time: 2.2481  data: 1.0550  max mem: 7989\n",
            "Train Epoch: [19]  [  50/2500]  eta: 0:40:28  lr: 0.000041  loss_ita: 6.1363  loss_itm: 0.6342  loss_lm: 4.7595  time: 0.9812  data: 0.0003  max mem: 7989\n",
            "Train Epoch: [19]  [ 100/2500]  eta: 0:38:56  lr: 0.000041  loss_ita: 6.1894  loss_itm: 0.6489  loss_lm: 4.6459  time: 0.9512  data: 0.0005  max mem: 7989\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}