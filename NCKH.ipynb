{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khoi123345/NCKH/blob/main/NCKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K72SnZ4sz5",
        "outputId": "5b53d954-a522-4b0c-9fd7-a8247c77e1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y0on5SJkiKC2",
        "outputId": "7b3ad1a3-bb95-49cc-f604-cde295430af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting cog\n",
            "  Downloading cog-0.16.8-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Collecting attrs<24,>=20.1 (from cog)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastapi<0.119.0,>=0.100 (from cog)\n",
            "  Downloading fastapi-0.118.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.12/dist-packages (from cog) (2.12.3)\n",
            "Collecting structlog<25,>=20 (from cog)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.40.0)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi<0.119.0,>=0.100->cog)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9->cog) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<0.119.0,>=0.100->cog) (4.12.1)\n",
            "Downloading cog-0.16.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.118.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: structlog, attrs, starlette, fastapi, cog\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.50.0\n",
            "    Uninstalling starlette-0.50.0:\n",
            "      Successfully uninstalled starlette-0.50.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.123.10\n",
            "    Uninstalling fastapi-0.123.10:\n",
            "      Successfully uninstalled fastapi-0.123.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sse-starlette 3.1.2 requires starlette>=0.49.1, but you have starlette 0.48.0 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 cog-0.16.8 fastapi-0.118.3 starlette-0.48.0 structlog-24.4.0\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.19.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (2.32.4)\n",
            "Collecting sacremoses (from transformers==4.16.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.16.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.16.2) (2026.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses->transformers==4.16.2) (1.5.3)\n",
            "Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.6\n",
            "    Uninstalling transformers-4.57.6:\n",
            "      Successfully uninstalled transformers-4.57.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.0 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sacremoses-0.1.1 transformers-4.16.2\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fairscale==0.4.4) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->fairscale==0.4.4) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->fairscale==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->fairscale==0.4.4) (3.0.3)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292935 sha256=7c92929b8adfabacee98b2ef5d4e94f8755cf206c055433a69995b78b69856f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/20/41/aa2b3bbf5be3e499093089cf636460d2111a2b3314ae0ebbc6\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.4\n",
            "\u001b[33mWARNING: Skipping pycocoevalcap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers timm torch pillow cog\n",
        "!pip install ruamel.yaml\n",
        "!pip install transformers==4.16.2\n",
        "!pip install timm==0.4.12\n",
        "!pip install fairscale==0.4.4\n",
        "# gỡ cài đặt cũ nếu có\n",
        "!pip uninstall -y pycocoevalcap\n",
        "\n",
        "# cài đặt\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC3sZbDay9W"
      },
      "source": [
        "TextVQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gEThk07uicki",
        "outputId": "1e478fc8-2088-49e7-efb5-7547a8dcd3df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-20 10:35:57--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.175.34.84, 3.175.34.29, 3.175.34.94, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.175.34.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107289766 (102M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_train.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>] 102.32M  98.2MB/s    in 1.0s    \n",
            "\n",
            "2026-01-20 10:35:58 (98.2 MB/s) - ‘/content/textvqa/TextVQA_train.json’ saved [107289766/107289766]\n",
            "\n",
            "--2026-01-20 10:35:58--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.175.34.84, 3.175.34.29, 3.175.34.94, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.175.34.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15782960 (15M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_val.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  15.05M  45.7MB/s    in 0.3s    \n",
            "\n",
            "2026-01-20 10:35:59 (45.7 MB/s) - ‘/content/textvqa/TextVQA_val.json’ saved [15782960/15782960]\n",
            "\n",
            "--2026-01-20 10:35:59--  https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.175.34.84, 3.175.34.29, 3.175.34.94, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.175.34.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13582003 (13M) [text/plain]\n",
            "Saving to: ‘/content/textvqa/TextVQA_test.json’\n",
            "\n",
            "/content/textvqa/Te 100%[===================>]  12.95M  43.1MB/s    in 0.3s    \n",
            "\n",
            "2026-01-20 10:36:00 (43.1 MB/s) - ‘/content/textvqa/TextVQA_test.json’ saved [13582003/13582003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Tạo thư mục chứa dữ liệu\n",
        "!mkdir -p /content/textvqa\n",
        "\n",
        "# Tải annotation (train, val, test)\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_train.json -O /content/textvqa/TextVQA_train.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_val.json -O /content/textvqa/TextVQA_val.json\n",
        "!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5_test.json -O /content/textvqa/TextVQA_test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2W9vwLBqign5",
        "outputId": "723ec925-b92d-438d-e790-edf45437d7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-20 10:36:00--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.175.34.84, 3.175.34.29, 3.175.34.94, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.175.34.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘/content/textvqa/train_val_images.zip’\n",
            "\n",
            "/content/textvqa/tr 100%[===================>]   6.59G  58.2MB/s    in 78s     \n",
            "\n",
            "2026-01-20 10:37:18 (87.0 MB/s) - ‘/content/textvqa/train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip -O /content/textvqa/train_val_images.zip\n",
        "!unzip -q /content/textvqa/train_val_images.zip -d /content/textvqa/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train TextVQA (sửa lại config nếu cần)"
      ],
      "metadata": {
        "id": "Rm9I-uzJ-j9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJR81Ssjyin",
        "outputId": "a822500c-6765-448a-b097-666471cb6f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Creating vqa datasets\n",
            ">>> Check dataset size:\n",
            "Train dataset size: 34602\n",
            "Val dataset size: 5000\n",
            "Creating model\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 285kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 26.7MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.54MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 3.16MB/s]\n",
            "=> loading checkpoint './output/textVQA/checkpoint_00.pth'\n",
            "=> loaded checkpoint './output/textVQA/checkpoint_00.pth' (epoch 1)\n",
            "Start training\n",
            "Loss: 18.0213\n",
            "==========================================\n",
            "Train Epoch: [1]  [   0/8650]  eta: 8:41:45  lr: 0.000020  loss: 18.0213  time: 3.6191  data: 1.3212  max mem: 6922\n",
            "Loss: 20.3541\n",
            "==========================================\n",
            "Loss: 15.6865\n",
            "==========================================\n",
            "Loss: 19.0375\n",
            "==========================================\n",
            "Loss: 22.353\n",
            "==========================================\n",
            "Loss: 17.213\n",
            "==========================================\n",
            "Loss: 14.6663\n",
            "==========================================\n",
            "Loss: 22.5208\n",
            "==========================================\n",
            "Loss: 22.3326\n",
            "==========================================\n",
            "Loss: 23.3974\n",
            "==========================================\n",
            "Loss: 18.9809\n",
            "==========================================\n",
            "Loss: 23.0083\n",
            "==========================================\n",
            "Loss: 12.0474\n",
            "==========================================\n",
            "Loss: 27.0992\n",
            "==========================================\n",
            "Loss: 14.7998\n",
            "==========================================\n",
            "Loss: 29.8942\n",
            "==========================================\n",
            "Loss: 17.8424\n",
            "==========================================\n",
            "Loss: 12.1815\n",
            "==========================================\n",
            "Loss: 20.6553\n",
            "==========================================\n",
            "Loss: 32.7879\n",
            "==========================================\n",
            "Loss: 26.4066\n",
            "==========================================\n",
            "Loss: 16.4823\n",
            "==========================================\n",
            "Loss: 25.192\n",
            "==========================================\n",
            "Loss: 21.3599\n",
            "==========================================\n",
            "Loss: 12.707\n",
            "==========================================\n",
            "Loss: 12.7933\n",
            "==========================================\n",
            "Loss: 19.8889\n",
            "==========================================\n",
            "Loss: 21.3653\n",
            "==========================================\n",
            "Loss: 13.9768\n",
            "==========================================\n",
            "Loss: 22.718\n",
            "==========================================\n",
            "Loss: 41.3385\n",
            "==========================================\n",
            "Loss: 32.8064\n",
            "==========================================\n",
            "Loss: 17.9516\n",
            "==========================================\n",
            "Loss: 20.3629\n",
            "==========================================\n",
            "Loss: 16.7477\n",
            "==========================================\n",
            "Loss: 14.3636\n",
            "==========================================\n",
            "Loss: 32.225\n",
            "==========================================\n",
            "Loss: 27.7842\n",
            "==========================================\n",
            "Loss: 18.7061\n",
            "==========================================\n",
            "Loss: 22.2025\n",
            "==========================================\n",
            "Loss: 14.7057\n",
            "==========================================\n",
            "Loss: 26.123\n",
            "==========================================\n",
            "Loss: 16.1308\n",
            "==========================================\n",
            "Loss: 17.3781\n",
            "==========================================\n",
            "Loss: 10.2893\n",
            "==========================================\n",
            "Loss: 15.1519\n",
            "==========================================\n",
            "Loss: 18.1992\n",
            "==========================================\n",
            "Loss: 23.8038\n",
            "==========================================\n",
            "Loss: 12.1579\n",
            "==========================================\n",
            "Loss: 30.3016\n",
            "==========================================\n",
            "Train Epoch: [1]  [  50/8650]  eta: 1:15:37  lr: 0.000020  loss: 9.8084  time: 0.4040  data: 0.0003  max mem: 6940\n",
            "Train Epoch: [1]  [ 100/8650]  eta: 1:11:03  lr: 0.000020  loss: 24.8522  time: 0.4050  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [ 150/8650]  eta: 1:09:24  lr: 0.000020  loss: 18.3403  time: 0.4342  data: 0.0003  max mem: 9086\n",
            "Train Epoch: [1]  [ 200/8650]  eta: 1:08:29  lr: 0.000020  loss: 16.5583  time: 0.4457  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 250/8650]  eta: 1:07:47  lr: 0.000020  loss: 15.9814  time: 0.4541  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [ 300/8650]  eta: 1:07:02  lr: 0.000020  loss: 14.5253  time: 0.4498  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 350/8650]  eta: 1:06:23  lr: 0.000020  loss: 20.9223  time: 0.4490  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 400/8650]  eta: 1:05:41  lr: 0.000020  loss: 21.3319  time: 0.4574  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 450/8650]  eta: 1:05:01  lr: 0.000020  loss: 17.3729  time: 0.4598  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [ 500/8650]  eta: 1:04:28  lr: 0.000020  loss: 20.9557  time: 0.4611  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 550/8650]  eta: 1:03:48  lr: 0.000020  loss: 14.3973  time: 0.4580  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [ 600/8650]  eta: 1:03:18  lr: 0.000020  loss: 19.4456  time: 0.4716  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 650/8650]  eta: 1:02:53  lr: 0.000020  loss: 12.3833  time: 0.4829  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 700/8650]  eta: 1:02:27  lr: 0.000020  loss: 21.5214  time: 0.4853  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 750/8650]  eta: 1:01:59  lr: 0.000020  loss: 20.3844  time: 0.4789  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 800/8650]  eta: 1:01:34  lr: 0.000020  loss: 22.0041  time: 0.4730  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [ 850/8650]  eta: 1:01:14  lr: 0.000020  loss: 15.3439  time: 0.4784  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [ 900/8650]  eta: 1:00:49  lr: 0.000020  loss: 23.6376  time: 0.4818  data: 0.0004  max mem: 9086\n",
            "Train Epoch: [1]  [ 950/8650]  eta: 1:00:22  lr: 0.000020  loss: 18.3488  time: 0.4739  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [1000/8650]  eta: 0:59:56  lr: 0.000020  loss: 37.3383  time: 0.4838  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [1050/8650]  eta: 0:59:33  lr: 0.000020  loss: 37.5881  time: 0.4954  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [1100/8650]  eta: 0:59:07  lr: 0.000020  loss: 24.2115  time: 0.4798  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1150/8650]  eta: 0:58:40  lr: 0.000020  loss: 21.8421  time: 0.4849  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [1200/8650]  eta: 0:58:14  lr: 0.000020  loss: 21.9620  time: 0.4830  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [1250/8650]  eta: 0:57:53  lr: 0.000020  loss: 27.5806  time: 0.5203  data: 0.0005  max mem: 9086\n",
            "Train Epoch: [1]  [1300/8650]  eta: 0:57:30  lr: 0.000020  loss: 11.8522  time: 0.5028  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1350/8650]  eta: 0:57:06  lr: 0.000020  loss: 18.6968  time: 0.5013  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1400/8650]  eta: 0:56:42  lr: 0.000020  loss: 31.1378  time: 0.4985  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [1450/8650]  eta: 0:56:16  lr: 0.000020  loss: 14.2946  time: 0.5027  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1500/8650]  eta: 0:55:51  lr: 0.000020  loss: 34.3791  time: 0.5194  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1550/8650]  eta: 0:55:28  lr: 0.000020  loss: 28.5466  time: 0.5206  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1600/8650]  eta: 0:55:02  lr: 0.000020  loss: 25.7295  time: 0.5147  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [1650/8650]  eta: 0:54:37  lr: 0.000020  loss: 28.2977  time: 0.5283  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [1700/8650]  eta: 0:54:14  lr: 0.000020  loss: 21.1407  time: 0.5554  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [1750/8650]  eta: 0:53:49  lr: 0.000020  loss: 14.9652  time: 0.5241  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [1800/8650]  eta: 0:53:25  lr: 0.000020  loss: 12.5731  time: 0.5311  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [1850/8650]  eta: 0:53:00  lr: 0.000020  loss: 15.3829  time: 0.5451  data: 0.0012  max mem: 9086\n",
            "Train Epoch: [1]  [1900/8650]  eta: 0:52:36  lr: 0.000020  loss: 23.0921  time: 0.5501  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [1950/8650]  eta: 0:52:12  lr: 0.000020  loss: 26.7940  time: 0.5558  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [2000/8650]  eta: 0:51:47  lr: 0.000020  loss: 21.3009  time: 0.5575  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2050/8650]  eta: 0:51:24  lr: 0.000020  loss: 18.3009  time: 0.5714  data: 0.0013  max mem: 9086\n",
            "Train Epoch: [1]  [2100/8650]  eta: 0:50:59  lr: 0.000020  loss: 17.7155  time: 0.5482  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [2150/8650]  eta: 0:50:36  lr: 0.000020  loss: 14.8014  time: 0.5684  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [2200/8650]  eta: 0:50:11  lr: 0.000020  loss: 16.3732  time: 0.5649  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [2250/8650]  eta: 0:49:47  lr: 0.000020  loss: 16.1633  time: 0.5658  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2300/8650]  eta: 0:49:24  lr: 0.000020  loss: 24.2512  time: 0.5653  data: 0.0011  max mem: 9086\n",
            "Train Epoch: [1]  [2350/8650]  eta: 0:49:00  lr: 0.000020  loss: 22.3515  time: 0.5643  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [2400/8650]  eta: 0:48:37  lr: 0.000020  loss: 16.9707  time: 0.5678  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [2450/8650]  eta: 0:48:12  lr: 0.000020  loss: 19.9952  time: 0.5518  data: 0.0008  max mem: 9086\n",
            "Train Epoch: [1]  [2500/8650]  eta: 0:47:48  lr: 0.000020  loss: 12.8021  time: 0.5526  data: 0.0006  max mem: 9086\n",
            "Train Epoch: [1]  [2550/8650]  eta: 0:47:24  lr: 0.000020  loss: 15.0916  time: 0.5498  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2600/8650]  eta: 0:47:01  lr: 0.000020  loss: 23.0702  time: 0.5529  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2650/8650]  eta: 0:46:37  lr: 0.000020  loss: 16.7397  time: 0.5498  data: 0.0007  max mem: 9086\n",
            "Train Epoch: [1]  [2700/8650]  eta: 0:46:13  lr: 0.000020  loss: 12.8185  time: 0.5630  data: 0.0010  max mem: 9086\n",
            "Train Epoch: [1]  [2750/8650]  eta: 0:45:50  lr: 0.000020  loss: 12.7175  time: 0.5627  data: 0.0009  max mem: 9086\n",
            "Train Epoch: [1]  [2800/8650]  eta: 0:45:26  lr: 0.000020  loss: 20.2372  time: 0.5505  data: 0.0007  max mem: 9086\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa.py \\\n",
        "    --config ./configs/textvqa.yaml \\\n",
        "    --output_dir ./output/textVQA \\\n",
        "    --device cuda \\\n",
        "    --distributed False \\\n",
        "    --resume ./output/textVQA/checkpoint_02.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwGEdm4q5aL",
        "outputId": "26128e73-dfa8-4c1c-98a2-cddae7a2e1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
            "/usr/local/lib/python3.12/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n",
            "Not using distributed mode\n",
            "Creating vqa datasets...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating SimpleLLaVA model (Float32)...\n",
            "Loading BLIP ViT: base, size: 224\n",
            "Loading LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "2025-11-27 10:28:33.284013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764239313.452546   16233 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764239313.498604   16233 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764239313.850262   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850300   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850304   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764239313.850310   16233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-27 10:28:33.888460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using Initial LR: 1e-05\n",
            "Start training\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/BLIP/transform/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "/content/drive/MyDrive/BLIP/transform/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 180, in <module>\n",
            "    main(args, config)\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 139, in main\n",
            "    train_stats = train(model, train_loader, optimizer, epoch, device) \n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/BLIP/train_vqa_llava.py\", line 56, in train\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 82, in _use_grad\n",
            "    ret = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 237, in step\n",
            "    has_complex = self._init_group(\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 181, in _init_group\n",
            "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
            "                          ^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 162628 has 14.74 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 31.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "!python train_vqa_llava.py --config ./configs/textvqa.yaml --output_dir ./output/textVQA_llava --device cuda --distributed False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oLyiLEWV2SQi",
        "outputId": "2ebf0440-c1be-4e54-c20f-0005bda56c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/BLIP/train_vqa_llava.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/BLIP/train_vqa_llava.py\n",
        "import argparse\n",
        "import os\n",
        "from ruamel.yaml import YAML\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "\n",
        "from models.simple_llava import SimpleLLaVA\n",
        "import utils\n",
        "from utils import cosine_lr_schedule\n",
        "from data import create_dataset, create_sampler, create_loader\n",
        "from data.vqa_dataset import vqa_collate_fn\n",
        "from data.textvqa_dataset import textvqa_collate_fn\n",
        "from data.utils import save_result\n",
        "\n",
        "def train(model, data_loader, optimizer, epoch, device, max_debug_batches=20):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "\n",
        "    header = 'Train Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "\n",
        "    for i, (image, question, answer, weights, n) in enumerate(\n",
        "            metric_logger.log_every(data_loader, print_freq, header)):\n",
        "\n",
        "        # Check NaN input\n",
        "        if torch.isnan(image).any():\n",
        "            print(f\"Skipping Batch {i} (Input Image NaN)\")\n",
        "            continue\n",
        "\n",
        "        image = image.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Chạy trực tiếp (Float32)\n",
        "        loss = model(image, question, answer, train=True)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"WARNING: Batch {i} Loss is NaN. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping (Vẫn giữ để an toàn)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
        "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, device, config):\n",
        "    model.eval()\n",
        "    result = []\n",
        "    print(\"Generating answers...\")\n",
        "\n",
        "    for n, (image, question, question_id) in enumerate(data_loader):\n",
        "        image = image.to(device, non_blocking=True)\n",
        "        answers = model(image, question, train=False)\n",
        "\n",
        "        for answer, ques_id in zip(answers, question_id):\n",
        "            ques_id = int(ques_id.item())\n",
        "            result.append({\"question_id\": ques_id, \"answer\": answer})\n",
        "\n",
        "        if n % 10 == 0:\n",
        "            print(f\"Sample: Q: {question[0]} | A: {answers[0]}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def main(args, config):\n",
        "    utils.init_distributed_mode(args)\n",
        "    device = torch.device(args.device)\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    print(\"Creating vqa datasets...\")\n",
        "    datasets = create_dataset('textvqa', config)\n",
        "\n",
        "    if args.distributed:\n",
        "        num_tasks = utils.get_world_size()\n",
        "        global_rank = utils.get_rank()\n",
        "        samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
        "    else:\n",
        "        samplers = [None, None]\n",
        "\n",
        "    train_loader, test_loader = create_loader(datasets, samplers,\n",
        "                                              batch_size=[config['batch_size_train'], config['batch_size_test']],\n",
        "                                              num_workers=[4,4], is_trains=[True, False],\n",
        "                                              collate_fns=[textvqa_collate_fn, None])\n",
        "\n",
        "    print(\"Creating SimpleLLaVA model (Float32)...\")\n",
        "    model = SimpleLLaVA(\n",
        "        llm_model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "        image_size=config['image_size'],\n",
        "        vit_type=config['vit'],\n",
        "        vit_grad_ckpt=config.get('vit_grad_ckpt', False),\n",
        "        vit_ckpt_layer=config.get('vit_ckpt_layer', 0),\n",
        "        pretrained_blip_url=config['pretrained'],\n",
        "        freeze_vision=True\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    # Init LR an toàn\n",
        "    init_lr = 1e-5\n",
        "    print(f\"Using Initial LR: {init_lr}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=init_lr, weight_decay=config['weight_decay'])\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(0, config['max_epoch']):\n",
        "        if not args.evaluate:\n",
        "            if args.distributed:\n",
        "                train_loader.sampler.set_epoch(epoch)\n",
        "            cosine_lr_schedule(optimizer, epoch, config['max_epoch'], init_lr, 1e-6)\n",
        "            train_stats = train(model, train_loader, optimizer, epoch, device)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        if utils.is_main_process():\n",
        "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, 'epoch': epoch}\n",
        "            with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "            save_obj = { 'model': model_without_ddp.state_dict(), 'epoch': epoch }\n",
        "            torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
        "\n",
        "        if args.distributed:\n",
        "            dist.barrier()\n",
        "\n",
        "    print(\"Starting Evaluation...\")\n",
        "    vqa_result = evaluation(model_without_ddp, test_loader, device, config)\n",
        "    result_file = save_result(vqa_result, args.result_dir, 'vqa_result')\n",
        "    total_time = time.time() - start_time\n",
        "    print('Training time {}'.format(str(datetime.timedelta(seconds=int(total_time)))))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', default='./configs/textvqa.yaml')\n",
        "    parser.add_argument('--output_dir', default='output/textVQA')\n",
        "    parser.add_argument('--evaluate', action='store_true')\n",
        "    parser.add_argument('--device', default='cuda')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--world_size', default=1, type=int)\n",
        "    parser.add_argument('--dist_url', default='env://')\n",
        "    parser.add_argument('--distributed', default=False, type=bool)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    yaml_loader = YAML()\n",
        "    yaml_loader.typ = 'safe'\n",
        "    with open(args.config, 'r') as f:\n",
        "      config = yaml_loader.load(f)\n",
        "    args.result_dir = os.path.join(args.output_dir, 'result')\n",
        "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
        "    with open(os.path.join(args.output_dir, 'config.yaml'), 'w') as f:\n",
        "      yaml_loader.dump(config, f)\n",
        "    main(args, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHhslGC2SmK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1lHqswFgw4"
      },
      "source": [
        "**FILE PREDICT.PY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTImihuAJlbd",
        "outputId": "fe714746-929e-409b-8b68-8e60a3455971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BLIP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZs3jGOqLoEV"
      },
      "outputs": [],
      "source": [
        "# 🩹 Fix lỗi Cog không có class Predictor trên Colab\n",
        "import types, sys\n",
        "\n",
        "# Tạo 1 mô-đun ảo \"cog\" giả\n",
        "fake_cog = types.SimpleNamespace()\n",
        "\n",
        "# Tạo class giả Predictor và decorator input\n",
        "class DummyCogPredictor:\n",
        "    pass\n",
        "\n",
        "def dummy_input(*args, **kwargs):\n",
        "    def wrap(f):\n",
        "        return f\n",
        "    return wrap\n",
        "\n",
        "# Gắn vào mô-đun ảo\n",
        "fake_cog.Predictor = DummyCogPredictor\n",
        "fake_cog.input = dummy_input\n",
        "\n",
        "# Gắn mô-đun này vào sys.modules để Python hiểu \"import cog\"\n",
        "sys.modules[\"cog\"] = fake_cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4o2dqoNsGMWP",
        "outputId": "6f2620ea-2653-4caa-b297-9dfbcf68b385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from checkpoints/model*_base_caption.pth\n",
            "load checkpoint from checkpoints/model*_vqa.pth\n",
            "load checkpoint from checkpoints/model_base_retrieval_coco.pth\n",
            "Answer: can't read it\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from predict import Predictor\n",
        "\n",
        "# 1️⃣ Tạo đối tượng Predictor\n",
        "p = Predictor()\n",
        "\n",
        "# 2️⃣ Load model\n",
        "p.setup()\n",
        "\n",
        "# 3️⃣ Gọi hàm predict\n",
        "result = p.predict(\n",
        "    image=Path(\"/content/textvqa/train_images/0000599864fd15b3.jpg\"),\n",
        "    task=\"visual_question_answering\",\n",
        "    question=\"What is written on the sign?\",\n",
        "    caption=None\n",
        ")\n",
        "\n",
        "# 4️⃣ In kết quả\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCleRojuu43",
        "outputId": "57877f8b-7737-4849-8837-4d6824c99028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 28.00% (28/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "correct_exact = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Exact match: chỉ đúng nếu trùng hoàn toàn với ít nhất một đáp án\n",
        "    if pred in true_answers:\n",
        "        correct_exact += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Exact Match Accuracy: {correct_exact/total*100:.2f}% ({correct_exact}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xgwidxMFfd",
        "outputId": "cf16d7d3-2ea1-4969-d4c2-0dcb95080b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soft Match Accuracy: 37.00% (37/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def soft_match(pred, answers):\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    for ans in answers:\n",
        "        ans_tokens = set(ans.lower().split())\n",
        "        if len(pred_tokens & ans_tokens) > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "correct_soft = 0\n",
        "total = 0\n",
        "\n",
        "for item in tqdm(data['data'][:100]):\n",
        "    image_path = Path(f\"/content/textvqa/train_images/{item['image_id']}.jpg\")\n",
        "    question = item['question']\n",
        "    true_answers = [a.lower().strip() for a in item['answers']]\n",
        "\n",
        "    result = p.predict(image=image_path, task=\"visual_question_answering\", question=question, caption=None)\n",
        "    pred = result.replace(\"Answer:\", \"\").strip().lower()\n",
        "\n",
        "    # Soft match: chỉ cần có từ trùng là tính đúng\n",
        "    if soft_match(pred, true_answers):\n",
        "        correct_soft += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Soft Match Accuracy: {correct_soft/total*100:.2f}% ({correct_soft}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPep4dxQKH9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}